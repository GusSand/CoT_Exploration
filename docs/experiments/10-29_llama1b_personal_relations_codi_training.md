# Personal Relations CODI Training - LLaMA-3.2-1B

**Date**: 2025-10-29
**Model**: LLaMA-3.2-1B-Instruct with CODI
**Task**: Personal Relations (Graph Traversal Reasoning)
**Status**: âœ… COMPLETE

---

## Executive Summary

Successfully trained a CODI model on the Personal Relations task, achieving **43.7% accuracy** (328/750 correct), matching the few-shot baseline. This represents a **3x improvement** over CODI v1 (14.8%), demonstrating that **universe context is critical** for enabling graph traversal reasoning in continuous thought space.

### Key Result
**Universe context transforms CODI performance on graph reasoning from near-failure (14.8%) to baseline-matching (43.7%)**

---

## Experiment Configuration

### Model Architecture
- **Base Model**: meta-llama/Llama-3.2-1B-Instruct
- **CODI Configuration**:
  - Continuous thought tokens: 6 (CT0-CT5)
  - LoRA rank: 128
  - LoRA alpha: 32
  - Projection dimension: 2048
  - Distillation loss factor: 20

### Training Parameters
- **Epochs**: 10
- **Learning rate**: 8e-4 with cosine decay
- **Batch size**: 32 per device
- **Gradient accumulation**: 2 steps
- **Precision**: bfloat16
- **Total steps**: 270

### Dataset
- **Source**: Personal Relations task with universe context (v2)
- **Train set**: 2,700 examples
- **Test set**: 750 examples
- **Format**: Each example includes:
  - Full relationship universe (all person-relation-person triples)
  - Query: "Who is the [relation] of [person]?"
  - Answer: Single name

---

## Results

### Performance Metrics
| Model | Accuracy | Correct/Total |
|-------|----------|---------------|
| **CODI v2 (with universe)** | **43.7%** | 328/750 |
| Few-shot baseline (with universe) | 43.8% | 329/750 |
| **CODI v1 (no universe)** | 14.8% | 111/750 |

### Training Convergence
- **Initial loss**: 5.3
- **Final loss**: 0.11
- **Reduction**: 48x improvement
- **Distill loss**: 0.029 (final)
- **Convergence**: Stable, learning rate decayed to 0

### Model Behavior
The trained model successfully:
1. Encodes relationship graphs in 6 continuous thought tokens
2. Performs multi-hop traversal (e.g., "father of brother" = "uncle")
3. Outputs structured answers: " = [Name]\nThe answer is: [Name]"

---

## Technical Implementation

### Critical Configuration Requirements

#### 1. Model Loading (Evaluation)
```python
# REQUIRED for checkpoint loading
model_args = ModelArgs(
    model_name_or_path="meta-llama/Llama-3.2-1B-Instruct",
    use_lora=True,  # CRITICAL: Must match training
    lora_r=128,
    lora_alpha=32,
    full_precision=True,  # CRITICAL: Must match training (no quantization)
)

training_args = TrainingArgs(
    num_latent=6,
    use_lora=True,  # CRITICAL: Creates LoRA layers
    use_prj=True,
    prj_dim=2048,
)

# Initialize LoRA config BEFORE model creation
lora_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=model_args.lora_r,
    lora_alpha=model_args.lora_alpha,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "up_proj", "down_proj", "gate_proj"],
)

# Load model WITH LoRA layers
model = CODI(model_args, training_args, lora_config)
checkpoint = torch.load(f"{checkpoint_path}/pytorch_model.bin")
model.load_state_dict(checkpoint, strict=False)
```

#### 2. Answer Extraction
```python
def extract_answer(generated_text: str, question: str) -> str:
    """Extract answer handling model's specific output format."""
    # Remove question
    answer_part = generated_text.split(question)[-1].strip()

    # Look for "answer is:" format (most reliable)
    if "answer is:" in answer_part.lower():
        answer_part = answer_part.lower().split("answer is:")[-1].strip()
        answer = answer_part.split()[0]
        return answer.replace('.', '').replace(',', '').strip().title()

    # Skip "=" prefix if present
    if answer_part.startswith('='):
        answer_part = answer_part[1:].strip()

    # Extract first word
    answer = answer_part.split()[0] if answer_part.split() else answer_part
    return answer.replace('.', '').replace(',', '').strip()
```

###3. Debugging Journey

**Issue 1**: 0-1.6% accuracy despite training convergence
- **Root cause**: LoRA weights not loaded (`use_lora=False` during eval)
- **Solution**: Enable LoRA in both ModelArgs and TrainingArgs

**Issue 2**: Shape mismatch errors during loading
- **Root cause**: Quantization enabled during eval but not during training
- **Solution**: Set `full_precision=True` to match training config

**Issue 3**: 0% accuracy with correct model loading
- **Root cause**: Answer extraction failing on "=" prefix
- **Solution**: Updated parser to handle " = Name\nThe answer is: Name" format

---

## Key Discoveries

### 1. Universe Context is Essential for Graph Reasoning

**Without universe context** (v1):
- Model cannot infer relationships
- Accuracy: 14.8% (barely above random for 16 people)
- CODI unable to compress implicit graph structure

**With universe context** (v2):
- Model successfully encodes full relationship graph
- Accuracy: 43.7% (matches few-shot baseline)
- CODI compresses explicit graph into 6 continuous tokens

**Implication**: For structured reasoning tasks, CODI needs explicit structure in training data to learn effective continuous representations.

### 2. Task Characteristics

Personal Relations differs from GSM8K/CommonsenseQA:
- **Graph structure**: Relationships form explicit graph
- **Compositional**: Multi-hop reasoning (e.g., "father of brother")
- **Deterministic**: Single correct answer given universe
- **Context-dependent**: Requires full universe to answer any query

This sets up for interesting mechanistic comparison:
- GSM8K: Sequential arithmetic computation
- CommonsenseQA: Parallel semantic associations
- Personal Relations: Graph traversal with composition

---

## Files and Artifacts

### Model Checkpoint
- **Path**: `/home/paperspace/dev/CoT_Exploration/models/personal_relations_1b_codi_v2/personal_relations_1b_latent_v2/Llama-3.2-1B-Instruct/ep_10/lr_0.0008/seed_42/checkpoint-270`
- **Size**: 2.7GB (pytorch_model.bin)
- **Contents**: LoRA weights (224 keys) + Projection weights (6 keys) + Base model state

### Evaluation Scripts
- `eval_personal_relations_v2_1b_SIMPLE.py`: Working evaluation script with correct loading and answer extraction
- `eval_personal_relations_1b_correct.py`: Alternative evaluation using different CODI loading pattern

### Results
- `evaluation_results/personal_relations_1b_eval_FINAL_CORRECT.json`: Full results (328/750 correct, 43.7%)
- `evaluation_results/eval_FINAL_CORRECT.log`: Detailed evaluation log

### Training Data
- Training: `/home/paperspace/dev/CoT_Exploration/data/personal_relations/personal_relations_train_codi_v2.json` (2,700 examples)
- Test: `/home/paperspace/dev/CoT_Exploration/data/personal_relations/personal_relations_test_codi_v2.json` (750 examples)

---

## Next Steps

### Immediate: 3-Way Mechanistic Comparison
Compare how CODI encodes different reasoning types:
1. **Personal Relations** (this model): Graph traversal
2. **GSM8K**: Sequential arithmetic
3. **CommonsenseQA**: Semantic associations

**Research Questions**:
- Do continuous thought tokens specialize differently by task?
- How does attention flow differ across reasoning types?
- Are there shared vs. task-specific patterns in CT0-CT5?

### Future Work
- Increase model size (LLaMA-7B) to see if accuracy improves
- Analyze which relationship types are hardest (depth, composition)
- Probe what each CT token encodes (relationships vs. intermediate nodes)
- Test generalization to unseen relationship graphs

---

## Reproducibility

### Training Command
```bash
python train.py \
  --output_dir /home/paperspace/dev/CoT_Exploration/models/personal_relations_1b_codi_v2 \
  --model_name_or_path meta-llama/Llama-3.2-1B-Instruct \
  --data_name personal_relations_v2 \
  --num_train_epochs 10 \
  --learning_rate 8e-4 \
  --per_device_train_batch_size 32 \
  --gradient_accumulation_steps 2 \
  --num_latent 6 \
  --lora_r 128 \
  --lora_alpha 32 \
  --use_prj True \
  --prj_dim 2048 \
  --distill_loss_factor 20 \
  --bf16 \
  --seed 42
```

### Evaluation Command
```bash
python eval_personal_relations_v2_1b_SIMPLE.py \
  --checkpoint /path/to/checkpoint-270 \
  --test_file /path/to/personal_relations_test_codi_v2.json \
  --output personal_relations_eval_results.json
```

---

## Conclusion

**Success**: Universe context enables CODI to match few-shot baseline on graph reasoning task.

**Key Insight**: CODI's continuous thought space can effectively compress structured, compositional reasoning when provided with explicit context. This validates that latent reasoning is viable beyond just arithmetic (GSM8K) and semantic tasks (CommonsenseQA).

**Impact**: Enables direct mechanistic comparison of how CODI encodes three fundamentally different reasoning types, advancing our understanding of latent continuous thought representations.
