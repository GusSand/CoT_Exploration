#!/usr/bin/env python3
"""
Generate final report for contrastive CODI deception detection experiment.
"""

import json
import argparse
from pathlib import Path
from datetime import datetime

def load_results(results_dir):
    """Load all experimental results."""
    results_dir = Path(results_dir)

    with open(results_dir / "probe_comparison_results.json") as f:
        comparison = json.load(f)

    with open(results_dir / "ct_token_probe_results.json") as f:
        ct_results = json.load(f)

    with open(results_dir / "regular_hidden_probe_results.json") as f:
        regular_results = json.load(f)

    return comparison, ct_results, regular_results

def generate_report(comparison, ct_results, regular_results, output_path):
    """Generate markdown report."""
    report = []

    # Header
    report.append("# Contrastive CODI Deception Detection Results")
    report.append("")
    report.append(f"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}")
    report.append(f"**Experiment**: LIARS-BENCH Contrastive Training Smoke Test")
    report.append(f"**Model**: LLaMA-3.2-1B-Instruct with CODI (6 CT tokens)")
    report.append("")

    # Research Question
    report.append("## ðŸ”¬ Research Question")
    report.append("")
    report.append("**Can continuous thought (CT) tokens encode deception intent when trained with contrastive examples?**")
    report.append("")

    # Key Findings
    summary = comparison['summary']
    ct_mean = summary['ct_tokens_mean_accuracy']
    reg_mean = summary['regular_hidden_mean_accuracy']
    advantage = summary['ct_tokens_advantage']

    report.append("## ðŸŽ¯ Key Findings")
    report.append("")

    if ct_mean > reg_mean:
        report.append(f"âœ… **YES**: CT tokens outperform regular hidden states by **{advantage:.3f}** accuracy points")
        report.append("   - Continuous thought appears to encode deception-relevant information!")
    elif ct_mean < reg_mean:
        report.append(f"âŒ **NO**: CT tokens underperform regular hidden states by **{-advantage:.3f}** accuracy points")
        report.append("   - Regular hidden states are better for deception detection")
    else:
        report.append("ðŸ¤· **UNCLEAR**: CT tokens and regular hidden states perform similarly")

    report.append("")
    report.append(f"- **CT Tokens Mean Accuracy**: {ct_mean:.3f}")
    report.append(f"- **Regular Hidden States Mean Accuracy**: {reg_mean:.3f}")
    report.append(f"- **Random Baseline**: {summary['random_baseline']:.3f}")
    report.append("")

    # Methodology
    report.append("## ðŸ“‹ Methodology")
    report.append("")
    report.append("### Dataset")
    report.append("- **Source**: LIARS-BENCH (144 honest/deceptive question pairs)")
    report.append("- **Training**: 288 samples (144 honest + 144 deceptive)")
    report.append("- **Testing**: 288 samples (144 honest + 144 deceptive)")
    report.append("- **Split**: Question-level held-out (no question overlap)")
    report.append("")

    report.append("### Training")
    report.append("- **Model**: LLaMA-3.2-1B-Instruct")
    report.append("- **Method**: CODI with contrastive training")
    report.append("- **CT Tokens**: 6 tokens (CT0 through CT5)")
    report.append("- **Epochs**: 8 (smoke test)")
    report.append("- **System Prompts**: Contrasting honest vs deceptive instructions")
    report.append("")

    report.append("### Probe Training")
    report.append("- **Algorithm**: Logistic Regression with balanced class weights")
    report.append("- **Layers**: [4, 5, 9, 12, 15] (equivalent to Apollo's layer 22/80 = 27.5%)")
    report.append("- **Features**: Mean activation across all token positions")
    report.append("- **Evaluation**: 5-fold stratified cross-validation")
    report.append("- **Preprocessing**: StandardScaler normalization")
    report.append("")

    # Detailed Results
    report.append("## ðŸ“Š Detailed Results")
    report.append("")

    report.append("### Layer-by-Layer Performance")
    report.append("")
    report.append("| Layer | CT Tokens | Regular Hidden | Difference | CT AUC | Regular AUC |")
    report.append("|-------|-----------|----------------|------------|---------|-------------|")

    for ct_res, reg_res in zip(comparison['ct_token_results'], comparison['regular_hidden_results']):
        layer = ct_res['layer']
        ct_acc = ct_res['accuracy']
        reg_acc = reg_res['accuracy']
        diff = ct_acc - reg_acc
        ct_auc = ct_res['auc']
        reg_auc = reg_res['auc']

        report.append(f"| {layer} | {ct_acc:.3f} Â± {ct_res['accuracy_std']:.3f} | {reg_acc:.3f} Â± {reg_res['accuracy_std']:.3f} | {diff:+.3f} | {ct_auc:.3f} | {reg_auc:.3f} |")

    report.append("")

    # Best Performance
    report.append("### Best Performance")
    report.append("")
    report.append(f"- **CT Tokens Best**: {summary['ct_tokens_best_accuracy']:.3f}")
    report.append(f"- **Regular Hidden Best**: {summary['regular_hidden_best_accuracy']:.3f}")
    report.append("")

    # Analysis
    report.append("## ðŸ§  Analysis")
    report.append("")

    if advantage > 0.02:  # 2% advantage threshold
        report.append("### Strong Evidence for CT Token Effectiveness")
        report.append(f"- CT tokens show a meaningful advantage ({advantage:.3f}) over regular hidden states")
        report.append("- This suggests that CODI's continuous thought compression captures deception-relevant features")
        report.append("- The contrastive training successfully differentiated honest vs deceptive intent")
    elif advantage < -0.02:
        report.append("### Evidence Against CT Token Effectiveness")
        report.append(f"- Regular hidden states outperform CT tokens by {-advantage:.3f}")
        report.append("- This suggests that raw activations contain more deception-relevant information")
        report.append("- CODI compression may lose important deception signals")
    else:
        report.append("### Mixed Results")
        report.append("- Performance difference is small and may not be meaningful")
        report.append("- Both methods significantly exceed random baseline")
        report.append("- Need larger dataset or longer training for conclusive results")

    report.append("")

    # Limitations
    report.append("## âš ï¸ Limitations")
    report.append("")
    report.append("- **Small Dataset**: Only 144 question pairs (smoke test)")
    report.append("- **Short Training**: 8 epochs vs typical 20+ epochs")
    report.append("- **Single Model**: Only tested LLaMA-1B")
    report.append("- **Layer Selection**: Heuristic based on Apollo Research (27.5% depth)")
    report.append("- **Aggregation**: Simple mean across tokens (could try other methods)")
    report.append("")

    # Next Steps
    report.append("## ðŸš€ Next Steps")
    report.append("")
    if advantage > 0:
        report.append("### Scale Up (CT tokens show promise)")
        report.append("- Use larger dataset (Apollo's full Instructed-Pairs)")
        report.append("- Train for full 20+ epochs")
        report.append("- Test multiple model sizes")
        report.append("- Explore different CT token aggregation methods")
    else:
        report.append("### Investigate Alternative Approaches")
        report.append("- Try different probe architectures (non-linear)")
        report.append("- Experiment with different CT token extraction methods")
        report.append("- Compare with other continuous reasoning methods")

    report.append("- Analyze specific examples where CT tokens succeed/fail")
    report.append("- Visualize learned representations")
    report.append("")

    # Technical Details
    report.append("## ðŸ”§ Technical Details")
    report.append("")
    report.append("### Files Generated")
    report.append("- `ct_token_activations.pkl`: CT token activation data")
    report.append("- `regular_hidden_activations.pkl`: Regular hidden state data")
    report.append("- `probe_comparison_results.json`: Performance comparison")
    report.append("- `ct_token_probe_results.json`: CT token probe details")
    report.append("- `regular_hidden_probe_results.json`: Regular probe details")
    report.append("")

    report.append("### Reproducibility")
    report.append("- All scripts available in `/src/experiments/contrastive_codi/`")
    report.append("- Random seeds fixed (42) for reproducible results")
    report.append("- Cross-validation ensures robust evaluation")
    report.append("")

    # Write report
    with open(output_path, 'w') as f:
        f.write("\\n".join(report))

    print(f"Report generated: {output_path}")

def main():
    parser = argparse.ArgumentParser(description="Generate experiment report")
    parser.add_argument("--results_dir", required=True, help="Directory containing results")
    parser.add_argument("--output_path", required=True, help="Output path for report")

    args = parser.parse_args()

    comparison, ct_results, regular_results = load_results(args.results_dir)
    generate_report(comparison, ct_results, regular_results, args.output_path)

if __name__ == "__main__":
    main()