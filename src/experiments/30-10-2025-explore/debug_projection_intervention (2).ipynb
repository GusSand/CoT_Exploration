{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Projection Intervention: GPT-2 vs LLaMA\n",
    "\n",
    "This notebook contains minimal working examples of CODI, focussing on LLAMA. There is also similar code for GPT-2, but it has not been tested properly.\n",
    "\n",
    "This includes:\n",
    "- loading model\n",
    "- generating continuous chain of thought\n",
    "- decoding continuous chain of thought\n",
    "- projection replacement of decoded number tokens by specific token like '5'\n",
    "- generating chain of thought with and without BOT (beginning of thought token)\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu128)\n",
      "Collecting peft\n",
      "  Using cached peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting datasets\n",
      "  Using cached datasets-4.3.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting hf_transfer\n",
      "  Using cached hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting dotenv\n",
      "  Using cached dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (7.1.0)\n",
      "Collecting accelerate>=0.21.0 (from peft)\n",
      "  Using cached accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.60.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (112 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/lib/python3/dist-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Using cached pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Using cached pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Collecting python-dotenv (from dotenv)\n",
      "  Using cached python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets)\n",
      "  Using cached yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Using cached matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "Using cached datasets-4.3.0-py3-none-any.whl (506 kB)\n",
      "Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Using cached multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached hf_transfer-0.1.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
      "Using cached accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Using cached aiohttp-3.13.2-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.8 MB)\n",
      "Using cached multidict-6.7.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (256 kB)\n",
      "Using cached yarl-1.22.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (377 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.60.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
      "Using cached frozenlist-1.8.0-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (242 kB)\n",
      "Using cached kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
      "Using cached propcache-0.4.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (221 kB)\n",
      "Using cached pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
      "Using cached regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Using cached pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Using cached xxhash-3.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: pytz, xxhash, tzdata, tqdm, safetensors, regex, python-dotenv, pyarrow, propcache, multidict, kiwisolver, hf-xet, hf_transfer, frozenlist, fonttools, dill, cycler, contourpy, aiohappyeyeballs, yarl, pandas, multiprocess, matplotlib, huggingface-hub, dotenv, aiosignal, tokenizers, aiohttp, transformers, accelerate, peft, datasets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m32/32\u001b[0m [datasets]/32\u001b[0m [datasets]e]s]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.11.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 contourpy-1.3.3 cycler-0.12.1 datasets-4.3.0 dill-0.4.0 dotenv-0.9.9 fonttools-4.60.1 frozenlist-1.8.0 hf-xet-1.2.0 hf_transfer-0.1.9 huggingface-hub-0.36.0 kiwisolver-1.4.9 matplotlib-3.10.7 multidict-6.7.0 multiprocess-0.70.16 pandas-2.3.3 peft-0.17.1 propcache-0.4.1 pyarrow-22.0.0 python-dotenv-1.2.1 pytz-2025.2 regex-2025.10.23 safetensors-0.6.2 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.1 tzdata-2025.2 xxhash-3.6.0 yarl-1.22.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch peft matplotlib datasets tqdm hf_transfer dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train.py',\n",
       " 'test.py',\n",
       " 'src',\n",
       " 'scripts',\n",
       " 'requirements.txt',\n",
       " 'probe_latent_token.py',\n",
       " 'outputs',\n",
       " 'imgs',\n",
       " 'README.md',\n",
       " '.git']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"/workspace/CoT_Exploration/codi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, \"/workspace/CoT_Exploration/codi\")\n",
    "from src.model import CODI, ModelArguments, TrainingArguments\n",
    "from peft import LoraConfig, TaskType\n",
    "from transformers import AutoTokenizer\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test text:\n",
      "Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "18\n",
      "\n",
      "Length: 283 characters\n"
     ]
    }
   ],
   "source": [
    "# GSM8K example\n",
    "question = \"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\"\n",
    "answer = \"Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day. She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer's market. #### 18\"\n",
    "answer = 18\n",
    "\n",
    "test_text = f\"{question}\\n{answer}\"\n",
    "print(f\"Test text:\\n{test_text}\")\n",
    "print(f\"\\nLength: {len(test_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number detection regex\n",
    "number_regex = re.compile(r'^\\s?\\d+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CODI-LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Logged in to HuggingFace\n"
     ]
    }
   ],
   "source": [
    "# Login to HuggingFace (optional - only needed  for gated models like LLaMA)\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "  # Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "  # Get HuggingFace token\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "if hf_token:\n",
    "      from huggingface_hub import login\n",
    "      login(token=hf_token)\n",
    "      print(\"✓ Logged in to HuggingFace\")\n",
    "else:\n",
    "      print(\"⚠ No HF_TOKEN found in .env file -  proceeding without authentication\")\n",
    "      print(\"  (This is fine if models are public)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llama_model():\n",
    "    \"\"\"Load CODI-LLaMA model from HuggingFace Hub\"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"Loading CODI-LLaMA from HuggingFace Hub\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    llama_model_name = \"zen-E/CODI-llama3.2-1b-Instruct\"\n",
    "\n",
    "    llama_model_args = ModelArguments(\n",
    "        model_name_or_path=\"meta-llama/Llama-3.2-1B\",\n",
    "        lora_init=True,\n",
    "        lora_r=128,\n",
    "        lora_alpha=32,\n",
    "        ckpt_dir=None,  # Not using local checkpoint\n",
    "        full_precision=True,\n",
    "        token=None\n",
    "    )\n",
    "\n",
    "    llama_training_args = TrainingArguments(\n",
    "        output_dir=\"./outputs\",\n",
    "        model_max_length=512,\n",
    "        inf_latent_iterations=6,\n",
    "        use_prj=True,\n",
    "        prj_dim=2048,\n",
    "        remove_eos=True,\n",
    "        greedy=True,\n",
    "        bf16=False,\n",
    "        inf_num_iterations=1\n",
    "    )\n",
    "\n",
    "    llama_lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=llama_model_args.lora_r,\n",
    "        lora_alpha=llama_model_args.lora_alpha,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n",
    "                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        init_lora_weights=True,\n",
    "    )\n",
    "\n",
    "    # Initialize the CODI model\n",
    "    llama_model = CODI(llama_model_args, llama_training_args, llama_lora_config)\n",
    "    \n",
    "    # Download and load weights from HuggingFace Hub\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    \n",
    "    print(f\"Downloading weights from {llama_model_name}...\")\n",
    "    checkpoint_path = hf_hub_download(\n",
    "        repo_id=llama_model_name,\n",
    "        filename=\"pytorch_model.bin\",\n",
    "        token=hf_token if hf_token else None\n",
    "    )\n",
    "    \n",
    "    print(f\"Loading weights from {checkpoint_path}...\")\n",
    "    state_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    llama_model.load_state_dict(state_dict, strict=False)\n",
    "    llama_model.codi.tie_weights()\n",
    "    \n",
    "    llama_model = llama_model.to(device)\n",
    "    llama_model = llama_model.to(torch.bfloat16)\n",
    "    llama_model.eval()\n",
    "\n",
    "    llama_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "    \n",
    "    print(\"✓ CODI-LLaMA loaded successfully from HuggingFace Hub\")\n",
    "\n",
    "    return llama_model, llama_tokenizer, llama_training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Loading CODI-LLaMA from Hugging Face Hub\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 98574336 || all params: 1334394880 || trainable%: 7.387193811774817\n",
      "Downloading weights from zen-E/CODI-llama3.2-1b-Instruct...\n",
      "Loading weights from /workspace/.cache/huggingface/hub/models--zen-E--CODI-llama3.2-1b-Instruct/snapshots/b2c88ba224b06b12b52ef39b87f794b98a6eb1c8/pytorch_model.bin...\n",
      "✓ CODI-LLaMA loaded successfully from HuggingFace Hub\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Loading CODI-LLaMA from Hugging Face Hub\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load from HuggingFace Hub\n",
    "llama_model_name = \"zen-E/CODI-llama3.2-1b-Instruct\"\n",
    "\n",
    "llama_model_args = ModelArguments(\n",
    "    model_name_or_path=\"meta-llama/Llama-3.2-1B\",\n",
    "    lora_init=True,\n",
    "    lora_r=128,\n",
    "    lora_alpha=32,\n",
    "    ckpt_dir=None,  # Not using local checkpoint\n",
    "    full_precision=True,\n",
    "    token=None\n",
    ")\n",
    "\n",
    "llama_training_args = TrainingArguments(\n",
    "    output_dir=\"./outputs\",\n",
    "    model_max_length=512,\n",
    "    inf_latent_iterations=6,\n",
    "    use_prj=True,\n",
    "    prj_dim=2048,\n",
    "    remove_eos=True,\n",
    "    greedy=True,\n",
    "    bf16=False,\n",
    "    inf_num_iterations=1\n",
    ")\n",
    "\n",
    "llama_lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=llama_model_args.lora_r,\n",
    "    lora_alpha=llama_model_args.lora_alpha,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    init_lora_weights=True,\n",
    ")\n",
    "\n",
    "# Initialize the CODI model\n",
    "llama_model = CODI(llama_model_args, llama_training_args, llama_lora_config)\n",
    "\n",
    "# Download and load weights from HuggingFace Hub\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "print(f\"Downloading weights from {llama_model_name}...\")\n",
    "checkpoint_path = hf_hub_download(\n",
    "    repo_id=llama_model_name,\n",
    "    filename=\"pytorch_model.bin\",\n",
    "    token=hf_token if hf_token else None\n",
    ")\n",
    "\n",
    "print(f\"Loading weights from {checkpoint_path}...\")\n",
    "state_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "llama_model.load_state_dict(state_dict, strict=False)\n",
    "llama_model.codi.tie_weights()\n",
    "\n",
    "llama_model = llama_model.to(device)\n",
    "llama_model = llama_model.to(torch.bfloat16)\n",
    "llama_model.eval()\n",
    "\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "print(\"✓ CODI-LLaMA loaded successfully from HuggingFace Hub\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Checkpoint NOT found at: /workspace/CoT_Exploration/models/CODI-llama3.2-1b/pytorch_model.bin\n",
      "\n",
      "Checking directory contents:\n",
      "Directory doesn't exist: /workspace/CoT_Exploration/models/CODI-llama3.2-1b\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_path = \"/workspace/CoT_Exploration/models/CODI-llama3.2-1b/pytorch_model.bin\"\n",
    "if os.path.exists(checkpoint_path):\n",
    "    size_mb = os.path.getsize(checkpoint_path) / (1024 * 1024)\n",
    "    print(f\"✓ Checkpoint found: {size_mb:.1f} MB\")\n",
    "else:\n",
    "    print(f\"❌ Checkpoint NOT found at: {checkpoint_path}\")\n",
    "    print(f\"\\nChecking directory contents:\")\n",
    "    dir_path = os.path.dirname(checkpoint_path)\n",
    "    if os.path.exists(dir_path):\n",
    "        files = os.listdir(dir_path)\n",
    "        print(f\"Files in {dir_path}:\")\n",
    "        for f in files:\n",
    "            print(f\"  - {f}\")\n",
    "    else:\n",
    "        print(f\"Directory doesn't exist: {dir_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if CODI weights are loaded:\n",
      "Model checkpoint dir: None\n",
      "BoT token ID: 128257\n",
      "EoT token ID: 128258\n",
      "Using projection: True\n",
      "Projection dim: 2048\n",
      "✓ Projection layer found: Sequential(\n",
      "  (0): Dropout(p=0.0, inplace=False)\n",
      "  (1): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (2): GELU(approximate='none')\n",
      "  (3): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (ln): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Verify CODI weights are loaded\n",
    "print(\"Checking if CODI weights are loaded:\")\n",
    "print(f\"Model checkpoint dir: {llama_model_args.ckpt_dir}\")\n",
    "print(f\"BoT token ID: {llama_model.bot_id}\")\n",
    "print(f\"EoT token ID: {llama_model.eot_id}\")\n",
    "print(f\"Using projection: {llama_training_args.use_prj}\")\n",
    "print(f\"Projection dim: {llama_training_args.prj_dim if llama_training_args.use_prj else 'N/A'}\")\n",
    "\n",
    "# Check if projection layer exists\n",
    "if hasattr(llama_model, 'prj'):\n",
    "    print(f\"✓ Projection layer found: {llama_model.prj}\")\n",
    "else:\n",
    "    print(\"✗ No projection layer - this might be the problem!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LLaMA Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "question2 = \"Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?\"\n",
    "answer2 = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LLaMA: Running inference with Chain-of-Thought\n",
      "================================================================================\n",
      "WITH BoT - Input IDs: tensor([[128000,  18820,    295,    596,  78878,  11203,    220,    845,  19335,\n",
      "            824,   1938,     13,   3005,  50777,   2380,    369,  17954,   1475,\n",
      "           6693,    323,    293,   2094,  55404,   1354,    369,   1077,   4885,\n",
      "           1475,   1938,    449,   3116,     13,   3005,  31878,    279,  27410,\n",
      "            520,    279,  20957,      6,   3157,   7446,    369,    400,     17,\n",
      "            824,   7878,  37085,  19151,     13,   2650,   1790,    304,  11441,\n",
      "           1587,   1364,   1304,   1475,   1938,    520,    279,  20957,      6,\n",
      "           3157,     30, 128257]], device='cuda:0')\n",
      "WITH BoT - Last 5 token IDs: [20957, 6, 3157, 30, 128257]\n",
      "WITH BoT - BoT token ID should be: 128257\n",
      "WITH BoT - Last token is BoT?: True\n",
      "Input shape: torch.Size([1, 66])\n",
      "✓ Position 0 (BoT): shape=torch.Size([1, 1, 2048])\n",
      "✓ Position 1 (T1): shape=torch.Size([1, 1, 2048])\n",
      "✓ Position 2 (T2): shape=torch.Size([1, 1, 2048])\n",
      "✓ Position 3 (T3): shape=torch.Size([1, 1, 2048])\n",
      "✓ Position 4 (T4): shape=torch.Size([1, 1, 2048])\n",
      "✓ Position 5 (T5): shape=torch.Size([1, 1, 2048])\n",
      "✓ Position 6 (T6): shape=torch.Size([1, 1, 2048])\n",
      "\n",
      "Continuous thoughts shape: torch.Size([1, 7, 2048])\n",
      "Number of thought positions: 7\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LLaMA: Running inference with Chain-of-Thought\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 1\n",
    "\n",
    "question = question\n",
    "questions = [question]\n",
    "\n",
    "# Tokenize with BoT token\n",
    "if llama_training_args.remove_eos:\n",
    "    bot_tensor = torch.tensor([llama_model.bot_id], dtype=torch.long).expand(batch_size, 1).to(device)\n",
    "else:\n",
    "    bot_tensor = torch.tensor([llama_tokenizer.eos_token_id, llama_model.bot_id], \n",
    "                              dtype=torch.long).expand(batch_size, 2).to(device)\n",
    "\n",
    "inputs = llama_tokenizer(questions, return_tensors=\"pt\", padding=False)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "inputs[\"input_ids\"] = torch.cat((inputs[\"input_ids\"], bot_tensor), dim=1)\n",
    "inputs[\"attention_mask\"] = torch.cat((inputs[\"attention_mask\"], torch.ones_like(bot_tensor)), dim=1)\n",
    "\n",
    "print(f\"WITH BoT - Input IDs: {inputs['input_ids']}\")\n",
    "print(f\"WITH BoT - Last 5 token IDs: {inputs['input_ids'][0, -5:].tolist()}\")\n",
    "print(f\"WITH BoT - BoT token ID should be: {llama_model.bot_id}\")\n",
    "print(f\"WITH BoT - Last token is BoT?: {inputs['input_ids'][0, -1].item() == llama_model.bot_id}\")\n",
    "\n",
    "\n",
    "print(f\"Input shape: {inputs['input_ids'].shape}\")\n",
    "\n",
    "# Store only the 7 chain-of-thought hidden states\n",
    "cot_hidden_states = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Initial encoding (position 0: BoT)\n",
    "    past_key_values = None\n",
    "    outputs = llama_model.codi(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        use_cache=True,\n",
    "        output_hidden_states=True,\n",
    "        past_key_values=past_key_values,\n",
    "        attention_mask=inputs[\"attention_mask\"]\n",
    "    )\n",
    "    past_key_values = outputs.past_key_values\n",
    "    \n",
    "    # 🔥 CRITICAL: Only take the LAST position, not all positions\n",
    "    latent_embd = outputs.hidden_states[-1][:, -1:, :]  # Shape: [batch, 1, hidden_dim]\n",
    "    \n",
    "    # Store BoT position (BEFORE projection)\n",
    "    cot_hidden_states.append(latent_embd.clone())\n",
    "    print(f\"✓ Position 0 (BoT): shape={latent_embd.shape}\")\n",
    "    \n",
    "    # Apply initial projection\n",
    "    if llama_training_args.use_prj:\n",
    "        latent_embd = llama_model.prj(latent_embd)\n",
    "    \n",
    "    # Chain-of-Thought iterations (positions 1-6)\n",
    "    for i in range(llama_training_args.inf_latent_iterations):\n",
    "        outputs = llama_model.codi(\n",
    "            inputs_embeds=latent_embd,\n",
    "            use_cache=True,\n",
    "            output_hidden_states=True,\n",
    "            past_key_values=past_key_values\n",
    "        )\n",
    "        past_key_values = outputs.past_key_values\n",
    "        \n",
    "        # 🔥 CRITICAL: Only take the LAST position\n",
    "        latent_embd = outputs.hidden_states[-1][:, -1:, :]  # Shape: [batch, 1, hidden_dim]\n",
    "        \n",
    "        # Store BEFORE projection\n",
    "        cot_hidden_states.append(latent_embd.clone())\n",
    "        print(f\"✓ Position {i+1} (T{i+1}): shape={latent_embd.shape}\")\n",
    "        \n",
    "        # Apply projection for next iteration\n",
    "        if llama_training_args.use_prj:\n",
    "            latent_embd = llama_model.prj(latent_embd)\n",
    "\n",
    "# Stack all 7 positions\n",
    "llama_continuous_thoughts = torch.cat(cot_hidden_states, dim=1)\n",
    "print(f\"\\nContinuous thoughts shape: {llama_continuous_thoughts.shape}\")\n",
    "print(f\"Number of thought positions: {llama_continuous_thoughts.shape[1]}\")\n",
    "\n",
    "\n",
    "\n",
    "# Also create llama_latent_embd as alias for compatibility with other cells\n",
    "llama_latent_embd = llama_continuous_thoughts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ SUCCESS: Collected exactly 7 CoT positions!\n"
     ]
    }
   ],
   "source": [
    "# Verify we have exactly 7 positions\n",
    "if llama_continuous_thoughts.shape[1] != 7:\n",
    "    print(f\"\\n❌ ERROR: Expected 7 positions, got {llama_continuous_thoughts.shape[1]}\")\n",
    "else:\n",
    "    print(f\"✓ SUCCESS: Collected exactly 7 CoT positions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LLaMA: Running inference with Custom-length Chain-of-Thought\n",
      "================================================================================\n",
      "WITH BoT - Input IDs: tensor([[128000,  18820,    295,    596,  78878,  11203,    220,    845,  19335,\n",
      "            824,   1938,     13,   3005,  50777,   2380,    369,  17954,   1475,\n",
      "           6693,    323,    293,   2094,  55404,   1354,    369,   1077,   4885,\n",
      "           1475,   1938,    449,   3116,     13,   3005,  31878,    279,  27410,\n",
      "            520,    279,  20957,      6,   3157,   7446,    369,    400,     17,\n",
      "            824,   7878,  37085,  19151,     13,   2650,   1790,    304,  11441,\n",
      "           1587,   1364,   1304,   1475,   1938,    520,    279,  20957,      6,\n",
      "           3157,     30, 128257]], device='cuda:0')\n",
      "WITH BoT - Last 5 token IDs: [20957, 6, 3157, 30, 128257]\n",
      "WITH BoT - BoT token ID should be: 128257\n",
      "WITH BoT - Last token is BoT?: True\n",
      "Input shape: torch.Size([1, 66])\n",
      "Question: Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "Trying with CoT length of 2\n",
      "✓ Position 0 (BoT): shape=torch.Size([1, 1, 2048])\n",
      "✓ Position 1 (T1): shape=torch.Size([1, 1, 2048])\n",
      "✓ Position 2 (T2): shape=torch.Size([1, 1, 2048])\n",
      "\n",
      "Continuous thoughts shape: torch.Size([1, 3, 2048])\n",
      "Number of thought positions: 3\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "\n",
    "cot_length=2\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LLaMA: Running inference with Custom-length Chain-of-Thought\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 1\n",
    "\n",
    "question = question\n",
    "questions = [question]\n",
    "\n",
    "# Tokenize with BoT token\n",
    "if llama_training_args.remove_eos:\n",
    "    bot_tensor = torch.tensor([llama_model.bot_id], dtype=torch.long).expand(batch_size, 1).to(device)\n",
    "else:\n",
    "    bot_tensor = torch.tensor([llama_tokenizer.eos_token_id, llama_model.bot_id], \n",
    "                              dtype=torch.long).expand(batch_size, 2).to(device)\n",
    "\n",
    "inputs = llama_tokenizer(questions, return_tensors=\"pt\", padding=False)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "inputs[\"input_ids\"] = torch.cat((inputs[\"input_ids\"], bot_tensor), dim=1)\n",
    "inputs[\"attention_mask\"] = torch.cat((inputs[\"attention_mask\"], torch.ones_like(bot_tensor)), dim=1)\n",
    "\n",
    "print(f\"WITH BoT - Input IDs: {inputs['input_ids']}\")\n",
    "print(f\"WITH BoT - Last 5 token IDs: {inputs['input_ids'][0, -5:].tolist()}\")\n",
    "print(f\"WITH BoT - BoT token ID should be: {llama_model.bot_id}\")\n",
    "print(f\"WITH BoT - Last token is BoT?: {inputs['input_ids'][0, -1].item() == llama_model.bot_id}\")\n",
    "\n",
    "\n",
    "print(f\"Input shape: {inputs['input_ids'].shape}\")\n",
    "\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Trying with CoT length of {cot_length}\")\n",
    "\n",
    "cot_hidden_states = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Initial encoding (position 0: BoT)\n",
    "    past_key_values = None\n",
    "    outputs = llama_model.codi(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        use_cache=True,\n",
    "        output_hidden_states=True,\n",
    "        past_key_values=past_key_values,\n",
    "        attention_mask=inputs[\"attention_mask\"]\n",
    "    )\n",
    "    past_key_values = outputs.past_key_values\n",
    "    \n",
    "    # 🔥 CRITICAL: Only take the LAST position, not all positions\n",
    "    latent_embd = outputs.hidden_states[-1][:, -1:, :]  # Shape: [batch, 1, hidden_dim]\n",
    "    \n",
    "    # Store BoT position (BEFORE projection)\n",
    "    cot_hidden_states.append(latent_embd.clone())\n",
    "    print(f\"✓ Position 0 (BoT): shape={latent_embd.shape}\")\n",
    "    \n",
    "    # Apply initial projection\n",
    "    if llama_training_args.use_prj:\n",
    "        latent_embd = llama_model.prj(latent_embd)\n",
    "    \n",
    "    # Chain-of-Thought iterations (positions 1-6)\n",
    "    for i in range(cot_length):\n",
    "        outputs = llama_model.codi(\n",
    "            inputs_embeds=latent_embd,\n",
    "            use_cache=True,\n",
    "            output_hidden_states=True,\n",
    "            past_key_values=past_key_values\n",
    "        )\n",
    "        past_key_values = outputs.past_key_values\n",
    "        \n",
    "        # 🔥 CRITICAL: Only take the LAST position\n",
    "        latent_embd = outputs.hidden_states[-1][:, -1:, :]  # Shape: [batch, 1, hidden_dim]\n",
    "        \n",
    "        # Store BEFORE projection\n",
    "        cot_hidden_states.append(latent_embd.clone())\n",
    "        print(f\"✓ Position {i+1} (T{i+1}): shape={latent_embd.shape}\")\n",
    "        \n",
    "        # Apply projection for next iteration\n",
    "        if llama_training_args.use_prj:\n",
    "            latent_embd = llama_model.prj(latent_embd)\n",
    "\n",
    "# Stack all positions\n",
    "llama_continuous_thoughts = torch.cat(cot_hidden_states, dim=1)\n",
    "print(f\"\\nContinuous thoughts shape: {llama_continuous_thoughts.shape}\")\n",
    "print(f\"Number of thought positions: {llama_continuous_thoughts.shape[1]}\")\n",
    "\n",
    "\n",
    "\n",
    "# Also create llama_latent_embd as alias for compatibility with other cells\n",
    "llama_latent_embd = llama_continuous_thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode LLaMA Continuous Thoughts (like section5_analysis.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LLaMA: Decoding Continuous Thoughts\n",
      "================================================================================\n",
      "Continuous thoughts shape: torch.Size([1, 3, 2048])\n",
      "\n",
      "Decoded tokens from continuous thoughts:\n",
      "  BoT  [pos= 0]: token_id=   24 → '9' ← NUMBER!\n",
      "  T1   [pos= 1]: token_id=   22 → '7' ← NUMBER!\n",
      "  T2   [pos= 2]: token_id=   10 → '+'\n",
      "\n",
      "✓ Numbers detected at positions: [0, 1]\n",
      "✓ Total numbers: 2\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LLaMA: Decoding Continuous Thoughts\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Note: HuggingFace models don't have projection layers, decode directly\n",
    "print(f\"Continuous thoughts shape: {llama_continuous_thoughts.shape}\")\n",
    "\n",
    "# Decode first positions directly from continuous thoughts\n",
    "print(\"\\nDecoded tokens from continuous thoughts:\")\n",
    "llama_decoded_tokens = []\n",
    "llama_number_positions = []\n",
    "\n",
    "for i in range(min(50, llama_continuous_thoughts.shape[1])):\n",
    "    logits = llama_model.codi.lm_head(llama_continuous_thoughts[:, i, :])\n",
    "    top1_token_id = torch.argmax(logits, dim=-1).item()\n",
    "    top1_token_str = llama_tokenizer.decode([top1_token_id])\n",
    "\n",
    "    is_number = bool(number_regex.match(top1_token_str))\n",
    "    pos_type = \"BoT\" if i == 0 else f\"T{i}\"\n",
    "\n",
    "    llama_decoded_tokens.append((i, top1_token_id, top1_token_str, is_number))\n",
    "    if is_number:\n",
    "        llama_number_positions.append(i)\n",
    "\n",
    "    marker = \" ← NUMBER!\" if is_number else \"\"\n",
    "    print(f\"  {pos_type:4s} [pos={i:2d}]: token_id={top1_token_id:5d} → '{top1_token_str}'{marker}\")\n",
    "\n",
    "print(f\"\\n✓ Numbers detected at positions: {llama_number_positions}\")\n",
    "print(f\"✓ Total numbers: {len(llama_number_positions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LLaMA: Generate Final Answer\n",
      "================================================================================\n",
      "Stopped at step 49 (max length)\n",
      "\n",
      "Full decoded answer: The answer is: 18The answer is: 36The answer is: 36The answer is: 36The answer is: 72The answer is: 72The answer is: 144The\n",
      "\n",
      "Extracted numerical answer: 18.0\n",
      "Expected answer: 18\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LLaMA: Generate Final Answer\")\n",
    "print(\"=\"*80)\n",
    "with torch.no_grad():\n",
    "    # Signal end-of-thought with EOT token\n",
    "    if llama_training_args.remove_eos:\n",
    "        eot_tensor = torch.tensor([[llama_model.eot_id]], dtype=torch.long).expand(batch_size, 1, 1).to(device)\n",
    "    else:\n",
    "        eot_tensor = torch.tensor([[llama_tokenizer.eos_token_id, llama_model.eot_id]], \n",
    "                                   dtype=torch.long).expand(batch_size, 1, 2).to(device)\n",
    "    \n",
    "    eot_emb = llama_model.get_embd(llama_model.codi, llama_model.model_name)(eot_tensor).squeeze(1)\n",
    "    output = eot_emb\n",
    "    \n",
    "    # Generate answer tokens autoregressively\n",
    "    full_answer = None\n",
    "    pred_tokens = []\n",
    "    max_length = 256\n",
    "    \n",
    "    for step in range(max_length):\n",
    "        out = llama_model.codi(\n",
    "            inputs_embeds=output,\n",
    "            output_hidden_states=False,\n",
    "            attention_mask=None,\n",
    "            use_cache=True,\n",
    "            output_attentions=False,\n",
    "            past_key_values=past_key_values\n",
    "        )\n",
    "        past_key_values = out.past_key_values\n",
    "        \n",
    "        # Get logits for vocabulary tokens only\n",
    "        logits = out.logits[:, -1, :llama_model.codi.config.vocab_size-1]\n",
    "        next_token_id = torch.argmax(logits, dim=-1).item()\n",
    "        \n",
    "        pred_tokens.append(next_token_id)\n",
    "        \n",
    "        # Decode current token\n",
    "        current_token_str = llama_tokenizer.decode([next_token_id])\n",
    "        \n",
    "        # Stop if EOS token\n",
    "        if next_token_id == llama_tokenizer.eos_token_id:\n",
    "            print(f\"Stopped at step {step} (EOS token)\")\n",
    "            break\n",
    "        \n",
    "        # Stop immediately after generating a number token\n",
    "        #if number_regex.match(current_token_str.strip()):\n",
    "        #    print(f\"Stopped at step {step} (found number: '{current_token_str}')\")\n",
    "        #    full_answer=current_token_str\n",
    "        #    break\n",
    "        \n",
    "        # Hard limit to prevent loops\n",
    "        if step >= 49:\n",
    "            print(f\"Stopped at step {step} (max length)\")\n",
    "            break\n",
    "        \n",
    "        # Prepare next input\n",
    "        output = llama_model.get_embd(llama_model.codi, llama_model.model_name)(\n",
    "            torch.tensor([[next_token_id]], device=device)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # Extract numerical answer\n",
    "    def extract_answer_number(text):\n",
    "        text = text.replace(',', '')\n",
    "        numbers = [s for s in re.findall(r'-?\\d+\\.?\\d*', text)]\n",
    "        if not numbers:\n",
    "            return None\n",
    "        return float(numbers[0])\n",
    "\n",
    "    # Decode the full answer from pred_tokens\n",
    "    full_answer = llama_tokenizer.decode(pred_tokens, skip_special_tokens=True)\n",
    "    print(f\"\\nFull decoded answer: {full_answer}\")\n",
    "    \n",
    "    predicted_number = extract_answer_number(full_answer)\n",
    "    print(f\"\\nExtracted numerical answer: {predicted_number}\")\n",
    "    print(f\"Expected answer: {answer}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, re\n",
    "\n",
    "def run_continuous_cot_codi_llama(\n",
    "    llama_model,\n",
    "    llama_tokenizer,\n",
    "    llama_training_args,\n",
    "    question: str,\n",
    "    answer: str,\n",
    "    cot_length: int = 2,\n",
    "    device: str = None,\n",
    "    max_answer_len: int = 256,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run CODI-LLaMA continuous chain-of-thought inference, decoding, and evaluation\n",
    "    exactly as implemented in the notebook.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    llama_model : model with .codi, .bot_id, .eot_id, .prj, .get_embd, .model_name\n",
    "    llama_tokenizer : tokenizer matching model\n",
    "    llama_training_args : object with .remove_eos and .use_prj attributes\n",
    "    question : str\n",
    "    answer : str (reference numerical or textual answer)\n",
    "    cot_length : int, number of CoT latent iterations\n",
    "    device : torch.device or 'cuda'/'cpu' (auto if None)\n",
    "    max_answer_len : int, maximum generated answer tokens\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict containing:\n",
    "        - continuous_thoughts : torch.Tensor of latent states\n",
    "        - decoded_tokens : list of (pos, token_id, token_str, is_number)\n",
    "        - full_answer : str\n",
    "        - predicted_number : float or None\n",
    "        - match : bool or None\n",
    "    \"\"\"\n",
    "    number_regex = re.compile(r\"^-?\\d+\\.?\\d*$\")\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"LLaMA: Running inference with Custom-length Chain-of-Thought\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    else:\n",
    "        device = torch.device(device)\n",
    "\n",
    "    batch_size = 1\n",
    "    questions = [question]\n",
    "\n",
    "    # --- Build input with BoT token(s)\n",
    "    if llama_training_args.remove_eos:\n",
    "        bot_tensor = torch.tensor([llama_model.bot_id], dtype=torch.long).expand(batch_size, 1).to(device)\n",
    "    else:\n",
    "        bot_tensor = torch.tensor([llama_tokenizer.eos_token_id, llama_model.bot_id],\n",
    "                                  dtype=torch.long).expand(batch_size, 2).to(device)\n",
    "\n",
    "    inputs = llama_tokenizer(questions, return_tensors=\"pt\", padding=False)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    inputs[\"input_ids\"] = torch.cat((inputs[\"input_ids\"], bot_tensor), dim=1)\n",
    "    inputs[\"attention_mask\"] = torch.cat((inputs[\"attention_mask\"], torch.ones_like(bot_tensor)), dim=1)\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Trying with CoT length of {cot_length}\")\n",
    "\n",
    "    cot_hidden_states = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        past_key_values = None\n",
    "        outputs = llama_model.codi(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            use_cache=True,\n",
    "            output_hidden_states=True,\n",
    "            past_key_values=past_key_values,\n",
    "            attention_mask=inputs[\"attention_mask\"]\n",
    "        )\n",
    "        past_key_values = outputs.past_key_values\n",
    "        latent_embd = outputs.hidden_states[-1][:, -1:, :]  # BoT position\n",
    "        cot_hidden_states.append(latent_embd.clone())\n",
    "        print(f\"✓ Position 0 (BoT): shape={latent_embd.shape}\")\n",
    "\n",
    "        if llama_training_args.use_prj:\n",
    "            latent_embd = llama_model.prj(latent_embd)\n",
    "\n",
    "        # Chain of Thought\n",
    "        for i in range(cot_length):\n",
    "            outputs = llama_model.codi(\n",
    "                inputs_embeds=latent_embd,\n",
    "                use_cache=True,\n",
    "                output_hidden_states=True,\n",
    "                past_key_values=past_key_values\n",
    "            )\n",
    "            past_key_values = outputs.past_key_values\n",
    "            latent_embd = outputs.hidden_states[-1][:, -1:, :]\n",
    "            cot_hidden_states.append(latent_embd.clone())\n",
    "            print(f\"✓ Position {i+1} (T{i+1}): shape={latent_embd.shape}\")\n",
    "\n",
    "            if llama_training_args.use_prj:\n",
    "                latent_embd = llama_model.prj(latent_embd)\n",
    "\n",
    "    llama_continuous_thoughts = torch.cat(cot_hidden_states, dim=1)\n",
    "    print(f\"\\nContinuous thoughts shape: {llama_continuous_thoughts.shape}\")\n",
    "\n",
    "    # --- Decode Continuous Thoughts\n",
    "    print(\"=\" * 80)\n",
    "    print(\"LLaMA: Decoding Continuous Thoughts\")\n",
    "    print(\"=\" * 80)\n",
    "    llama_decoded_tokens = []\n",
    "    llama_number_positions = []\n",
    "\n",
    "    for i in range(llama_continuous_thoughts.shape[1]):\n",
    "        logits = llama_model.codi.lm_head(llama_continuous_thoughts[:, i, :])\n",
    "        top1_token_id = torch.argmax(logits, dim=-1).item()\n",
    "        top1_token_str = llama_tokenizer.decode([top1_token_id])\n",
    "        is_number = bool(number_regex.match(top1_token_str.strip()))\n",
    "        pos_type = \"BoT\" if i == 0 else f\"T{i}\"\n",
    "        llama_decoded_tokens.append((i, top1_token_id, top1_token_str, is_number))\n",
    "        if is_number:\n",
    "            llama_number_positions.append(i)\n",
    "        marker = \" ← NUMBER!\" if is_number else \"\"\n",
    "        print(f\"  {pos_type:4s} [pos={i:2d}]: token_id={top1_token_id:5d} → '{top1_token_str}'{marker}\")\n",
    "\n",
    "    print(f\"\\n✓ Numbers detected at positions: {llama_number_positions}\")\n",
    "    print(f\"✓ Total numbers: {len(llama_number_positions)}\")\n",
    "\n",
    "    # --- Generate Final Answer\n",
    "    print(\"=\" * 80)\n",
    "    print(\"LLaMA: Generate Final Answer\")\n",
    "    print(\"=\" * 80)\n",
    "    with torch.no_grad():\n",
    "        if llama_training_args.remove_eos:\n",
    "            eot_tensor = torch.tensor([[llama_model.eot_id]], dtype=torch.long).expand(batch_size, 1, 1).to(device)\n",
    "        else:\n",
    "            eot_tensor = torch.tensor([[llama_tokenizer.eos_token_id, llama_model.eot_id]],\n",
    "                                      dtype=torch.long).expand(batch_size, 1, 2).to(device)\n",
    "        eot_emb = llama_model.get_embd(llama_model.codi, llama_model.model_name)(eot_tensor).squeeze(1)\n",
    "        output = eot_emb\n",
    "\n",
    "        full_answer = \"\"\n",
    "        pred_tokens = []\n",
    "\n",
    "        for step in range(max_answer_len):\n",
    "            out = llama_model.codi(\n",
    "                inputs_embeds=output,\n",
    "                output_hidden_states=False,\n",
    "                use_cache=True,\n",
    "                output_attentions=False,\n",
    "                past_key_values=past_key_values\n",
    "            )\n",
    "            past_key_values = out.past_key_values\n",
    "            logits = out.logits[:, -1, :llama_model.codi.config.vocab_size - 1]\n",
    "            next_token_id = torch.argmax(logits, dim=-1).item()\n",
    "            pred_tokens.append(next_token_id)\n",
    "            tok_str = llama_tokenizer.decode([next_token_id])\n",
    "            if next_token_id == llama_tokenizer.eos_token_id:\n",
    "                print(f\"Stopped at step {step} (EOS token)\")\n",
    "                break\n",
    "            if step >= 49:\n",
    "                print(f\"Stopped at step {step} (max length)\")\n",
    "                break\n",
    "            output = llama_model.get_embd(llama_model.codi, llama_model.model_name)(\n",
    "                torch.tensor([[next_token_id]], device=device)\n",
    "            )\n",
    "\n",
    "        # decode\n",
    "        full_answer = llama_tokenizer.decode(pred_tokens, skip_special_tokens=True)\n",
    "        print(f\"\\nFull decoded answer: {full_answer}\")\n",
    "\n",
    "        def extract_answer_number(text):\n",
    "            text = text.replace(',', '')\n",
    "            numbers = re.findall(r'-?\\d+\\.?\\d*', text)\n",
    "            return float(numbers[0]) if numbers else None\n",
    "\n",
    "        predicted_number = extract_answer_number(full_answer)\n",
    "        print(f\"\\nExtracted numerical answer: {predicted_number}\")\n",
    "        print(f\"Expected answer: \\n{answer}\")\n",
    "\n",
    "    # --- Evaluate\n",
    "    match = None\n",
    "    if predicted_number is not None:\n",
    "        try:\n",
    "            ref = float(answer)\n",
    "            match = abs(predicted_number - ref) < 1e-6\n",
    "        except ValueError:\n",
    "            match = (full_answer.strip() == str(answer).strip())\n",
    "\n",
    "    return {\n",
    "        \"continuous_thoughts\": llama_continuous_thoughts,\n",
    "        \"decoded_tokens\": llama_decoded_tokens,\n",
    "        \"full_answer\": full_answer,\n",
    "        \"predicted_number\": predicted_number,\n",
    "        \"reference_answer\": answer,\n",
    "        \"match\": match,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LLaMA: Running inference with Custom-length Chain-of-Thought\n",
      "================================================================================\n",
      "Question: Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "Trying with CoT length of 1\n",
      "✓ Position 0 (BoT): shape=torch.Size([1, 1, 2048])\n",
      "✓ Position 1 (T1): shape=torch.Size([1, 1, 2048])\n",
      "\n",
      "Continuous thoughts shape: torch.Size([1, 2, 2048])\n",
      "================================================================================\n",
      "LLaMA: Decoding Continuous Thoughts\n",
      "================================================================================\n",
      "  BoT  [pos= 0]: token_id=   24 → '9' ← NUMBER!\n",
      "  T1   [pos= 1]: token_id=   22 → '7' ← NUMBER!\n",
      "\n",
      "✓ Numbers detected at positions: [0, 1]\n",
      "✓ Total numbers: 2\n",
      "================================================================================\n",
      "LLaMA: Generate Final Answer\n",
      "================================================================================\n",
      "Stopped at step 49 (max length)\n",
      "\n",
      "Full decoded answer: The answer is: 18The answer is: 36The answer is: 36The answer is: 36The answer is: 72The answer is: 72The answer is: 144The\n",
      "\n",
      "Extracted numerical answer: 18.0\n",
      "Expected answer: \n",
      "18\n",
      "{'continuous_thoughts': tensor([[[-0.1953,  2.3906, -0.5430,  ...,  1.5703, -2.6562, -0.9023],\n",
      "         [-2.0469, -1.4766, -2.2031,  ...,  0.9648,  2.3438,  3.2188]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16), 'decoded_tokens': [(0, 24, '9', True), (1, 22, '7', True)], 'full_answer': 'The answer is: 18The answer is: 36The answer is: 36The answer is: 36The answer is: 72The answer is: 72The answer is: 144The', 'predicted_number': 18.0, 'reference_answer': 18, 'match': True}\n"
     ]
    }
   ],
   "source": [
    "# Example usage (uncomment + adapt inside your notebook):\n",
    "res = run_continuous_cot_codi_llama(\n",
    "    llama_model=llama_model,\n",
    "    llama_tokenizer=llama_tokenizer,\n",
    "    llama_training_args=llama_training_args,\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    #question=\"Q: 12 + 5 = ?\",\n",
    "    #answer=\"17\",\n",
    "    cot_length=1,\n",
    ")\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LLaMA: Running inference with Custom-length Chain-of-Thought\n",
      "================================================================================\n",
      "Question: Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "Trying with CoT length of 0\n",
      "✓ Position 0 (BoT): shape=torch.Size([1, 1, 2048])\n",
      "\n",
      "Continuous thoughts shape: torch.Size([1, 1, 2048])\n",
      "================================================================================\n",
      "LLaMA: Decoding Continuous Thoughts\n",
      "================================================================================\n",
      "  BoT  [pos= 0]: token_id=   24 → '9' ← NUMBER!\n",
      "\n",
      "✓ Numbers detected at positions: [0]\n",
      "✓ Total numbers: 1\n",
      "================================================================================\n",
      "LLaMA: Generate Final Answer\n",
      "================================================================================\n",
      "Stopped at step 49 (max length)\n",
      "\n",
      "Full decoded answer: 9/4=2.25>> <<16-3-2.25=10.75>>The answer is: 17The answer is: 34The answer is: 34The answer is: 68The\n",
      "\n",
      "Extracted numerical answer: 9.0\n",
      "Expected answer: \n",
      "18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'continuous_thoughts': tensor([[[-0.1953,  2.3906, -0.5430,  ...,  1.5703, -2.6562, -0.9023]]],\n",
       "        device='cuda:0', dtype=torch.bfloat16),\n",
       " 'decoded_tokens': [(0, 24, '9', True)],\n",
       " 'full_answer': '9/4=2.25>> <<16-3-2.25=10.75>>The answer is: 17The answer is: 34The answer is: 34The answer is: 68The',\n",
       " 'predicted_number': 9.0,\n",
       " 'reference_answer': 18,\n",
       " 'match': False}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_continuous_cot_codi_llama(\n",
    "    llama_model=llama_model,\n",
    "    llama_tokenizer=llama_tokenizer,\n",
    "    llama_training_args=llama_training_args,\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    #question=\"Q: 12 + 5 = ?\",\n",
    "    #answer=\"17\",\n",
    "    cot_length=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_continuous_cot_codi_llama_intervene(\n",
    "    question: str,\n",
    "    answer: str,\n",
    "    cot_length: int = 6,\n",
    "    bot: bool = True,\n",
    "    target_token: str = None,\n",
    "    k: float = 1.0,\n",
    "    device=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Run CODI chain-of-thought on a preloaded CODI-LLaMA model, optionally with\n",
    "    causal projection intervention (replace activations with target token embedding).\n",
    "\n",
    "    Args:\n",
    "        question (str): The input question for inference.\n",
    "        answer (str): Reference (expected) answer for comparison.\n",
    "        cot_length (int): Number of CoT latent iterations.\n",
    "        bot (bool): Whether to include BoT token at the end of input.\n",
    "        target_token (str, optional): If given, perform causal intervention\n",
    "            by replacing projection of number-token activations with the embedding\n",
    "            of this token.\n",
    "        k (float): Scaling strength for the projection replacement.\n",
    "        device: torch.device to use (GPU or CPU).\n",
    "\n",
    "    Returns:\n",
    "        dict with decoded intermediate tokens, generated answer, and reference comparison.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"LLaMA: Running Continuous Chain-of-Thought\" +\n",
    "          (\" with CAUSAL INTERVENTION\" if target_token else \"\"))\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    batch_size = 1\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    number_regex = re.compile(r\"^-?\\d+\\.?\\d*$\")\n",
    "\n",
    "    # Prepare question input\n",
    "    questions = [question]\n",
    "    if bot:\n",
    "        if llama_training_args.remove_eos:\n",
    "            bot_tensor = torch.tensor([llama_model.bot_id], dtype=torch.long).expand(batch_size, 1).to(device)\n",
    "        else:\n",
    "            bot_tensor = torch.tensor(\n",
    "                [llama_tokenizer.eos_token_id, llama_model.bot_id],\n",
    "                dtype=torch.long\n",
    "            ).expand(batch_size, 2).to(device)\n",
    "    else:\n",
    "        bot_tensor = None\n",
    "\n",
    "    inputs = llama_tokenizer(questions, return_tensors=\"pt\", padding=False)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    if bot:\n",
    "        inputs[\"input_ids\"] = torch.cat((inputs[\"input_ids\"], bot_tensor), dim=1)\n",
    "        inputs[\"attention_mask\"] = torch.cat(\n",
    "            (inputs[\"attention_mask\"], torch.ones_like(bot_tensor)), dim=1\n",
    "        )\n",
    "\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"CoT length: {cot_length}\")\n",
    "\n",
    "    # Prepare intervention\n",
    "    intervention_enabled = target_token is not None\n",
    "    if intervention_enabled:\n",
    "        target_token_id = llama_tokenizer.encode(target_token, add_special_tokens=False)[0]\n",
    "        embedding_layer = llama_model.codi.get_input_embeddings()\n",
    "        target_embd = embedding_layer(torch.tensor([target_token_id], device=device))\n",
    "        print(f\"\\nCAUSAL INTERVENTION enabled → target_token='{target_token}', k={k}\")\n",
    "    else:\n",
    "        print(\"\\nRunning baseline CoT (no intervention).\")\n",
    "\n",
    "    # Run chain-of-thought\n",
    "    cot_hidden_states = []\n",
    "    llama_interventions = []\n",
    "    intervened_positions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        past_key_values = None\n",
    "        outputs = llama_model.codi(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            use_cache=True,\n",
    "            output_hidden_states=True,\n",
    "            attention_mask=inputs[\"attention_mask\"]\n",
    "        )\n",
    "        past_key_values = outputs.past_key_values\n",
    "        latent_embd = outputs.hidden_states[-1][:, -1:, :]\n",
    "        cot_hidden_states.append(latent_embd.clone())\n",
    "\n",
    "        if llama_training_args.use_prj:\n",
    "            latent_embd = llama_model.prj(latent_embd)\n",
    "\n",
    "        for i in range(cot_length):\n",
    "            outputs = llama_model.codi(\n",
    "                inputs_embeds=latent_embd,\n",
    "                use_cache=True,\n",
    "                output_hidden_states=True,\n",
    "                past_key_values=past_key_values\n",
    "            )\n",
    "            past_key_values = outputs.past_key_values\n",
    "            latent_embd = outputs.hidden_states[-1][:, -1:, :]\n",
    "            cot_hidden_states.append(latent_embd.clone())\n",
    "\n",
    "            # decode predicted token\n",
    "            logits = llama_model.codi.lm_head(latent_embd.squeeze(1))\n",
    "            top1_token_id = torch.argmax(logits, dim=-1).item()\n",
    "            top1_token_str = llama_tokenizer.decode([top1_token_id])\n",
    "            is_number = bool(number_regex.match(top1_token_str))\n",
    "            pos_type = \"BoT\" if i == 0 else f\"T{i}\"\n",
    "\n",
    "            # Causal projection intervention\n",
    "            if intervention_enabled and is_number:\n",
    "                predicted_embd = embedding_layer(torch.tensor([top1_token_id], device=device))\n",
    "                A = latent_embd.squeeze(1)\n",
    "\n",
    "                E_pred_norm = predicted_embd / torch.norm(predicted_embd, dim=-1, keepdim=True)\n",
    "                E_target_norm = target_embd / torch.norm(target_embd, dim=-1, keepdim=True)\n",
    "                proj_predicted = torch.sum(A * E_pred_norm, dim=-1, keepdim=True) * E_pred_norm\n",
    "                proj_target = torch.norm(proj_predicted, dim=-1, keepdim=True) * E_target_norm\n",
    "                A_modified = A - proj_predicted + k * proj_target\n",
    "\n",
    "                logits_modified = llama_model.codi.lm_head(A_modified)\n",
    "                new_token_id = torch.argmax(logits_modified, dim=-1).item()\n",
    "                new_token_str = llama_tokenizer.decode([new_token_id])\n",
    "\n",
    "                latent_embd = A_modified.unsqueeze(1)\n",
    "                llama_interventions.append({\n",
    "                    \"position\": i,\n",
    "                    \"predicted_token\": top1_token_str,\n",
    "                    \"new_token\": new_token_str,\n",
    "                    \"intervened\": True\n",
    "                })\n",
    "                intervened_positions.append(i)\n",
    "                print(f\"{pos_type:4s} [{i:2d}]: '{top1_token_str}' → '{new_token_str}' ✓ INTERVENED\")\n",
    "            else:\n",
    "                print(f\"{pos_type:4s} [{i:2d}]: '{top1_token_str}'\")\n",
    "\n",
    "            if llama_training_args.use_prj:\n",
    "                latent_embd = llama_model.prj(latent_embd)\n",
    "\n",
    "    llama_continuous_thoughts = torch.cat(cot_hidden_states, dim=1)\n",
    "    print(f\"\\nContinuous thoughts shape: {llama_continuous_thoughts.shape}\")\n",
    "\n",
    "    # Decode intermediate tokens\n",
    "    decoded = []\n",
    "    print(\"\\nDecoded intermediate tokens:\")\n",
    "    for i in range(min(50, llama_continuous_thoughts.shape[1])):\n",
    "        logits = llama_model.codi.lm_head(llama_continuous_thoughts[:, i, :])\n",
    "        top1_token_id = torch.argmax(logits, dim=-1).item()\n",
    "        tok_str = llama_tokenizer.decode([top1_token_id])\n",
    "        print(f\"  pos={i:2d}: '{tok_str}'\")\n",
    "        decoded.append(tok_str)\n",
    "\n",
    "    # Generate final answer\n",
    "    print(\"\\nGenerating final answer...\")\n",
    "    if llama_training_args.remove_eos:\n",
    "        eot_tensor = torch.tensor([[llama_model.eot_id]], dtype=torch.long).expand(batch_size, 1, 1).to(device)\n",
    "    else:\n",
    "        eot_tensor = torch.tensor(\n",
    "            [[llama_tokenizer.eos_token_id, llama_model.eot_id]],\n",
    "            dtype=torch.long\n",
    "        ).expand(batch_size, 1, 2).to(device)\n",
    "\n",
    "    eot_emb = llama_model.get_embd(llama_model.codi, llama_model.model_name)(eot_tensor).squeeze(1)\n",
    "    output = eot_emb\n",
    "    pred_tokens = []\n",
    "    for step in range(256):\n",
    "        out = llama_model.codi(inputs_embeds=output, use_cache=True, past_key_values=past_key_values)\n",
    "        past_key_values = out.past_key_values\n",
    "        logits = out.logits[:, -1, :llama_model.codi.config.vocab_size-1]\n",
    "        next_token_id = torch.argmax(logits, dim=-1).item()\n",
    "        pred_tokens.append(next_token_id)\n",
    "        current_token_str = llama_tokenizer.decode([next_token_id])\n",
    "        if next_token_id == llama_tokenizer.eos_token_id or re.match(number_regex, current_token_str.strip()) or step >= 49:\n",
    "            break\n",
    "        output = llama_model.get_embd(llama_model.codi, llama_model.model_name)(\n",
    "            torch.tensor([[next_token_id]], device=device)\n",
    "        )\n",
    "\n",
    "    full_answer = llama_tokenizer.decode(pred_tokens, skip_special_tokens=True)\n",
    "    print(f\"\\nGenerated answer: {full_answer}\")\n",
    "\n",
    "    def extract_number(text):\n",
    "        nums = re.findall(r\"-?\\d+\\.?\\d*\", text.replace(\",\", \"\"))\n",
    "        return float(nums[0]) if nums else None\n",
    "\n",
    "    predicted_number = extract_number(full_answer)\n",
    "    match = str(predicted_number) == str(answer)\n",
    "    print(f\"Extracted number: {predicted_number}  |  Reference: {answer}  |  Match: {match}\")\n",
    "\n",
    "    return {\n",
    "        \"decoded_tokens\": decoded,\n",
    "        \"generated_answer\": full_answer,\n",
    "        \"predicted_number\": predicted_number,\n",
    "        \"reference_answer\": answer,\n",
    "        \"match\": match,\n",
    "        \"interventions\": llama_interventions,\n",
    "        \"intervened_positions\": intervened_positions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LLaMA: Running Continuous Chain-of-Thought\n",
      "================================================================================\n",
      "Question: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?\n",
      "CoT length: 6\n",
      "\n",
      "Running baseline CoT (no intervention).\n",
      "BoT  [ 0]: '40'\n",
      "T1   [ 1]: '80'\n",
      "T2   [ 2]: ' ('\n",
      "T3   [ 3]: '24'\n",
      "T4   [ 4]: '24'\n",
      "T5   [ 5]: '>>'\n",
      "\n",
      "Continuous thoughts shape: torch.Size([1, 7, 2048])\n",
      "\n",
      "Decoded intermediate tokens:\n",
      "  pos= 0: '8'\n",
      "  pos= 1: '40'\n",
      "  pos= 2: '80'\n",
      "  pos= 3: ' ('\n",
      "  pos= 4: '24'\n",
      "  pos= 5: '24'\n",
      "  pos= 6: '>>'\n",
      "\n",
      "Generating final answer...\n",
      "\n",
      "Generated answer: The answer is: 64\n",
      "Extracted number: 64.0  |  Reference: 64  |  Match: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'decoded_tokens': ['8', '40', '80', ' (', '24', '24', '>>'],\n",
       " 'generated_answer': 'The answer is: 64',\n",
       " 'predicted_number': 64.0,\n",
       " 'reference_answer': 64,\n",
       " 'match': False,\n",
       " 'interventions': [],\n",
       " 'intervened_positions': []}"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_continuous_cot_codi_llama_intervene(\n",
    "    question=question2,\n",
    "    answer=answer2,\n",
    "    cot_length=6\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LLaMA: Running Continuous Chain-of-Thought with CAUSAL INTERVENTION\n",
      "================================================================================\n",
      "Question: Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "CoT length: 6\n",
      "\n",
      "CAUSAL INTERVENTION enabled → target_token='5', k=1.0\n",
      "BoT  [ 0]: '7' → ' Gib' ✓ INTERVENED\n",
      "T1   [ 1]: '7' → '+' ✓ INTERVENED\n",
      "T2   [ 2]: '-'\n",
      "T3   [ 3]: '9' → '5' ✓ INTERVENED\n",
      "T4   [ 4]: '9' → '5' ✓ INTERVENED\n",
      "T5   [ 5]: '\"'\n",
      "\n",
      "Continuous thoughts shape: torch.Size([1, 7, 2048])\n",
      "\n",
      "Decoded intermediate tokens:\n",
      "  pos= 0: '9'\n",
      "  pos= 1: '7'\n",
      "  pos= 2: '7'\n",
      "  pos= 3: '-'\n",
      "  pos= 4: '9'\n",
      "  pos= 5: '9'\n",
      "  pos= 6: '\"'\n",
      "\n",
      "Generating final answer...\n",
      "\n",
      "Generated answer: The answer is: 18\n",
      "Extracted number: 18.0  |  Reference: 18  |  Match: False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'decoded_tokens': ['9', '7', '7', '-', '9', '9', '\"'],\n",
       " 'generated_answer': 'The answer is: 18',\n",
       " 'predicted_number': 18.0,\n",
       " 'reference_answer': 18,\n",
       " 'match': False,\n",
       " 'interventions': [{'position': 0,\n",
       "   'predicted_token': '7',\n",
       "   'new_token': ' Gib',\n",
       "   'intervened': True},\n",
       "  {'position': 1,\n",
       "   'predicted_token': '7',\n",
       "   'new_token': '+',\n",
       "   'intervened': True},\n",
       "  {'position': 3,\n",
       "   'predicted_token': '9',\n",
       "   'new_token': '5',\n",
       "   'intervened': True},\n",
       "  {'position': 4,\n",
       "   'predicted_token': '9',\n",
       "   'new_token': '5',\n",
       "   'intervened': True}],\n",
       " 'intervened_positions': [0, 1, 3, 4]}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_continuous_cot_codi_llama_intervene(\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    cot_length=6,\n",
    "    target_token=\"5\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMA: Projection Intervention (Target Token = '5', k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LLaMA: CAUSAL Projection Intervention\n",
      "================================================================================\n",
      "Target token: '5'\n",
      "Target token ID: 20\n",
      "k (top-k intervention): 3\n",
      "\n",
      "Running CAUSAL intervention (affects downstream positions)...\n",
      "\n",
      "BoT  [pos= 0]: '9' → '5' ✓ INTERVENED (causal)\n",
      "T1   [pos= 1]: '7' → '5' ✓ INTERVENED (causal)\n",
      "T2   [pos= 2]: '7' → '5' ✓ INTERVENED (causal)\n",
      "T3   [pos= 3]: '-' (not a number, no intervention)\n",
      "T4   [pos= 4]: '9' → '5' ✓ INTERVENED (causal)\n",
      "T5   [pos= 5]: '9' → '5' ✓ INTERVENED (causal)\n",
      "T6   [pos= 6]: '\"' (not a number, no intervention)\n",
      "\n",
      "================================================================================\n",
      "✓ Total interventions: 5\n",
      "✓ Intervened at positions: [0, 1, 2, 4, 5]\n",
      "================================================================================\n",
      "\n",
      "Generating final answer with intervened CoT...\n",
      "Stopped at step 5 (found number: '18')\n",
      "\n",
      "Intervened Answer: The answer is: 18\n",
      "Intervened numerical answer: 18.0\n",
      "\n",
      "(Compare to original expected: 18)\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LLaMA: CAUSAL Projection Intervention\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# CRITICAL: Reset to ensure clean state\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "past_key_values = None  # Clear any cached state\n",
    "\n",
    "target_token = '5'\n",
    "k = 3\n",
    "\n",
    "# Get target token embedding\n",
    "target_token_id = llama_tokenizer.encode(target_token, add_special_tokens=False)[0]\n",
    "embedding_layer = llama_model.codi.get_input_embeddings()\n",
    "target_embd = embedding_layer(torch.tensor([target_token_id], device=device))\n",
    "\n",
    "print(f\"Target token: '{target_token}'\")\n",
    "print(f\"Target token ID: {target_token_id}\")\n",
    "print(f\"k (top-k intervention): {k}\")\n",
    "print(f\"\\nRunning CAUSAL intervention (affects downstream positions)...\\n\")\n",
    "\n",
    "# Re-run the chain-of-thought with interventions\n",
    "batch_size = 1\n",
    "\n",
    "# Set pad token if needed\n",
    "if llama_tokenizer.pad_token is None:\n",
    "    llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "    llama_tokenizer.pad_token_id = llama_tokenizer.eos_token_id\n",
    "\n",
    "# Tokenize with BoT token\n",
    "if llama_training_args.remove_eos:\n",
    "    bot_tensor = torch.tensor([llama_model.bot_id], dtype=torch.long).expand(batch_size, 1).to(device)\n",
    "else:\n",
    "    bot_tensor = torch.tensor([llama_tokenizer.eos_token_id, llama_model.bot_id], \n",
    "                              dtype=torch.long).expand(batch_size, 2).to(device)\n",
    "\n",
    "inputs = llama_tokenizer([question], return_tensors=\"pt\", padding=False)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "inputs[\"input_ids\"] = torch.cat((inputs[\"input_ids\"], bot_tensor), dim=1)\n",
    "inputs[\"attention_mask\"] = torch.cat((inputs[\"attention_mask\"], torch.ones_like(bot_tensor)), dim=1)\n",
    "\n",
    "llama_interventions = []\n",
    "intervened_positions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Initial encoding (position 0: BoT)\n",
    "    past_key_values = None\n",
    "    outputs = llama_model.codi(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        use_cache=True,\n",
    "        output_hidden_states=True,\n",
    "        past_key_values=past_key_values,\n",
    "        attention_mask=inputs[\"attention_mask\"]\n",
    "    )\n",
    "    past_key_values = outputs.past_key_values\n",
    "    latent_embd = outputs.hidden_states[-1][:, -1:, :]\n",
    "    \n",
    "    # Check BoT position\n",
    "    logits = llama_model.codi.lm_head(latent_embd.squeeze(1))\n",
    "    top1_token_id = torch.argmax(logits, dim=-1).item()\n",
    "    top1_token_str = llama_tokenizer.decode([top1_token_id])\n",
    "    is_number = bool(number_regex.match(top1_token_str))\n",
    "    \n",
    "    # Intervene at BoT if it's a number\n",
    "    if is_number:\n",
    "        predicted_embd = embedding_layer(torch.tensor([top1_token_id], device=device))\n",
    "        A = latent_embd.squeeze(1)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        E_pred_norm = predicted_embd / torch.norm(predicted_embd, dim=-1, keepdim=True)\n",
    "        E_target_norm = target_embd / torch.norm(target_embd, dim=-1, keepdim=True)\n",
    "        \n",
    "        # Projection intervention\n",
    "        proj_predicted = torch.sum(A * E_pred_norm, dim=-1, keepdim=True) * E_pred_norm\n",
    "        proj_target = torch.norm(proj_predicted, dim =-1, keepdim=True) * E_target_norm\n",
    "        #proj_target = torch.sum(A * E_target_norm, dim=-1, keepdim=True) * E_target_norm\n",
    "        A_modified = A - proj_predicted + k * proj_target\n",
    "        \n",
    "        # Check what it decodes to after intervention\n",
    "        logits_modified = llama_model.codi.lm_head(A_modified)\n",
    "        new_token_id = torch.argmax(logits_modified, dim=-1).item()\n",
    "        new_token_str = llama_tokenizer.decode([new_token_id])\n",
    "        \n",
    "        # CAUSALLY apply the intervention\n",
    "        latent_embd = A_modified.unsqueeze(1)\n",
    "        \n",
    "        llama_interventions.append({\n",
    "            'position': 0,\n",
    "            'predicted_token': top1_token_str,\n",
    "            'new_token': new_token_str,\n",
    "            'intervened': True\n",
    "        })\n",
    "        intervened_positions.append(0)\n",
    "        print(f\"BoT  [pos= 0]: '{top1_token_str}' → '{new_token_str}' ✓ INTERVENED (causal)\")\n",
    "    else:\n",
    "        print(f\"BoT  [pos= 0]: '{top1_token_str}' (not a number, no intervention)\")\n",
    "    \n",
    "    # Apply initial projection\n",
    "    if llama_training_args.use_prj:\n",
    "        latent_embd = llama_model.prj(latent_embd)\n",
    "    \n",
    "    # Chain-of-Thought iterations (positions 1-6)\n",
    "    for i in range(llama_training_args.inf_latent_iterations):\n",
    "        outputs = llama_model.codi(\n",
    "            inputs_embeds=latent_embd,\n",
    "            use_cache=True,\n",
    "            output_hidden_states=True,\n",
    "            past_key_values=past_key_values\n",
    "        )\n",
    "        past_key_values = outputs.past_key_values\n",
    "        latent_embd = outputs.hidden_states[-1][:, -1:, :]\n",
    "        \n",
    "        # Check this position\n",
    "        logits = llama_model.codi.lm_head(latent_embd.squeeze(1))\n",
    "        top1_token_id = torch.argmax(logits, dim=-1).item()\n",
    "        top1_token_str = llama_tokenizer.decode([top1_token_id])\n",
    "        is_number = bool(number_regex.match(top1_token_str))\n",
    "        \n",
    "        pos_type = f\"T{i+1}\"\n",
    "        \n",
    "        # Intervene if it's a number\n",
    "        if is_number:\n",
    "            predicted_embd = embedding_layer(torch.tensor([top1_token_id], device=device))\n",
    "            A = latent_embd.squeeze(1)\n",
    "            \n",
    "            # Normalize embeddings\n",
    "            E_pred_norm = predicted_embd / torch.norm(predicted_embd, dim=-1, keepdim=True)\n",
    "            E_target_norm = target_embd / torch.norm(target_embd, dim=-1, keepdim=True)\n",
    "            \n",
    "            # Projection intervention\n",
    "            proj_predicted = torch.sum(A * E_pred_norm, dim=-1, keepdim=True) * E_pred_norm\n",
    "            proj_target = torch.sum(A * E_target_norm, dim=-1, keepdim=True) * E_target_norm\n",
    "            A_modified = A - proj_predicted + k * proj_target\n",
    "            \n",
    "            # Check what it decodes to after intervention\n",
    "            logits_modified = llama_model.codi.lm_head(A_modified)\n",
    "            new_token_id = torch.argmax(logits_modified, dim=-1).item()\n",
    "            new_token_str = llama_tokenizer.decode([new_token_id])\n",
    "            \n",
    "            # CAUSALLY apply the intervention\n",
    "            latent_embd = A_modified.unsqueeze(1)\n",
    "            \n",
    "            llama_interventions.append({\n",
    "                'position': i+1,\n",
    "                'predicted_token': top1_token_str,\n",
    "                'new_token': new_token_str,\n",
    "                'intervened': True\n",
    "            })\n",
    "            intervened_positions.append(i+1)\n",
    "            print(f\"{pos_type:4s} [pos={i+1:2d}]: '{top1_token_str}' → '{new_token_str}' ✓ INTERVENED (causal)\")\n",
    "        else:\n",
    "            print(f\"{pos_type:4s} [pos={i+1:2d}]: '{top1_token_str}' (not a number, no intervention)\")\n",
    "        \n",
    "        # Apply projection for next iteration\n",
    "        if llama_training_args.use_prj:\n",
    "            latent_embd = llama_model.prj(latent_embd)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Total interventions: {len(llama_interventions)}\")\n",
    "print(f\"✓ Intervened at positions: {intervened_positions}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Now generate the final answer with the intervened chain-of-thought\n",
    "print(\"\\nGenerating final answer with intervened CoT...\")\n",
    "\n",
    "# Signal end-of-thought with EOT token\n",
    "if llama_training_args.remove_eos:\n",
    "    eot_tensor = torch.tensor([[llama_model.eot_id]], dtype=torch.long).expand(batch_size, 1, 1).to(device)\n",
    "else:\n",
    "    eot_tensor = torch.tensor([[llama_tokenizer.eos_token_id, llama_model.eot_id]], \n",
    "                               dtype=torch.long).expand(batch_size, 1, 2).to(device)\n",
    "\n",
    "eot_emb = llama_model.get_embd(llama_model.codi, llama_model.model_name)(eot_tensor).squeeze(1)\n",
    "output = eot_emb\n",
    "\n",
    "# Generate answer tokens\n",
    "# Generate answer tokens\n",
    "pred_tokens = []\n",
    "found_number = False\n",
    "\n",
    "for step in range(256):\n",
    "    out = llama_model.codi(\n",
    "        inputs_embeds=output,\n",
    "        output_hidden_states=False,\n",
    "        attention_mask=None,\n",
    "        use_cache=True,\n",
    "        output_attentions=False,\n",
    "        past_key_values=past_key_values\n",
    "    )\n",
    "    past_key_values = out.past_key_values\n",
    "    \n",
    "    logits = out.logits[:, -1, :llama_model.codi.config.vocab_size-1]\n",
    "    next_token_id = torch.argmax(logits, dim=-1).item()\n",
    "    pred_tokens.append(next_token_id)\n",
    "    \n",
    "    # Decode current token\n",
    "    current_token_str = llama_tokenizer.decode([next_token_id])\n",
    "    \n",
    "    # Stop if EOS token\n",
    "    if next_token_id == llama_tokenizer.eos_token_id:\n",
    "        print(f\"Stopped at step {step} (EOS token)\")\n",
    "        break\n",
    "    \n",
    "    # Stop immediately after generating a number token\n",
    "    if number_regex.match(current_token_str.strip()):\n",
    "        print(f\"Stopped at step {step} (found number: '{current_token_str}')\")\n",
    "        break\n",
    "    \n",
    "    # Hard limit to prevent loops\n",
    "    if step >= 49:\n",
    "        print(f\"Stopped at step {step} (max length)\")\n",
    "        break\n",
    "    \n",
    "    output = llama_model.get_embd(llama_model.codi, llama_model.model_name)(\n",
    "        torch.tensor([[next_token_id]], device=device)\n",
    "    )\n",
    "# Decode and extract answer\n",
    "intervened_answer = llama_tokenizer.decode(pred_tokens, skip_special_tokens=True)\n",
    "print(f\"\\nIntervened Answer: {intervened_answer}\")\n",
    "\n",
    "def extract_answer_number(text):\n",
    "    text = text.replace(',', '')\n",
    "    numbers = [s for s in re.findall(r'-?\\d+\\.?\\d*', text)]\n",
    "    if not numbers:\n",
    "        return None\n",
    "    return float(numbers[0])\n",
    "\n",
    "intervened_number = extract_answer_number(intervened_answer)\n",
    "print(f\"Intervened numerical answer: {intervened_number}\")\n",
    "print(f\"\\n(Compare to original expected: {answer})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LLaMA: CAUSAL Projection Intervention\n",
      "================================================================================\n",
      "Target token: '5'\n",
      "Target token ID: 20\n",
      "k (top-k intervention): 3\n",
      "\n",
      "Running CAUSAL intervention (affects downstream positions)...\n",
      "\n",
      "BoT  [pos= 0]: '9' → '5' ✓ INTERVENED (causal)\n",
      "T1   [pos= 1]: '7' → '5' ✓ INTERVENED (causal)\n",
      "T2   [pos= 2]: '7' → '5' ✓ INTERVENED (causal)\n",
      "T3   [pos= 3]: '-' (not a number, no intervention)\n",
      "T4   [pos= 4]: '9' → '5' ✓ INTERVENED (causal)\n",
      "T5   [pos= 5]: '9' → '5' ✓ INTERVENED (causal)\n",
      "T6   [pos= 6]: '\"' (not a number, no intervention)\n",
      "\n",
      "================================================================================\n",
      "✓ Total interventions: 5\n",
      "✓ Intervened at positions: [0, 1, 2, 4, 5]\n",
      "================================================================================\n",
      "\n",
      "Generating final answer with intervened CoT...\n",
      "Stopped at step 5 (found number: '18')\n",
      "\n",
      "Intervened Answer: The answer is: 18\n",
      "Intervened numerical answer: 18.0\n",
      "\n",
      "(Compare to original expected: 18)\n"
     ]
    }
   ],
   "source": [
    "##REPLACEMENT INTERVENTION \n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LLaMA: CAUSAL Projection Intervention\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# CRITICAL: Reset to ensure clean state\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "past_key_values = None  # Clear any cached state\n",
    "\n",
    "target_token = '5'\n",
    "k = 3\n",
    "\n",
    "# Get target token embedding\n",
    "target_token_id = llama_tokenizer.encode(target_token, add_special_tokens=False)[0]\n",
    "embedding_layer = llama_model.codi.get_input_embeddings()\n",
    "target_embd = embedding_layer(torch.tensor([target_token_id], device=device))\n",
    "\n",
    "print(f\"Target token: '{target_token}'\")\n",
    "print(f\"Target token ID: {target_token_id}\")\n",
    "print(f\"k (top-k intervention): {k}\")\n",
    "print(f\"\\nRunning CAUSAL intervention (affects downstream positions)...\\n\")\n",
    "\n",
    "# Re-run the chain-of-thought with interventions\n",
    "batch_size = 1\n",
    "\n",
    "# Set pad token if needed\n",
    "if llama_tokenizer.pad_token is None:\n",
    "    llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "    llama_tokenizer.pad_token_id = llama_tokenizer.eos_token_id\n",
    "\n",
    "# Tokenize with BoT token\n",
    "if llama_training_args.remove_eos:\n",
    "    bot_tensor = torch.tensor([llama_model.bot_id], dtype=torch.long).expand(batch_size, 1).to(device)\n",
    "else:\n",
    "    bot_tensor = torch.tensor([llama_tokenizer.eos_token_id, llama_model.bot_id], \n",
    "                              dtype=torch.long).expand(batch_size, 2).to(device)\n",
    "\n",
    "inputs = llama_tokenizer([question], return_tensors=\"pt\", padding=False)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "inputs[\"input_ids\"] = torch.cat((inputs[\"input_ids\"], bot_tensor), dim=1)\n",
    "inputs[\"attention_mask\"] = torch.cat((inputs[\"attention_mask\"], torch.ones_like(bot_tensor)), dim=1)\n",
    "\n",
    "llama_interventions = []\n",
    "intervened_positions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Initial encoding (position 0: BoT)\n",
    "    past_key_values = None\n",
    "    outputs = llama_model.codi(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        use_cache=True,\n",
    "        output_hidden_states=True,\n",
    "        past_key_values=past_key_values,\n",
    "        attention_mask=inputs[\"attention_mask\"]\n",
    "    )\n",
    "    past_key_values = outputs.past_key_values\n",
    "    latent_embd = outputs.hidden_states[-1][:, -1:, :]\n",
    "    \n",
    "    # Check BoT position\n",
    "    logits = llama_model.codi.lm_head(latent_embd.squeeze(1))\n",
    "    top1_token_id = torch.argmax(logits, dim=-1).item()\n",
    "    top1_token_str = llama_tokenizer.decode([top1_token_id])\n",
    "    is_number = bool(number_regex.match(top1_token_str))\n",
    "    \n",
    "    # Intervene at BoT if it's a number\n",
    "    if is_number:\n",
    "        predicted_embd = embedding_layer(torch.tensor([top1_token_id], device=device))\n",
    "        A = latent_embd.squeeze(1)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        E_pred_norm = predicted_embd / torch.norm(predicted_embd, dim=-1, keepdim=True)\n",
    "        E_target_norm = target_embd / torch.norm(target_embd, dim=-1, keepdim=True)\n",
    "        \n",
    "        # Projection intervention\n",
    "        proj_predicted = torch.sum(A * E_pred_norm, dim=-1, keepdim=True) * E_pred_norm\n",
    "        proj_target = torch.norm(proj_predicted, dim =-1, keepdim=True) * E_target_norm\n",
    "        #proj_target = torch.sum(A * E_target_norm, dim=-1, keepdim=True) * E_target_norm\n",
    "        A_modified = A - proj_predicted + k * proj_target\n",
    "        \n",
    "        # Check what it decodes to after intervention\n",
    "        logits_modified = llama_model.codi.lm_head(A_modified)\n",
    "        new_token_id = torch.argmax(logits_modified, dim=-1).item()\n",
    "        new_token_str = llama_tokenizer.decode([new_token_id])\n",
    "        \n",
    "        # CAUSALLY apply the intervention\n",
    "        latent_embd = A_modified.unsqueeze(1)\n",
    "        \n",
    "        llama_interventions.append({\n",
    "            'position': 0,\n",
    "            'predicted_token': top1_token_str,\n",
    "            'new_token': new_token_str,\n",
    "            'intervened': True\n",
    "        })\n",
    "        intervened_positions.append(0)\n",
    "        print(f\"BoT  [pos= 0]: '{top1_token_str}' → '{new_token_str}' ✓ INTERVENED (causal)\")\n",
    "    else:\n",
    "        print(f\"BoT  [pos= 0]: '{top1_token_str}' (not a number, no intervention)\")\n",
    "    \n",
    "    # Apply initial projection\n",
    "    if llama_training_args.use_prj:\n",
    "        latent_embd = llama_model.prj(latent_embd)\n",
    "    \n",
    "    # Chain-of-Thought iterations (positions 1-6)\n",
    "    for i in range(llama_training_args.inf_latent_iterations):\n",
    "        outputs = llama_model.codi(\n",
    "            inputs_embeds=latent_embd,\n",
    "            use_cache=True,\n",
    "            output_hidden_states=True,\n",
    "            past_key_values=past_key_values\n",
    "        )\n",
    "        past_key_values = outputs.past_key_values\n",
    "        latent_embd = outputs.hidden_states[-1][:, -1:, :]\n",
    "        \n",
    "        # Check this position\n",
    "        logits = llama_model.codi.lm_head(latent_embd.squeeze(1))\n",
    "        top1_token_id = torch.argmax(logits, dim=-1).item()\n",
    "        top1_token_str = llama_tokenizer.decode([top1_token_id])\n",
    "        is_number = bool(number_regex.match(top1_token_str))\n",
    "        \n",
    "        pos_type = f\"T{i+1}\"\n",
    "        \n",
    "        # Intervene if it's a number\n",
    "        if is_number:\n",
    "            predicted_embd = embedding_layer(torch.tensor([top1_token_id], device=device))\n",
    "            A = latent_embd.squeeze(1)\n",
    "            \n",
    "            # Normalize embeddings\n",
    "            E_pred_norm = predicted_embd / torch.norm(predicted_embd, dim=-1, keepdim=True)\n",
    "            E_target_norm = target_embd / torch.norm(target_embd, dim=-1, keepdim=True)\n",
    "            \n",
    "            # Projection intervention\n",
    "            proj_predicted = torch.sum(A * E_pred_norm, dim=-1, keepdim=True) * E_pred_norm\n",
    "            proj_target = torch.sum(A * E_target_norm, dim=-1, keepdim=True) * E_target_norm\n",
    "            A_modified = A - proj_predicted + k * proj_target\n",
    "            \n",
    "            # Check what it decodes to after intervention\n",
    "            logits_modified = llama_model.codi.lm_head(A_modified)\n",
    "            new_token_id = torch.argmax(logits_modified, dim=-1).item()\n",
    "            new_token_str = llama_tokenizer.decode([new_token_id])\n",
    "            \n",
    "            # CAUSALLY apply the intervention\n",
    "            latent_embd = A_modified.unsqueeze(1)\n",
    "            \n",
    "            llama_interventions.append({\n",
    "                'position': i+1,\n",
    "                'predicted_token': top1_token_str,\n",
    "                'new_token': new_token_str,\n",
    "                'intervened': True\n",
    "            })\n",
    "            intervened_positions.append(i+1)\n",
    "            print(f\"{pos_type:4s} [pos={i+1:2d}]: '{top1_token_str}' → '{new_token_str}' ✓ INTERVENED (causal)\")\n",
    "        else:\n",
    "            print(f\"{pos_type:4s} [pos={i+1:2d}]: '{top1_token_str}' (not a number, no intervention)\")\n",
    "        \n",
    "        # Apply projection for next iteration\n",
    "        if llama_training_args.use_prj:\n",
    "            latent_embd = llama_model.prj(latent_embd)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"✓ Total interventions: {len(llama_interventions)}\")\n",
    "print(f\"✓ Intervened at positions: {intervened_positions}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Now generate the final answer with the intervened chain-of-thought\n",
    "print(\"\\nGenerating final answer with intervened CoT...\")\n",
    "\n",
    "# Signal end-of-thought with EOT token\n",
    "if llama_training_args.remove_eos:\n",
    "    eot_tensor = torch.tensor([[llama_model.eot_id]], dtype=torch.long).expand(batch_size, 1, 1).to(device)\n",
    "else:\n",
    "    eot_tensor = torch.tensor([[llama_tokenizer.eos_token_id, llama_model.eot_id]], \n",
    "                               dtype=torch.long).expand(batch_size, 1, 2).to(device)\n",
    "\n",
    "eot_emb = llama_model.get_embd(llama_model.codi, llama_model.model_name)(eot_tensor).squeeze(1)\n",
    "output = eot_emb\n",
    "\n",
    "# Generate answer tokens\n",
    "# Generate answer tokens\n",
    "pred_tokens = []\n",
    "found_number = False\n",
    "\n",
    "for step in range(256):\n",
    "    out = llama_model.codi(\n",
    "        inputs_embeds=output,\n",
    "        output_hidden_states=False,\n",
    "        attention_mask=None,\n",
    "        use_cache=True,\n",
    "        output_attentions=False,\n",
    "        past_key_values=past_key_values\n",
    "    )\n",
    "    past_key_values = out.past_key_values\n",
    "    \n",
    "    logits = out.logits[:, -1, :llama_model.codi.config.vocab_size-1]\n",
    "    next_token_id = torch.argmax(logits, dim=-1).item()\n",
    "    pred_tokens.append(next_token_id)\n",
    "    \n",
    "    # Decode current token\n",
    "    current_token_str = llama_tokenizer.decode([next_token_id])\n",
    "    \n",
    "    # Stop if EOS token\n",
    "    if next_token_id == llama_tokenizer.eos_token_id:\n",
    "        print(f\"Stopped at step {step} (EOS token)\")\n",
    "        break\n",
    "    \n",
    "    # Stop immediately after generating a number token\n",
    "    if number_regex.match(current_token_str.strip()):\n",
    "        print(f\"Stopped at step {step} (found number: '{current_token_str}')\")\n",
    "        break\n",
    "    \n",
    "    # Hard limit to prevent loops\n",
    "    if step >= 49:\n",
    "        print(f\"Stopped at step {step} (max length)\")\n",
    "        break\n",
    "    \n",
    "    output = llama_model.get_embd(llama_model.codi, llama_model.model_name)(\n",
    "        torch.tensor([[next_token_id]], device=device)\n",
    "    )\n",
    "# Decode and extract answer\n",
    "intervened_answer = llama_tokenizer.decode(pred_tokens, skip_special_tokens=True)\n",
    "print(f\"\\nIntervened Answer: {intervened_answer}\")\n",
    "\n",
    "def extract_answer_number(text):\n",
    "    text = text.replace(',', '')\n",
    "    numbers = [s for s in re.findall(r'-?\\d+\\.?\\d*', text)]\n",
    "    if not numbers:\n",
    "        return None\n",
    "    return float(numbers[0])\n",
    "\n",
    "intervened_number = extract_answer_number(intervened_answer)\n",
    "print(f\"Intervened numerical answer: {intervened_number}\")\n",
    "print(f\"\\n(Compare to original expected: {answer})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LLaMA: Chain-of-Thought WITHOUT Beginning-of-Thought Token\n",
      "================================================================================\n",
      "WITHOUT BoT - Input IDs: tensor([[128000,     42,   4010,    277,   4024,    311,    279,   3637,    311,\n",
      "           3780,  29247,    369,    813,    502,  13455,     13,   3861,   9168,\n",
      "           7194,    400,     20,     11,    719,   1475,   2132,   9168,   7194,\n",
      "           1193,    220,   1399,      4,    315,    279,   3430,     13,    735,\n",
      "           4010,    277,   6944,    311,   3780,    220,    845,  29247,     13,\n",
      "           2650,   1790,   1587,    568,   1205,    311,   2343,    369,   1124,\n",
      "             30]], device='cuda:0')\n",
      "WITHOUT BoT - Last 5 token IDs: [311, 2343, 369, 1124, 30]\n",
      "WITHOUT BoT - BoT token ID should be: 128257\n",
      "WITHOUT BoT - Contains BoT?: False\n",
      "Input shape (no BoT): torch.Size([1, 55])\n",
      "✓ Position 0 (T0): shape=torch.Size([1, 1, 2048])\n",
      "✓ Position 1 (T1): shape=torch.Size([1, 1, 2048])\n",
      "✓ Position 2 (T2): shape=torch.Size([1, 1, 2048])\n",
      "✓ Position 3 (T3): shape=torch.Size([1, 1, 2048])\n",
      "✓ Position 4 (T4): shape=torch.Size([1, 1, 2048])\n",
      "✓ Position 5 (T5): shape=torch.Size([1, 1, 2048])\n",
      "\n",
      "✓ Total continuous thoughts (no BoT): torch.Size([1, 6, 2048])\n",
      "  Expected: [1, 6, hidden_dim]\n",
      "  Got: [1, 6, 2048]\n",
      "\n",
      "✓ SUCCESS: Collected exactly 6 CoT positions (no BoT)!\n",
      "\n",
      "================================================================================\n",
      "Decoding Continuous Thoughts (no BoT)\n",
      "================================================================================\n",
      "  T0   [pos= 0]: token_id= 1272 → '40' 🔢 NUMBER!\n",
      "  T1   [pos= 1]: token_id= 1490 → '80' 🔢 NUMBER!\n",
      "  T2   [pos= 2]: token_id=  320 → ' ('\n",
      "  T3   [pos= 3]: token_id= 1187 → '24' 🔢 NUMBER!\n",
      "  T4   [pos= 4]: token_id= 1187 → '24' 🔢 NUMBER!\n",
      "  T5   [pos= 5]: token_id=   28 → '='\n",
      "\n",
      "📊 Numbers at positions: [0, 1, 3, 4]\n",
      "📊 Total numbers: 4/6\n",
      "\n",
      "================================================================================\n",
      "Generate Final Answer (no BoT)\n",
      "================================================================================\n",
      "Stopped at step 5 (found number: '64')\n",
      "\n",
      "Generated Answer (no BoT): The answer is: 64\n",
      "Qeustion: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?\n",
      "Numerical answer (no BoT): 64.0\n",
      "Expected answer:  64\n",
      "\n",
      "================================================================================\n",
      "Comparison: With BoT vs Without BoT\n",
      "================================================================================\n",
      "With BoT:    7 positions (1 BoT + 6 CoT)\n",
      "Without BoT: 6 positions (0 BoT + 6 CoT)\n",
      "\n",
      "This tests whether the BoT token improves reasoning performance.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LLaMA: Chain-of-Thought WITHOUT Beginning-of-Thought Token\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 1\n",
    "\n",
    "questions = [question]\n",
    "\n",
    "# Set pad token if needed\n",
    "if llama_tokenizer.pad_token is None:\n",
    "    llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "    llama_tokenizer.pad_token_id = llama_tokenizer.eos_token_id\n",
    "\n",
    "# Tokenize WITHOUT BoT token - just the raw question\n",
    "inputs = llama_tokenizer(questions, return_tensors=\"pt\", padding=False)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "print(f\"WITHOUT BoT - Input IDs: {inputs['input_ids']}\")\n",
    "print(f\"WITHOUT BoT - Last 5 token IDs: {inputs['input_ids'][0, -5:].tolist()}\")\n",
    "print(f\"WITHOUT BoT - BoT token ID should be: {llama_model.bot_id}\")\n",
    "print(f\"WITHOUT BoT - Contains BoT?: {llama_model.bot_id in inputs['input_ids'][0].tolist()}\")\n",
    "print(f\"Input shape (no BoT): {inputs['input_ids'].shape}\")\n",
    "\n",
    "# Store the 6 chain-of-thought hidden states (no BoT position)\n",
    "cot_hidden_states_no_bot = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Initial encoding - start directly from the last token of the question\n",
    "    past_key_values = None\n",
    "    outputs = llama_model.codi(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        use_cache=True,\n",
    "        output_hidden_states=True,\n",
    "        past_key_values=past_key_values,\n",
    "        attention_mask=inputs[\"attention_mask\"]\n",
    "    )\n",
    "    past_key_values = outputs.past_key_values\n",
    "    \n",
    "    # Start from the last question token (no BoT)\n",
    "    latent_embd = outputs.hidden_states[-1][:, -1:, :]  # Shape: [batch, 1, hidden_dim]\n",
    "    \n",
    "    # Apply initial projection\n",
    "    if llama_training_args.use_prj:\n",
    "        latent_embd = llama_model.prj(latent_embd)\n",
    "    \n",
    "    # Chain-of-Thought iterations (6 positions, no BoT)\n",
    "    for i in range(llama_training_args.inf_latent_iterations):\n",
    "        outputs = llama_model.codi(\n",
    "            inputs_embeds=latent_embd,\n",
    "            use_cache=True,\n",
    "            output_hidden_states=True,\n",
    "            past_key_values=past_key_values\n",
    "        )\n",
    "        past_key_values = outputs.past_key_values\n",
    "        \n",
    "        # Get hidden state BEFORE projection\n",
    "        latent_embd = outputs.hidden_states[-1][:, -1:, :]  # Shape: [batch, 1, hidden_dim]\n",
    "        \n",
    "        # Store BEFORE projection for decoding\n",
    "        cot_hidden_states_no_bot.append(latent_embd.clone())\n",
    "        print(f\"✓ Position {i} (T{i}): shape={latent_embd.shape}\")\n",
    "        \n",
    "        # Apply projection for next iteration\n",
    "        if llama_training_args.use_prj:\n",
    "            latent_embd = llama_model.prj(latent_embd)\n",
    "\n",
    "# Stack all 6 positions (no BoT)\n",
    "llama_continuous_thoughts_no_bot = torch.cat(cot_hidden_states_no_bot, dim=1)\n",
    "print(f\"\\n✓ Total continuous thoughts (no BoT): {llama_continuous_thoughts_no_bot.shape}\")\n",
    "print(f\"  Expected: [1, 6, hidden_dim]\")\n",
    "print(f\"  Got: {list(llama_continuous_thoughts_no_bot.shape)}\")\n",
    "\n",
    "if llama_continuous_thoughts_no_bot.shape[1] != 6:\n",
    "    print(f\"\\n❌ ERROR: Expected 6 positions, got {llama_continuous_thoughts_no_bot.shape[1]}\")\n",
    "else:\n",
    "    print(f\"\\n✓ SUCCESS: Collected exactly 6 CoT positions (no BoT)!\")\n",
    "\n",
    "# Decode the continuous thoughts\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Decoding Continuous Thoughts (no BoT)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "llama_decoded_tokens_no_bot = []\n",
    "llama_number_positions_no_bot = []\n",
    "\n",
    "for i in range(llama_continuous_thoughts_no_bot.shape[1]):\n",
    "    hidden_state = llama_continuous_thoughts_no_bot[:, i, :]\n",
    "    \n",
    "    # Decode using CODI's LM head\n",
    "    logits = llama_model.codi.lm_head(hidden_state)\n",
    "    top1_token_id = torch.argmax(logits, dim=-1).item()\n",
    "    top1_token_str = llama_tokenizer.decode([top1_token_id])\n",
    "    \n",
    "    is_number = bool(number_regex.match(top1_token_str.strip()))\n",
    "    pos_type = f\"T{i}\"\n",
    "    \n",
    "    llama_decoded_tokens_no_bot.append((i, top1_token_id, top1_token_str, is_number))\n",
    "    if is_number:\n",
    "        llama_number_positions_no_bot.append(i)\n",
    "    \n",
    "    marker = \" 🔢 NUMBER!\" if is_number else \"\"\n",
    "    print(f\"  {pos_type:4s} [pos={i:2d}]: token_id={top1_token_id:5d} → '{top1_token_str}'{marker}\")\n",
    "\n",
    "print(f\"\\n📊 Numbers at positions: {llama_number_positions_no_bot}\")\n",
    "print(f\"📊 Total numbers: {len(llama_number_positions_no_bot)}/6\")\n",
    "\n",
    "# Generate final answer (no BoT)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Generate Final Answer (no BoT)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Signal end-of-thought with EOT token\n",
    "    if llama_training_args.remove_eos:\n",
    "        eot_tensor = torch.tensor([[llama_model.eot_id]], dtype=torch.long).expand(batch_size, 1, 1).to(device)\n",
    "    else:\n",
    "        eot_tensor = torch.tensor([[llama_tokenizer.eos_token_id, llama_model.eot_id]], \n",
    "                                   dtype=torch.long).expand(batch_size, 1, 2).to(device)\n",
    "    \n",
    "    eot_emb = llama_model.get_embd(llama_model.codi, llama_model.model_name)(eot_tensor).squeeze(1)\n",
    "    output = eot_emb\n",
    "    \n",
    "    # Generate answer tokens\n",
    "    pred_tokens = []\n",
    "    \n",
    "    for step in range(256):\n",
    "        out = llama_model.codi(\n",
    "            inputs_embeds=output,\n",
    "            output_hidden_states=False,\n",
    "            attention_mask=None,\n",
    "            use_cache=True,\n",
    "            output_attentions=False,\n",
    "            past_key_values=past_key_values\n",
    "        )\n",
    "        past_key_values = out.past_key_values\n",
    "        \n",
    "        logits = out.logits[:, -1, :llama_model.codi.config.vocab_size-1]\n",
    "        next_token_id = torch.argmax(logits, dim=-1).item()\n",
    "        pred_tokens.append(next_token_id)\n",
    "        \n",
    "        # Decode current token\n",
    "        current_token_str = llama_tokenizer.decode([next_token_id])\n",
    "        \n",
    "        # Stop if EOS token\n",
    "        if next_token_id == llama_tokenizer.eos_token_id:\n",
    "            print(f\"Stopped at step {step} (EOS token)\")\n",
    "            break\n",
    "        \n",
    "        # Stop immediately after generating a number token\n",
    "        if number_regex.match(current_token_str.strip()):\n",
    "            print(f\"Stopped at step {step} (found number: '{current_token_str}')\")\n",
    "            break\n",
    "        \n",
    "        # Hard limit to prevent loops\n",
    "        if step >= 49:\n",
    "            print(f\"Stopped at step {step} (max length)\")\n",
    "            break\n",
    "        \n",
    "        output = llama_model.get_embd(llama_model.codi, llama_model.model_name)(\n",
    "            torch.tensor([[next_token_id]], device=device)\n",
    "        )\n",
    "    \n",
    "    # Decode full answer\n",
    "    full_answer_no_bot = llama_tokenizer.decode(pred_tokens, skip_special_tokens=True)\n",
    "    print(f\"\\nGenerated Answer (no BoT): {full_answer_no_bot}\")\n",
    "    \n",
    "    # Extract numerical answer\n",
    "    def extract_answer_number(text):\n",
    "        text = text.replace(',', '')\n",
    "        numbers = [s for s in re.findall(r'-?\\d+\\.?\\d*', text)]\n",
    "        if not numbers:\n",
    "            return None\n",
    "        return float(numbers[-1])\n",
    "    \n",
    "    print(f\"Qeustion: {question}\")\n",
    "    predicted_number_no_bot = extract_answer_number(full_answer_no_bot)\n",
    "    print(f\"Numerical answer (no BoT): {predicted_number_no_bot}\")\n",
    "    print(f\"Expected answer: \",answer)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Comparison: With BoT vs Without BoT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"With BoT:    7 positions (1 BoT + 6 CoT)\")\n",
    "print(f\"Without BoT: 6 positions (0 BoT + 6 CoT)\")\n",
    "print(f\"\\nThis tests whether the BoT token improves reasoning performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CODI-GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Loading CODI-GPT-2 from Local Checkpoint\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "  # Load from HuggingFace Hub\n",
    "gpt2_model_name = \"lhao499/codi-gpt2\"\n",
    "\n",
    "gpt2_model_args = ModelArguments(\n",
    "      model_name_or_path=\"gpt2\",\n",
    "      lora_init=True,\n",
    "      lora_r=128,\n",
    "      lora_alpha=32,\n",
    "      ckpt_dir=\"/workspace/CoT_Exploration/models/CODI-gpt2\",  # Local checkpoint\n",
    "      full_precision=True,\n",
    "      token=None\n",
    "  )\n",
    "\n",
    "gpt2_training_args = TrainingArguments(\n",
    "      output_dir=\"./outputs\",\n",
    "      model_max_length=512,\n",
    "      inf_latent_iterations=6,\n",
    "      use_prj=True,\n",
    "      prj_dim=768,\n",
    "      remove_eos=True,\n",
    "      greedy=True,\n",
    "      bf16=False,\n",
    "      inf_num_iterations=1\n",
    "  )\n",
    "\n",
    "gpt2_lora_config = LoraConfig(\n",
    "      task_type=TaskType.CAUSAL_LM,\n",
    "      inference_mode=False,\n",
    "      r=gpt2_model_args.lora_r,\n",
    "      lora_alpha=gpt2_model_args.lora_alpha,\n",
    "      lora_dropout=0.1,\n",
    "      target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    "      init_lora_weights=True,\n",
    "  )\n",
    "\n",
    "gpt2_model = CODI(gpt2_model_args,\n",
    "gpt2_training_args, gpt2_lora_config)\n",
    "gpt2_model = gpt2_model.to(device)\n",
    "gpt2_model = gpt2_model.to(torch.bfloat16)\n",
    "gpt2_model.eval()\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "print(\"✓ CODI-GPT-2 loaded successfully from HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run GPT-2 Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GPT-2 Forward Pass\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "gpt2_inputs = gpt2_tokenizer(question, return_tensors=\"pt\", add_special_tokens=True)\n",
    "gpt2_input_ids = gpt2_inputs.input_ids.to(device)\n",
    "\n",
    "print(f\"Input shape: {gpt2_input_ids.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    gpt2_outputs = gpt2_model.codi(\n",
    "        input_ids=gpt2_input_ids,\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "    gpt2_continuous_thoughts = gpt2_outputs.hidden_states[-1]\n",
    "\n",
    "print(f\"Continuous thoughts shape: {gpt2_continuous_thoughts.shape}\")\n",
    "print(f\"Number of thought positions: {gpt2_continuous_thoughts.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode GPT-2 Continuous Thoughts (like section5_analysis.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GPT-2: Decoding Continuous Thoughts\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Note: HuggingFace models don't have projection layers, decode directly\n",
    "print(f\"Continuous thoughts shape: {gpt2_continuous_thoughts.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "# Decode first 15 positions directly from continuous thoughts\n",
    "print(\"\\nDecoded tokens from continuous thoughts:\")\n",
    "gpt2_decoded_tokens = []\n",
    "gpt2_number_positions = []\n",
    "\n",
    "for i in range(min(15, gpt2_continuous_thoughts.shape[1])):\n",
    "    logits = gpt2_model.codi.lm_head(gpt2_continuous_thoughts[:, i, :])\n",
    "    top1_token_id = torch.argmax(logits, dim=-1).item()\n",
    "    top1_token_str = gpt2_tokenizer.decode([top1_token_id])\n",
    "\n",
    "    is_number = bool(number_regex.match(top1_token_str))\n",
    "    pos_type = \"BoT\" if i == 0 else f\"T{i}\"\n",
    "\n",
    "    gpt2_decoded_tokens.append((i, top1_token_id, top1_token_str, is_number))\n",
    "    if is_number:\n",
    "        gpt2_number_positions.append(i)\n",
    "\n",
    "    marker = \" ← NUMBER!\" if is_number else \"\"\n",
    "    print(f\"  {pos_type:4s} [pos={i:2d}]: token_id={top1_token_id:5d} → '{top1_token_str}'{marker}\")\n",
    "\n",
    "print(f\"\\n✓ Numbers detected at positions: {gpt2_number_positions}\")\n",
    "print(f\"✓ Total numbers: {len(gpt2_number_positions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2: Projection Intervention (Target Token = '5', k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GPT-2: Projection Intervention\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "target_token = '5'\n",
    "k = 3\n",
    "\n",
    "# Get target token embedding\n",
    "target_token_id = gpt2_tokenizer.encode(target_token, add_special_tokens=False)[0]\n",
    "embedding_layer = gpt2_model.codi.get_input_embeddings()\n",
    "target_embd = embedding_layer(torch.tensor([target_token_id], device=device))\n",
    "\n",
    "print(f\"Target token: '{target_token}'\")\n",
    "print(f\"Target token ID: {target_token_id}\")\n",
    "print(f\"k (top-k intervention): {k}\")\n",
    "\n",
    "gpt2_interventions = []\n",
    "\n",
    "for i in range(min(15, gpt2_latent_embd.shape[1])):\n",
    "    pos_type = \"BoT\" if i == 0 else f\"T{i}\"\n",
    "    \n",
    "    # Get predicted token\n",
    "    logits = gpt2_model.codi.lm_head(gpt2_latent_embd[:, i, :])\n",
    "    top1_token_id = torch.argmax(logits, dim=-1).item()\n",
    "    top1_token_str = gpt2_tokenizer.decode([top1_token_id])\n",
    "    \n",
    "    # Check if it's a number\n",
    "    is_number = bool(number_regex.match(top1_token_str))\n",
    "    \n",
    "    if is_number:\n",
    "        # Get predicted token embedding\n",
    "        predicted_embd = embedding_layer(torch.tensor([top1_token_id], device=device))\n",
    "        \n",
    "        # Get activation\n",
    "        A = gpt2_latent_embd[:, i, :]  # [1, hidden_dim]\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        E_pred_norm = predicted_embd / torch.norm(predicted_embd, dim=-1, keepdim=True)\n",
    "        E_target_norm = target_embd / torch.norm(target_embd, dim=-1, keepdim=True)\n",
    "        \n",
    "        # Projection removal and replacement\n",
    "        proj_predicted = torch.sum(A * E_pred_norm, dim=-1, keepdim=True) * E_pred_norm\n",
    "        proj_target = torch.sum(A * E_target_norm, dim=-1, keepdim=True) * E_target_norm\n",
    "        \n",
    "        A_modified = A - proj_predicted + k * proj_target\n",
    "        \n",
    "        # Decode modified activation\n",
    "        logits_modified = gpt2_model.codi.lm_head(A_modified)\n",
    "        new_token_id = torch.argmax(logits_modified, dim=-1).item()\n",
    "        new_token_str = gpt2_tokenizer.decode([new_token_id])\n",
    "        \n",
    "        gpt2_interventions.append({\n",
    "            'position': i,\n",
    "            'predicted_token': top1_token_str,\n",
    "            'new_token': new_token_str,\n",
    "            'intervened': True\n",
    "        })\n",
    "        \n",
    "        print(f\"{pos_type:4s} [pos={i:2d}]: '{top1_token_str}' → '{new_token_str}' (intervened)\")\n",
    "    else:\n",
    "        print(f\"{pos_type:4s} [pos={i:2d}]: '{top1_token_str}' (not a number, skipped)\")\n",
    "\n",
    "print(f\"\\n✓ GPT-2 interventions: {len(gpt2_interventions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Tokenizers (Sanity Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Tokenizer Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_numbers = ['0', '1', '2', '3', '4', '5', '16', '18']\n",
    "\n",
    "print(\"\\nHow do tokenizers encode/decode numbers?\\n\")\n",
    "for num in test_numbers:\n",
    "    gpt2_ids = gpt2_tokenizer.encode(num, add_special_tokens=False)\n",
    "    gpt2_decoded = gpt2_tokenizer.decode(gpt2_ids)\n",
    "    gpt2_matches = bool(number_regex.match(gpt2_decoded))\n",
    "    \n",
    "    llama_ids = llama_tokenizer.encode(num, add_special_tokens=False)\n",
    "    llama_decoded = llama_tokenizer.decode(llama_ids)\n",
    "    llama_matches = bool(number_regex.match(llama_decoded))\n",
    "    \n",
    "    match_indicator = \"✓\" if gpt2_decoded == llama_decoded else \"✗ MISMATCH\"\n",
    "    \n",
    "    print(f\"Number: '{num}'\")\n",
    "    print(f\"  GPT-2:  IDs={gpt2_ids} → '{gpt2_decoded}' (matches={gpt2_matches})\")\n",
    "    print(f\"  LLaMA:  IDs={llama_ids} → '{llama_decoded}' (matches={llama_matches})\")\n",
    "    print(f\"  {match_indicator}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nGPT-2:\")\n",
    "print(f\"  - Numbers detected: {len(gpt2_number_positions)}\")\n",
    "print(f\"  - Interventions: {len(gpt2_interventions)}\")\n",
    "\n",
    "print(f\"\\nLLaMA:\")\n",
    "print(f\"  - Numbers detected: {len(llama_number_positions)}\")\n",
    "print(f\"  - Interventions: {len(llama_interventions)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIAGNOSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(llama_number_positions) == 0:\n",
    "    print(\"\\n⚠️  ROOT CAUSE IDENTIFIED:\")\n",
    "    print(\"LLaMA's lm_head does NOT predict number tokens from continuous thoughts.\")\n",
    "    print(\"This explains why there are 0 interventions.\")\n",
    "    print(\"\\nPossible reasons:\")\n",
    "    print(\"  1. Projection layers (bot_projection/thought_projection) not trained correctly\")\n",
    "    print(\"  2. lm_head not loaded correctly (check strict=False in load_state_dict)\")\n",
    "    print(\"  3. Vocabulary size mismatch causing wrong token predictions\")\n",
    "    print(\"  4. Continuous thoughts from different distribution than GPT-2\")\n",
    "elif len(llama_number_positions) < len(gpt2_number_positions):\n",
    "    print(\"\\n⚠️  LLaMA detects fewer numbers than GPT-2\")\n",
    "    print(f\"  GPT-2: {len(gpt2_number_positions)} numbers\")\n",
    "    print(f\"  LLaMA: {len(llama_number_positions)} numbers\")\n",
    "else:\n",
    "    print(\"\\n✓ Both models detect similar numbers of tokens\")\n",
    "    print(\"  The issue may be in the intervention logic, not the decoding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Investigation: Top-5 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Top-5 Predictions Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nGPT-2 - First 5 thought positions:\")\n",
    "for i in range(min(5, gpt2_latent_embd.shape[1])):\n",
    "    logits = gpt2_model.codi.lm_head(gpt2_latent_embd[:, i, :])\n",
    "    top5_vals, top5_ids = torch.topk(logits[0], 5)\n",
    "    top5_tokens = [gpt2_tokenizer.decode([tid.item()]) for tid in top5_ids]\n",
    "    \n",
    "    pos_type = \"BoT\" if i == 0 else f\"T{i}\"\n",
    "    print(f\"\\n  {pos_type} [pos={i}]:\")\n",
    "    for j, (token, val) in enumerate(zip(top5_tokens, top5_vals)):\n",
    "        is_num = \"← NUM\" if number_regex.match(token) else \"\"\n",
    "        print(f\"    {j+1}. '{token}' (logit={val.item():.2f}) {is_num}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"\\nLLaMA - First 5 thought positions:\")\n",
    "for i in range(min(5, llama_latent_embd.shape[1])):\n",
    "    logits = llama_model.codi.lm_head(llama_latent_embd[:, i, :])\n",
    "    top5_vals, top5_ids = torch.topk(logits[0], 5)\n",
    "    top5_tokens = [llama_tokenizer.decode([tid.item()]) for tid in top5_ids]\n",
    "    \n",
    "    pos_type = \"BoT\" if i == 0 else f\"T{i}\"\n",
    "    print(f\"\\n  {pos_type} [pos={i}]:\")\n",
    "    for j, (token, val) in enumerate(zip(top5_tokens, top5_vals)):\n",
    "        is_num = \"← NUM\" if number_regex.match(token) else \"\"\n",
    "        print(f\"    {j+1}. '{token}' (logit={val.item():.2f}) {is_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Activation Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Activation Norms\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "gpt2_norms = torch.norm(gpt2_latent_embd[0], dim=-1)\n",
    "llama_norms = torch.norm(llama_latent_embd[0], dim=-1)\n",
    "\n",
    "print(f\"\\nGPT-2 latent embedding norms (first 10 positions):\")\n",
    "for i in range(min(10, len(gpt2_norms))):\n",
    "    print(f\"  Position {i}: {gpt2_norms[i].item():.2f}\")\n",
    "\n",
    "print(f\"\\nLLaMA latent embedding norms (first 10 positions):\")\n",
    "for i in range(min(10, len(llama_norms))):\n",
    "    print(f\"  Position {i}: {llama_norms[i].item():.2f}\")\n",
    "\n",
    "print(f\"\\nGPT-2 mean norm: {gpt2_norms.mean().item():.2f}\")\n",
    "print(f\"LLaMA mean norm: {llama_norms.mean().item():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
