{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Projection Intervention: GPT-2 vs LLaMA\n",
    "\n",
    "This notebook contains minimal working examples of CODI, focussing on LLAMA. There is also similar code for GPT-2, but it has not been tested properly.\n",
    "\n",
    "This includes:\n",
    "- loading model\n",
    "- generating continuous chain of thought\n",
    "- decoding continuous chain of thought\n",
    "- projection replacement of decoded number tokens by specific token like '5'\n",
    "- generating chain of thought with and without BOT (beginning of thought token)\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers torch peft matplotlib datasets tqdm hf_transfer dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(\"/workspace/CoT_Exploration/codi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, \"/workspace/CoT_Exploration/codi\")\n",
    "from src.model import CODI, ModelArguments, TrainingArguments\n",
    "from peft import LoraConfig, TaskType\n",
    "from transformers import AutoTokenizer\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test text:\n",
      "Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day. She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer's market. #### 18\n",
      "\n",
      "Length: 410 characters\n"
     ]
    }
   ],
   "source": [
    "# GSM8K example\n",
    "question = \"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\"\n",
    "answer = \"Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day. She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer's market. #### 18\"\n",
    "answer = 18\n",
    "\n",
    "test_text = f\"{question}\\n{answer}\"\n",
    "print(f\"Test text:\\n{test_text}\")\n",
    "print(f\"\\nLength: {len(test_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number detection regex\n",
    "number_regex = re.compile(r'^\\s?\\d+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CODI-LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to HuggingFace (optional - only needed  for gated models like LLaMA)\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "  # Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "  # Get HuggingFace token\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "if hf_token:\n",
    "      from huggingface_hub import login\n",
    "      login(token=hf_token)\n",
    "      print(\"‚úì Logged in to HuggingFace\")\n",
    "else:\n",
    "      print(\"‚ö† No HF_TOKEN found in .env file -  proceeding without authentication\")\n",
    "      print(\"  (This is fine if models are public)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "checkpoint_path = \"/workspace/CoT_Exploration/models/CODI-llama3.2-1b/pytorch_model.bin\"\n",
    "if os.path.exists(checkpoint_path):\n",
    "    size_mb = os.path.getsize(checkpoint_path) / (1024 * 1024)\n",
    "    print(f\"‚úì Checkpoint found: {size_mb:.1f} MB\")\n",
    "else:\n",
    "    print(f\"‚ùå Checkpoint NOT found at: {checkpoint_path}\")\n",
    "    print(f\"\\nChecking directory contents:\")\n",
    "    dir_path = os.path.dirname(checkpoint_path)\n",
    "    if os.path.exists(dir_path):\n",
    "        files = os.listdir(dir_path)\n",
    "        print(f\"Files in {dir_path}:\")\n",
    "        for f in files:\n",
    "            print(f\"  - {f}\")\n",
    "    else:\n",
    "        print(f\"Directory doesn't exist: {dir_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Loading CODI-LLaMA from Local Checkpoint\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "llama_model_args = ModelArguments(\n",
    "    model_name_or_path=\"meta-llama/Llama-3.2-1B\",\n",
    "    lora_init=True,\n",
    "    lora_r=128,\n",
    "    lora_alpha=32,\n",
    "    ckpt_dir=\"/workspace/CoT_Exploration/models/CODI-llama3.2-1b\",  # Local checkpoint\n",
    "    full_precision=True,\n",
    "    token=None\n",
    ")\n",
    "\n",
    "llama_training_args = TrainingArguments(\n",
    "    output_dir=\"./outputs\",\n",
    "    model_max_length=512,\n",
    "    inf_latent_iterations=6,\n",
    "    use_prj=True,\n",
    "    prj_dim=2048,\n",
    "    remove_eos=True,\n",
    "    greedy=True,\n",
    "    bf16=False,\n",
    "    inf_num_iterations=1\n",
    ")\n",
    "\n",
    "llama_lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=llama_model_args.lora_r,\n",
    "    lora_alpha=llama_model_args.lora_alpha,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    init_lora_weights=True,\n",
    ")\n",
    "\n",
    "# Create model with random initialization\n",
    "llama_model = CODI(llama_model_args, llama_training_args, llama_lora_config)\n",
    "\n",
    "# üî• LOAD THE TRAINED CHECKPOINT - THIS IS WHAT YOU WERE MISSING! üî•\n",
    "checkpoint_path = os.path.join(llama_model_args.ckpt_dir, \"pytorch_model.bin\")\n",
    "print(f\"\\nLoading checkpoint from: {checkpoint_path}\")\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    print(f\"‚ùå ERROR: Checkpoint not found at {checkpoint_path}\")\n",
    "    print(f\"   Please verify the path exists\")\n",
    "else:\n",
    "    state_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    # Load the trained weights\n",
    "    missing_keys, unexpected_keys = llama_model.load_state_dict(state_dict, strict=False)\n",
    "    \n",
    "    print(f\"‚úì Checkpoint loaded successfully!\")\n",
    "    if missing_keys:\n",
    "        print(f\"  ‚ö† Missing keys: {len(missing_keys)} (this is often normal for LoRA)\")\n",
    "    if unexpected_keys:\n",
    "        print(f\"  ‚ö† Unexpected keys: {len(unexpected_keys)}\")\n",
    "    \n",
    "    # Tie weights for LLaMA models (important!)\n",
    "    llama_model.codi.tie_weights()\n",
    "    print(f\"‚úì Weights tied\")\n",
    "\n",
    "# Move to device and set precision\n",
    "llama_model = llama_model.to(device)\n",
    "llama_model = llama_model.to(torch.bfloat16)\n",
    "llama_model.eval()\n",
    "\n",
    "print(f\"‚úì Model moved to {device} with bfloat16 precision\")\n",
    "\n",
    "# Load tokenizer\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "# Verify critical model components\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Model Verification:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"‚úì BoT token ID: {llama_model.bot_id}\")\n",
    "print(f\"‚úì EoT token ID: {llama_model.eot_id}\")\n",
    "print(f\"‚úì Projection layer: {'Yes' if hasattr(llama_model, 'prj') else 'No'}\")\n",
    "if hasattr(llama_model, 'prj'):\n",
    "    print(f\"  - Projection dim: {llama_training_args.prj_dim}\")\n",
    "print(f\"‚úì LoRA rank: {llama_model_args.lora_r}\")\n",
    "print(f\"‚úì Chain-of-thought iterations: {llama_training_args.inf_latent_iterations}\")\n",
    "\n",
    "print(\"\\n‚úì CODI-LLaMA loaded successfully with trained weights!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify CODI weights are loaded\n",
    "print(\"Checking if CODI weights are loaded:\")\n",
    "print(f\"Model checkpoint dir: {llama_model_args.ckpt_dir}\")\n",
    "print(f\"BoT token ID: {llama_model.bot_id}\")\n",
    "print(f\"EoT token ID: {llama_model.eot_id}\")\n",
    "print(f\"Using projection: {llama_training_args.use_prj}\")\n",
    "print(f\"Projection dim: {llama_training_args.prj_dim if llama_training_args.use_prj else 'N/A'}\")\n",
    "\n",
    "# Check if projection layer exists\n",
    "if hasattr(llama_model, 'prj'):\n",
    "    print(f\"‚úì Projection layer found: {llama_model.prj}\")\n",
    "else:\n",
    "    print(\"‚úó No projection layer - this might be the problem!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LLaMA Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?\"\n",
    "answer = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LLaMA: Running inference with Chain-of-Thought\n",
      "================================================================================\n",
      "WITH BoT - Input IDs: tensor([[128000,     42,   4010,    277,   4024,    311,    279,   3637,    311,\n",
      "           3780,  29247,    369,    813,    502,  13455,     13,   3861,   9168,\n",
      "           7194,    400,     20,     11,    719,   1475,   2132,   9168,   7194,\n",
      "           1193,    220,   1399,      4,    315,    279,   3430,     13,    735,\n",
      "           4010,    277,   6944,    311,   3780,    220,    845,  29247,     13,\n",
      "           2650,   1790,   1587,    568,   1205,    311,   2343,    369,   1124,\n",
      "             30, 128257]], device='cuda:0')\n",
      "WITH BoT - Last 5 token IDs: [2343, 369, 1124, 30, 128257]\n",
      "WITH BoT - BoT token ID should be: 128257\n",
      "WITH BoT - Last token is BoT?: True\n",
      "Input shape: torch.Size([1, 56])\n",
      "‚úì Position 0 (BoT): shape=torch.Size([1, 1, 2048])\n",
      "‚úì Position 1 (T1): shape=torch.Size([1, 1, 2048])\n",
      "‚úì Position 2 (T2): shape=torch.Size([1, 1, 2048])\n",
      "‚úì Position 3 (T3): shape=torch.Size([1, 1, 2048])\n",
      "‚úì Position 4 (T4): shape=torch.Size([1, 1, 2048])\n",
      "‚úì Position 5 (T5): shape=torch.Size([1, 1, 2048])\n",
      "‚úì Position 6 (T6): shape=torch.Size([1, 1, 2048])\n",
      "\n",
      "Continuous thoughts shape: torch.Size([1, 7, 2048])\n",
      "Number of thought positions: 7\n",
      "‚úì SUCCESS: Collected exactly 7 CoT positions!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LLaMA: Running inference with Chain-of-Thought\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 1\n",
    "\n",
    "question = question\n",
    "questions = [question]\n",
    "\n",
    "# Tokenize with BoT token\n",
    "if llama_training_args.remove_eos:\n",
    "    bot_tensor = torch.tensor([llama_model.bot_id], dtype=torch.long).expand(batch_size, 1).to(device)\n",
    "else:\n",
    "    bot_tensor = torch.tensor([llama_tokenizer.eos_token_id, llama_model.bot_id], \n",
    "                              dtype=torch.long).expand(batch_size, 2).to(device)\n",
    "\n",
    "inputs = llama_tokenizer(questions, return_tensors=\"pt\", padding=False)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "inputs[\"input_ids\"] = torch.cat((inputs[\"input_ids\"], bot_tensor), dim=1)\n",
    "inputs[\"attention_mask\"] = torch.cat((inputs[\"attention_mask\"], torch.ones_like(bot_tensor)), dim=1)\n",
    "\n",
    "print(f\"WITH BoT - Input IDs: {inputs['input_ids']}\")\n",
    "print(f\"WITH BoT - Last 5 token IDs: {inputs['input_ids'][0, -5:].tolist()}\")\n",
    "print(f\"WITH BoT - BoT token ID should be: {llama_model.bot_id}\")\n",
    "print(f\"WITH BoT - Last token is BoT?: {inputs['input_ids'][0, -1].item() == llama_model.bot_id}\")\n",
    "\n",
    "\n",
    "print(f\"Input shape: {inputs['input_ids'].shape}\")\n",
    "\n",
    "# Store only the 7 chain-of-thought hidden states\n",
    "cot_hidden_states = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Initial encoding (position 0: BoT)\n",
    "    past_key_values = None\n",
    "    outputs = llama_model.codi(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        use_cache=True,\n",
    "        output_hidden_states=True,\n",
    "        past_key_values=past_key_values,\n",
    "        attention_mask=inputs[\"attention_mask\"]\n",
    "    )\n",
    "    past_key_values = outputs.past_key_values\n",
    "    \n",
    "    # üî• CRITICAL: Only take the LAST position, not all positions\n",
    "    latent_embd = outputs.hidden_states[-1][:, -1:, :]  # Shape: [batch, 1, hidden_dim]\n",
    "    \n",
    "    # Store BoT position (BEFORE projection)\n",
    "    cot_hidden_states.append(latent_embd.clone())\n",
    "    print(f\"‚úì Position 0 (BoT): shape={latent_embd.shape}\")\n",
    "    \n",
    "    # Apply initial projection\n",
    "    if llama_training_args.use_prj:\n",
    "        latent_embd = llama_model.prj(latent_embd)\n",
    "    \n",
    "    # Chain-of-Thought iterations (positions 1-6)\n",
    "    for i in range(llama_training_args.inf_latent_iterations):\n",
    "        outputs = llama_model.codi(\n",
    "            inputs_embeds=latent_embd,\n",
    "            use_cache=True,\n",
    "            output_hidden_states=True,\n",
    "            past_key_values=past_key_values\n",
    "        )\n",
    "        past_key_values = outputs.past_key_values\n",
    "        \n",
    "        # üî• CRITICAL: Only take the LAST position\n",
    "        latent_embd = outputs.hidden_states[-1][:, -1:, :]  # Shape: [batch, 1, hidden_dim]\n",
    "        \n",
    "        # Store BEFORE projection\n",
    "        cot_hidden_states.append(latent_embd.clone())\n",
    "        print(f\"‚úì Position {i+1} (T{i+1}): shape={latent_embd.shape}\")\n",
    "        \n",
    "        # Apply projection for next iteration\n",
    "        if llama_training_args.use_prj:\n",
    "            latent_embd = llama_model.prj(latent_embd)\n",
    "\n",
    "# Stack all 7 positions\n",
    "llama_continuous_thoughts = torch.cat(cot_hidden_states, dim=1)\n",
    "print(f\"\\nContinuous thoughts shape: {llama_continuous_thoughts.shape}\")\n",
    "print(f\"Number of thought positions: {llama_continuous_thoughts.shape[1]}\")\n",
    "\n",
    "# Verify we have exactly 7 positions\n",
    "if llama_continuous_thoughts.shape[1] != 7:\n",
    "    print(f\"\\n‚ùå ERROR: Expected 7 positions, got {llama_continuous_thoughts.shape[1]}\")\n",
    "else:\n",
    "    print(f\"‚úì SUCCESS: Collected exactly 7 CoT positions!\")\n",
    "\n",
    "# Also create llama_latent_embd as alias for compatibility with other cells\n",
    "llama_latent_embd = llama_continuous_thoughts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode LLaMA Continuous Thoughts (like section5_analysis.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LLaMA: Decoding Continuous Thoughts\n",
      "================================================================================\n",
      "Continuous thoughts shape: torch.Size([1, 7, 2048])\n",
      "\n",
      "Decoded tokens from continuous thoughts:\n",
      "  BoT  [pos= 0]: token_id=   23 ‚Üí '8' ‚Üê NUMBER!\n",
      "  T1   [pos= 1]: token_id= 1272 ‚Üí '40' ‚Üê NUMBER!\n",
      "  T2   [pos= 2]: token_id= 1490 ‚Üí '80' ‚Üê NUMBER!\n",
      "  T3   [pos= 3]: token_id=  320 ‚Üí ' ('\n",
      "  T4   [pos= 4]: token_id= 1187 ‚Üí '24' ‚Üê NUMBER!\n",
      "  T5   [pos= 5]: token_id= 1187 ‚Üí '24' ‚Üê NUMBER!\n",
      "  T6   [pos= 6]: token_id= 2511 ‚Üí '>>'\n",
      "\n",
      "‚úì Numbers detected at positions: [0, 1, 2, 4, 5]\n",
      "‚úì Total numbers: 5\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LLaMA: Decoding Continuous Thoughts\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Note: HuggingFace models don't have projection layers, decode directly\n",
    "print(f\"Continuous thoughts shape: {llama_continuous_thoughts.shape}\")\n",
    "\n",
    "# Decode first positions directly from continuous thoughts\n",
    "print(\"\\nDecoded tokens from continuous thoughts:\")\n",
    "llama_decoded_tokens = []\n",
    "llama_number_positions = []\n",
    "\n",
    "for i in range(min(50, llama_continuous_thoughts.shape[1])):\n",
    "    logits = llama_model.codi.lm_head(llama_continuous_thoughts[:, i, :])\n",
    "    top1_token_id = torch.argmax(logits, dim=-1).item()\n",
    "    top1_token_str = llama_tokenizer.decode([top1_token_id])\n",
    "\n",
    "    is_number = bool(number_regex.match(top1_token_str))\n",
    "    pos_type = \"BoT\" if i == 0 else f\"T{i}\"\n",
    "\n",
    "    llama_decoded_tokens.append((i, top1_token_id, top1_token_str, is_number))\n",
    "    if is_number:\n",
    "        llama_number_positions.append(i)\n",
    "\n",
    "    marker = \" ‚Üê NUMBER!\" if is_number else \"\"\n",
    "    print(f\"  {pos_type:4s} [pos={i:2d}]: token_id={top1_token_id:5d} ‚Üí '{top1_token_str}'{marker}\")\n",
    "\n",
    "print(f\"\\n‚úì Numbers detected at positions: {llama_number_positions}\")\n",
    "print(f\"‚úì Total numbers: {len(llama_number_positions)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LLaMA: Generate Final Answer\n",
      "================================================================================\n",
      "Stopped at step 0 (found number: '64')\n",
      "\n",
      "Extracted numerical answer: 64.0\n",
      "Expected answer: 64\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LLaMA: Generate Final Answer\")\n",
    "print(\"=\"*80)\n",
    "with torch.no_grad():\n",
    "    # Signal end-of-thought with EOT token\n",
    "    if llama_training_args.remove_eos:\n",
    "        eot_tensor = torch.tensor([[llama_model.eot_id]], dtype=torch.long).expand(batch_size, 1, 1).to(device)\n",
    "    else:\n",
    "        eot_tensor = torch.tensor([[llama_tokenizer.eos_token_id, llama_model.eot_id]], \n",
    "                                   dtype=torch.long).expand(batch_size, 1, 2).to(device)\n",
    "    \n",
    "    eot_emb = llama_model.get_embd(llama_model.codi, llama_model.model_name)(eot_tensor).squeeze(1)\n",
    "    output = eot_emb\n",
    "    \n",
    "    # Generate answer tokens autoregressively\n",
    "    pred_tokens = []\n",
    "    max_length = 256\n",
    "    \n",
    "    for step in range(max_length):\n",
    "        out = llama_model.codi(\n",
    "            inputs_embeds=output,\n",
    "            output_hidden_states=False,\n",
    "            attention_mask=None,\n",
    "            use_cache=True,\n",
    "            output_attentions=False,\n",
    "            past_key_values=past_key_values\n",
    "        )\n",
    "        past_key_values = out.past_key_values\n",
    "        \n",
    "        # Get logits for vocabulary tokens only\n",
    "        logits = out.logits[:, -1, :llama_model.codi.config.vocab_size-1]\n",
    "        next_token_id = torch.argmax(logits, dim=-1).item()\n",
    "        \n",
    "        pred_tokens.append(next_token_id)\n",
    "        \n",
    "        # Decode current token\n",
    "        current_token_str = llama_tokenizer.decode([next_token_id])\n",
    "        \n",
    "        # Stop if EOS token\n",
    "        if next_token_id == llama_tokenizer.eos_token_id:\n",
    "            print(f\"Stopped at step {step} (EOS token)\")\n",
    "            break\n",
    "        \n",
    "        # Stop immediately after generating a number token\n",
    "        if number_regex.match(current_token_str.strip()):\n",
    "            print(f\"Stopped at step {step} (found number: '{current_token_str}')\")\n",
    "            break\n",
    "        \n",
    "        # Hard limit to prevent loops\n",
    "        if step >= 49:\n",
    "            print(f\"Stopped at step {step} (max length)\")\n",
    "            break\n",
    "        \n",
    "        # Prepare next input\n",
    "        output = llama_model.get_embd(llama_model.codi, llama_model.model_name)(\n",
    "            torch.tensor([[next_token_id]], device=device)\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # Extract numerical answer\n",
    "    def extract_answer_number(text):\n",
    "        text = text.replace(',', '')\n",
    "        numbers = [s for s in re.findall(r'-?\\d+\\.?\\d*', text)]\n",
    "        if not numbers:\n",
    "            return None\n",
    "        return float(numbers[-1])\n",
    "    \n",
    "    predicted_number = extract_answer_number(full_answer)\n",
    "    print(f\"\\nExtracted numerical answer: {predicted_number}\")\n",
    "    print(f\"Expected answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CODI-GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Loading CODI-GPT-2 from Local Checkpoint\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "  # Load from HuggingFace Hub\n",
    "gpt2_model_name = \"lhao499/codi-gpt2\"\n",
    "\n",
    "gpt2_model_args = ModelArguments(\n",
    "      model_name_or_path=\"gpt2\",\n",
    "      lora_init=True,\n",
    "      lora_r=128,\n",
    "      lora_alpha=32,\n",
    "      ckpt_dir=\"/workspace/CoT_Exploration/models/CODI-gpt2\",  # Local checkpoint\n",
    "      full_precision=True,\n",
    "      token=None\n",
    "  )\n",
    "\n",
    "gpt2_training_args = TrainingArguments(\n",
    "      output_dir=\"./outputs\",\n",
    "      model_max_length=512,\n",
    "      inf_latent_iterations=6,\n",
    "      use_prj=True,\n",
    "      prj_dim=768,\n",
    "      remove_eos=True,\n",
    "      greedy=True,\n",
    "      bf16=False,\n",
    "      inf_num_iterations=1\n",
    "  )\n",
    "\n",
    "gpt2_lora_config = LoraConfig(\n",
    "      task_type=TaskType.CAUSAL_LM,\n",
    "      inference_mode=False,\n",
    "      r=gpt2_model_args.lora_r,\n",
    "      lora_alpha=gpt2_model_args.lora_alpha,\n",
    "      lora_dropout=0.1,\n",
    "      target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    "      init_lora_weights=True,\n",
    "  )\n",
    "\n",
    "gpt2_model = CODI(gpt2_model_args,\n",
    "gpt2_training_args, gpt2_lora_config)\n",
    "gpt2_model = gpt2_model.to(device)\n",
    "gpt2_model = gpt2_model.to(torch.bfloat16)\n",
    "gpt2_model.eval()\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "print(\"‚úì CODI-GPT-2 loaded successfully from HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run GPT-2 Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GPT-2 Forward Pass\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "gpt2_inputs = gpt2_tokenizer(question, return_tensors=\"pt\", add_special_tokens=True)\n",
    "gpt2_input_ids = gpt2_inputs.input_ids.to(device)\n",
    "\n",
    "print(f\"Input shape: {gpt2_input_ids.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    gpt2_outputs = gpt2_model.codi(\n",
    "        input_ids=gpt2_input_ids,\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "    gpt2_continuous_thoughts = gpt2_outputs.hidden_states[-1]\n",
    "\n",
    "print(f\"Continuous thoughts shape: {gpt2_continuous_thoughts.shape}\")\n",
    "print(f\"Number of thought positions: {gpt2_continuous_thoughts.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode GPT-2 Continuous Thoughts (like section5_analysis.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GPT-2: Decoding Continuous Thoughts\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Note: HuggingFace models don't have projection layers, decode directly\n",
    "print(f\"Continuous thoughts shape: {gpt2_continuous_thoughts.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "# Decode first 15 positions directly from continuous thoughts\n",
    "print(\"\\nDecoded tokens from continuous thoughts:\")\n",
    "gpt2_decoded_tokens = []\n",
    "gpt2_number_positions = []\n",
    "\n",
    "for i in range(min(15, gpt2_continuous_thoughts.shape[1])):\n",
    "    logits = gpt2_model.codi.lm_head(gpt2_continuous_thoughts[:, i, :])\n",
    "    top1_token_id = torch.argmax(logits, dim=-1).item()\n",
    "    top1_token_str = gpt2_tokenizer.decode([top1_token_id])\n",
    "\n",
    "    is_number = bool(number_regex.match(top1_token_str))\n",
    "    pos_type = \"BoT\" if i == 0 else f\"T{i}\"\n",
    "\n",
    "    gpt2_decoded_tokens.append((i, top1_token_id, top1_token_str, is_number))\n",
    "    if is_number:\n",
    "        gpt2_number_positions.append(i)\n",
    "\n",
    "    marker = \" ‚Üê NUMBER!\" if is_number else \"\"\n",
    "    print(f\"  {pos_type:4s} [pos={i:2d}]: token_id={top1_token_id:5d} ‚Üí '{top1_token_str}'{marker}\")\n",
    "\n",
    "print(f\"\\n‚úì Numbers detected at positions: {gpt2_number_positions}\")\n",
    "print(f\"‚úì Total numbers: {len(gpt2_number_positions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2: Projection Intervention (Target Token = '5', k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GPT-2: Projection Intervention\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "target_token = '5'\n",
    "k = 3\n",
    "\n",
    "# Get target token embedding\n",
    "target_token_id = gpt2_tokenizer.encode(target_token, add_special_tokens=False)[0]\n",
    "embedding_layer = gpt2_model.codi.get_input_embeddings()\n",
    "target_embd = embedding_layer(torch.tensor([target_token_id], device=device))\n",
    "\n",
    "print(f\"Target token: '{target_token}'\")\n",
    "print(f\"Target token ID: {target_token_id}\")\n",
    "print(f\"k (top-k intervention): {k}\")\n",
    "\n",
    "gpt2_interventions = []\n",
    "\n",
    "for i in range(min(15, gpt2_latent_embd.shape[1])):\n",
    "    pos_type = \"BoT\" if i == 0 else f\"T{i}\"\n",
    "    \n",
    "    # Get predicted token\n",
    "    logits = gpt2_model.codi.lm_head(gpt2_latent_embd[:, i, :])\n",
    "    top1_token_id = torch.argmax(logits, dim=-1).item()\n",
    "    top1_token_str = gpt2_tokenizer.decode([top1_token_id])\n",
    "    \n",
    "    # Check if it's a number\n",
    "    is_number = bool(number_regex.match(top1_token_str))\n",
    "    \n",
    "    if is_number:\n",
    "        # Get predicted token embedding\n",
    "        predicted_embd = embedding_layer(torch.tensor([top1_token_id], device=device))\n",
    "        \n",
    "        # Get activation\n",
    "        A = gpt2_latent_embd[:, i, :]  # [1, hidden_dim]\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        E_pred_norm = predicted_embd / torch.norm(predicted_embd, dim=-1, keepdim=True)\n",
    "        E_target_norm = target_embd / torch.norm(target_embd, dim=-1, keepdim=True)\n",
    "        \n",
    "        # Projection removal and replacement\n",
    "        proj_predicted = torch.sum(A * E_pred_norm, dim=-1, keepdim=True) * E_pred_norm\n",
    "        proj_target = torch.sum(A * E_target_norm, dim=-1, keepdim=True) * E_target_norm\n",
    "        \n",
    "        A_modified = A - proj_predicted + k * proj_target\n",
    "        \n",
    "        # Decode modified activation\n",
    "        logits_modified = gpt2_model.codi.lm_head(A_modified)\n",
    "        new_token_id = torch.argmax(logits_modified, dim=-1).item()\n",
    "        new_token_str = gpt2_tokenizer.decode([new_token_id])\n",
    "        \n",
    "        gpt2_interventions.append({\n",
    "            'position': i,\n",
    "            'predicted_token': top1_token_str,\n",
    "            'new_token': new_token_str,\n",
    "            'intervened': True\n",
    "        })\n",
    "        \n",
    "        print(f\"{pos_type:4s} [pos={i:2d}]: '{top1_token_str}' ‚Üí '{new_token_str}' (intervened)\")\n",
    "    else:\n",
    "        print(f\"{pos_type:4s} [pos={i:2d}]: '{top1_token_str}' (not a number, skipped)\")\n",
    "\n",
    "print(f\"\\n‚úì GPT-2 interventions: {len(gpt2_interventions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMA: Projection Intervention (Target Token = '5', k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LLaMA: CAUSAL Projection Intervention\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# CRITICAL: Reset to ensure clean state\n",
    "torch.manual_seed(42)  # For reproducibility\n",
    "past_key_values = None  # Clear any cached state\n",
    "\n",
    "target_token = '5'\n",
    "k = 3\n",
    "\n",
    "# Get target token embedding\n",
    "target_token_id = llama_tokenizer.encode(target_token, add_special_tokens=False)[0]\n",
    "embedding_layer = llama_model.codi.get_input_embeddings()\n",
    "target_embd = embedding_layer(torch.tensor([target_token_id], device=device))\n",
    "\n",
    "print(f\"Target token: '{target_token}'\")\n",
    "print(f\"Target token ID: {target_token_id}\")\n",
    "print(f\"k (top-k intervention): {k}\")\n",
    "print(f\"\\nRunning CAUSAL intervention (affects downstream positions)...\\n\")\n",
    "\n",
    "# Re-run the chain-of-thought with interventions\n",
    "batch_size = 1\n",
    "\n",
    "# Set pad token if needed\n",
    "if llama_tokenizer.pad_token is None:\n",
    "    llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "    llama_tokenizer.pad_token_id = llama_tokenizer.eos_token_id\n",
    "\n",
    "# Tokenize with BoT token\n",
    "if llama_training_args.remove_eos:\n",
    "    bot_tensor = torch.tensor([llama_model.bot_id], dtype=torch.long).expand(batch_size, 1).to(device)\n",
    "else:\n",
    "    bot_tensor = torch.tensor([llama_tokenizer.eos_token_id, llama_model.bot_id], \n",
    "                              dtype=torch.long).expand(batch_size, 2).to(device)\n",
    "\n",
    "inputs = llama_tokenizer([question], return_tensors=\"pt\", padding=False)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "inputs[\"input_ids\"] = torch.cat((inputs[\"input_ids\"], bot_tensor), dim=1)\n",
    "inputs[\"attention_mask\"] = torch.cat((inputs[\"attention_mask\"], torch.ones_like(bot_tensor)), dim=1)\n",
    "\n",
    "llama_interventions = []\n",
    "intervened_positions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Initial encoding (position 0: BoT)\n",
    "    past_key_values = None\n",
    "    outputs = llama_model.codi(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        use_cache=True,\n",
    "        output_hidden_states=True,\n",
    "        past_key_values=past_key_values,\n",
    "        attention_mask=inputs[\"attention_mask\"]\n",
    "    )\n",
    "    past_key_values = outputs.past_key_values\n",
    "    latent_embd = outputs.hidden_states[-1][:, -1:, :]\n",
    "    \n",
    "    # Check BoT position\n",
    "    logits = llama_model.codi.lm_head(latent_embd.squeeze(1))\n",
    "    top1_token_id = torch.argmax(logits, dim=-1).item()\n",
    "    top1_token_str = llama_tokenizer.decode([top1_token_id])\n",
    "    is_number = bool(number_regex.match(top1_token_str))\n",
    "    \n",
    "    # Intervene at BoT if it's a number\n",
    "    if is_number:\n",
    "        predicted_embd = embedding_layer(torch.tensor([top1_token_id], device=device))\n",
    "        A = latent_embd.squeeze(1)\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        E_pred_norm = predicted_embd / torch.norm(predicted_embd, dim=-1, keepdim=True)\n",
    "        E_target_norm = target_embd / torch.norm(target_embd, dim=-1, keepdim=True)\n",
    "        \n",
    "        # Projection intervention\n",
    "        proj_predicted = torch.sum(A * E_pred_norm, dim=-1, keepdim=True) * E_pred_norm\n",
    "        proj_target = torch.norm(proj_predicted, dim =-1, keepdim=True) * E_target_norm\n",
    "        #proj_target = torch.sum(A * E_target_norm, dim=-1, keepdim=True) * E_target_norm\n",
    "        A_modified = A - proj_predicted + k * proj_target\n",
    "        \n",
    "        # Check what it decodes to after intervention\n",
    "        logits_modified = llama_model.codi.lm_head(A_modified)\n",
    "        new_token_id = torch.argmax(logits_modified, dim=-1).item()\n",
    "        new_token_str = llama_tokenizer.decode([new_token_id])\n",
    "        \n",
    "        # CAUSALLY apply the intervention\n",
    "        latent_embd = A_modified.unsqueeze(1)\n",
    "        \n",
    "        llama_interventions.append({\n",
    "            'position': 0,\n",
    "            'predicted_token': top1_token_str,\n",
    "            'new_token': new_token_str,\n",
    "            'intervened': True\n",
    "        })\n",
    "        intervened_positions.append(0)\n",
    "        print(f\"BoT  [pos= 0]: '{top1_token_str}' ‚Üí '{new_token_str}' ‚úì INTERVENED (causal)\")\n",
    "    else:\n",
    "        print(f\"BoT  [pos= 0]: '{top1_token_str}' (not a number, no intervention)\")\n",
    "    \n",
    "    # Apply initial projection\n",
    "    if llama_training_args.use_prj:\n",
    "        latent_embd = llama_model.prj(latent_embd)\n",
    "    \n",
    "    # Chain-of-Thought iterations (positions 1-6)\n",
    "    for i in range(llama_training_args.inf_latent_iterations):\n",
    "        outputs = llama_model.codi(\n",
    "            inputs_embeds=latent_embd,\n",
    "            use_cache=True,\n",
    "            output_hidden_states=True,\n",
    "            past_key_values=past_key_values\n",
    "        )\n",
    "        past_key_values = outputs.past_key_values\n",
    "        latent_embd = outputs.hidden_states[-1][:, -1:, :]\n",
    "        \n",
    "        # Check this position\n",
    "        logits = llama_model.codi.lm_head(latent_embd.squeeze(1))\n",
    "        top1_token_id = torch.argmax(logits, dim=-1).item()\n",
    "        top1_token_str = llama_tokenizer.decode([top1_token_id])\n",
    "        is_number = bool(number_regex.match(top1_token_str))\n",
    "        \n",
    "        pos_type = f\"T{i+1}\"\n",
    "        \n",
    "        # Intervene if it's a number\n",
    "        if is_number:\n",
    "            predicted_embd = embedding_layer(torch.tensor([top1_token_id], device=device))\n",
    "            A = latent_embd.squeeze(1)\n",
    "            \n",
    "            # Normalize embeddings\n",
    "            E_pred_norm = predicted_embd / torch.norm(predicted_embd, dim=-1, keepdim=True)\n",
    "            E_target_norm = target_embd / torch.norm(target_embd, dim=-1, keepdim=True)\n",
    "            \n",
    "            # Projection intervention\n",
    "            proj_predicted = torch.sum(A * E_pred_norm, dim=-1, keepdim=True) * E_pred_norm\n",
    "            proj_target = torch.sum(A * E_target_norm, dim=-1, keepdim=True) * E_target_norm\n",
    "            A_modified = A - proj_predicted + k * proj_target\n",
    "            \n",
    "            # Check what it decodes to after intervention\n",
    "            logits_modified = llama_model.codi.lm_head(A_modified)\n",
    "            new_token_id = torch.argmax(logits_modified, dim=-1).item()\n",
    "            new_token_str = llama_tokenizer.decode([new_token_id])\n",
    "            \n",
    "            # CAUSALLY apply the intervention\n",
    "            latent_embd = A_modified.unsqueeze(1)\n",
    "            \n",
    "            llama_interventions.append({\n",
    "                'position': i+1,\n",
    "                'predicted_token': top1_token_str,\n",
    "                'new_token': new_token_str,\n",
    "                'intervened': True\n",
    "            })\n",
    "            intervened_positions.append(i+1)\n",
    "            print(f\"{pos_type:4s} [pos={i+1:2d}]: '{top1_token_str}' ‚Üí '{new_token_str}' ‚úì INTERVENED (causal)\")\n",
    "        else:\n",
    "            print(f\"{pos_type:4s} [pos={i+1:2d}]: '{top1_token_str}' (not a number, no intervention)\")\n",
    "        \n",
    "        # Apply projection for next iteration\n",
    "        if llama_training_args.use_prj:\n",
    "            latent_embd = llama_model.prj(latent_embd)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"‚úì Total interventions: {len(llama_interventions)}\")\n",
    "print(f\"‚úì Intervened at positions: {intervened_positions}\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Now generate the final answer with the intervened chain-of-thought\n",
    "print(\"\\nGenerating final answer with intervened CoT...\")\n",
    "\n",
    "# Signal end-of-thought with EOT token\n",
    "if llama_training_args.remove_eos:\n",
    "    eot_tensor = torch.tensor([[llama_model.eot_id]], dtype=torch.long).expand(batch_size, 1, 1).to(device)\n",
    "else:\n",
    "    eot_tensor = torch.tensor([[llama_tokenizer.eos_token_id, llama_model.eot_id]], \n",
    "                               dtype=torch.long).expand(batch_size, 1, 2).to(device)\n",
    "\n",
    "eot_emb = llama_model.get_embd(llama_model.codi, llama_model.model_name)(eot_tensor).squeeze(1)\n",
    "output = eot_emb\n",
    "\n",
    "# Generate answer tokens\n",
    "# Generate answer tokens\n",
    "pred_tokens = []\n",
    "found_number = False\n",
    "\n",
    "for step in range(256):\n",
    "    out = llama_model.codi(\n",
    "        inputs_embeds=output,\n",
    "        output_hidden_states=False,\n",
    "        attention_mask=None,\n",
    "        use_cache=True,\n",
    "        output_attentions=False,\n",
    "        past_key_values=past_key_values\n",
    "    )\n",
    "    past_key_values = out.past_key_values\n",
    "    \n",
    "    logits = out.logits[:, -1, :llama_model.codi.config.vocab_size-1]\n",
    "    next_token_id = torch.argmax(logits, dim=-1).item()\n",
    "    pred_tokens.append(next_token_id)\n",
    "    \n",
    "    # Decode current token\n",
    "    current_token_str = llama_tokenizer.decode([next_token_id])\n",
    "    \n",
    "    # Stop if EOS token\n",
    "    if next_token_id == llama_tokenizer.eos_token_id:\n",
    "        print(f\"Stopped at step {step} (EOS token)\")\n",
    "        break\n",
    "    \n",
    "    # Stop immediately after generating a number token\n",
    "    if number_regex.match(current_token_str.strip()):\n",
    "        print(f\"Stopped at step {step} (found number: '{current_token_str}')\")\n",
    "        break\n",
    "    \n",
    "    # Hard limit to prevent loops\n",
    "    if step >= 49:\n",
    "        print(f\"Stopped at step {step} (max length)\")\n",
    "        break\n",
    "    \n",
    "    output = llama_model.get_embd(llama_model.codi, llama_model.model_name)(\n",
    "        torch.tensor([[next_token_id]], device=device)\n",
    "    )\n",
    "# Decode and extract answer\n",
    "intervened_answer = llama_tokenizer.decode(pred_tokens, skip_special_tokens=True)\n",
    "print(f\"\\nIntervened Answer: {intervened_answer}\")\n",
    "\n",
    "def extract_answer_number(text):\n",
    "    text = text.replace(',', '')\n",
    "    numbers = [s for s in re.findall(r'-?\\d+\\.?\\d*', text)]\n",
    "    if not numbers:\n",
    "        return None\n",
    "    return float(numbers[0])\n",
    "\n",
    "intervened_number = extract_answer_number(intervened_answer)\n",
    "print(f\"Intervened numerical answer: {intervened_number}\")\n",
    "print(f\"\\n(Compare to original expected: 18)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LLaMA: Chain-of-Thought WITHOUT Beginning-of-Thought Token\n",
      "================================================================================\n",
      "WITHOUT BoT - Input IDs: tensor([[128000,     42,   4010,    277,   4024,    311,    279,   3637,    311,\n",
      "           3780,  29247,    369,    813,    502,  13455,     13,   3861,   9168,\n",
      "           7194,    400,     20,     11,    719,   1475,   2132,   9168,   7194,\n",
      "           1193,    220,   1399,      4,    315,    279,   3430,     13,    735,\n",
      "           4010,    277,   6944,    311,   3780,    220,    845,  29247,     13,\n",
      "           2650,   1790,   1587,    568,   1205,    311,   2343,    369,   1124,\n",
      "             30]], device='cuda:0')\n",
      "WITHOUT BoT - Last 5 token IDs: [311, 2343, 369, 1124, 30]\n",
      "WITHOUT BoT - BoT token ID should be: 128257\n",
      "WITHOUT BoT - Contains BoT?: False\n",
      "Input shape (no BoT): torch.Size([1, 55])\n",
      "‚úì Position 0 (T0): shape=torch.Size([1, 1, 2048])\n",
      "‚úì Position 1 (T1): shape=torch.Size([1, 1, 2048])\n",
      "‚úì Position 2 (T2): shape=torch.Size([1, 1, 2048])\n",
      "‚úì Position 3 (T3): shape=torch.Size([1, 1, 2048])\n",
      "‚úì Position 4 (T4): shape=torch.Size([1, 1, 2048])\n",
      "‚úì Position 5 (T5): shape=torch.Size([1, 1, 2048])\n",
      "\n",
      "‚úì Total continuous thoughts (no BoT): torch.Size([1, 6, 2048])\n",
      "  Expected: [1, 6, hidden_dim]\n",
      "  Got: [1, 6, 2048]\n",
      "\n",
      "‚úì SUCCESS: Collected exactly 6 CoT positions (no BoT)!\n",
      "\n",
      "================================================================================\n",
      "Decoding Continuous Thoughts (no BoT)\n",
      "================================================================================\n",
      "  T0   [pos= 0]: token_id= 1272 ‚Üí '40' üî¢ NUMBER!\n",
      "  T1   [pos= 1]: token_id= 1490 ‚Üí '80' üî¢ NUMBER!\n",
      "  T2   [pos= 2]: token_id=  320 ‚Üí ' ('\n",
      "  T3   [pos= 3]: token_id= 1187 ‚Üí '24' üî¢ NUMBER!\n",
      "  T4   [pos= 4]: token_id= 1187 ‚Üí '24' üî¢ NUMBER!\n",
      "  T5   [pos= 5]: token_id=   28 ‚Üí '='\n",
      "\n",
      "üìä Numbers at positions: [0, 1, 3, 4]\n",
      "üìä Total numbers: 4/6\n",
      "\n",
      "================================================================================\n",
      "Generate Final Answer (no BoT)\n",
      "================================================================================\n",
      "Stopped at step 5 (found number: '64')\n",
      "\n",
      "Generated Answer (no BoT): The answer is: 64\n",
      "Qeustion: Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?\n",
      "Numerical answer (no BoT): 64.0\n",
      "Expected answer:  64\n",
      "\n",
      "================================================================================\n",
      "Comparison: With BoT vs Without BoT\n",
      "================================================================================\n",
      "With BoT:    7 positions (1 BoT + 6 CoT)\n",
      "Without BoT: 6 positions (0 BoT + 6 CoT)\n",
      "\n",
      "This tests whether the BoT token improves reasoning performance.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LLaMA: Chain-of-Thought WITHOUT Beginning-of-Thought Token\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 1\n",
    "\n",
    "questions = [question]\n",
    "\n",
    "# Set pad token if needed\n",
    "if llama_tokenizer.pad_token is None:\n",
    "    llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "    llama_tokenizer.pad_token_id = llama_tokenizer.eos_token_id\n",
    "\n",
    "# Tokenize WITHOUT BoT token - just the raw question\n",
    "inputs = llama_tokenizer(questions, return_tensors=\"pt\", padding=False)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "print(f\"WITHOUT BoT - Input IDs: {inputs['input_ids']}\")\n",
    "print(f\"WITHOUT BoT - Last 5 token IDs: {inputs['input_ids'][0, -5:].tolist()}\")\n",
    "print(f\"WITHOUT BoT - BoT token ID should be: {llama_model.bot_id}\")\n",
    "print(f\"WITHOUT BoT - Contains BoT?: {llama_model.bot_id in inputs['input_ids'][0].tolist()}\")\n",
    "print(f\"Input shape (no BoT): {inputs['input_ids'].shape}\")\n",
    "\n",
    "# Store the 6 chain-of-thought hidden states (no BoT position)\n",
    "cot_hidden_states_no_bot = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Initial encoding - start directly from the last token of the question\n",
    "    past_key_values = None\n",
    "    outputs = llama_model.codi(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        use_cache=True,\n",
    "        output_hidden_states=True,\n",
    "        past_key_values=past_key_values,\n",
    "        attention_mask=inputs[\"attention_mask\"]\n",
    "    )\n",
    "    past_key_values = outputs.past_key_values\n",
    "    \n",
    "    # Start from the last question token (no BoT)\n",
    "    latent_embd = outputs.hidden_states[-1][:, -1:, :]  # Shape: [batch, 1, hidden_dim]\n",
    "    \n",
    "    # Apply initial projection\n",
    "    if llama_training_args.use_prj:\n",
    "        latent_embd = llama_model.prj(latent_embd)\n",
    "    \n",
    "    # Chain-of-Thought iterations (6 positions, no BoT)\n",
    "    for i in range(llama_training_args.inf_latent_iterations):\n",
    "        outputs = llama_model.codi(\n",
    "            inputs_embeds=latent_embd,\n",
    "            use_cache=True,\n",
    "            output_hidden_states=True,\n",
    "            past_key_values=past_key_values\n",
    "        )\n",
    "        past_key_values = outputs.past_key_values\n",
    "        \n",
    "        # Get hidden state BEFORE projection\n",
    "        latent_embd = outputs.hidden_states[-1][:, -1:, :]  # Shape: [batch, 1, hidden_dim]\n",
    "        \n",
    "        # Store BEFORE projection for decoding\n",
    "        cot_hidden_states_no_bot.append(latent_embd.clone())\n",
    "        print(f\"‚úì Position {i} (T{i}): shape={latent_embd.shape}\")\n",
    "        \n",
    "        # Apply projection for next iteration\n",
    "        if llama_training_args.use_prj:\n",
    "            latent_embd = llama_model.prj(latent_embd)\n",
    "\n",
    "# Stack all 6 positions (no BoT)\n",
    "llama_continuous_thoughts_no_bot = torch.cat(cot_hidden_states_no_bot, dim=1)\n",
    "print(f\"\\n‚úì Total continuous thoughts (no BoT): {llama_continuous_thoughts_no_bot.shape}\")\n",
    "print(f\"  Expected: [1, 6, hidden_dim]\")\n",
    "print(f\"  Got: {list(llama_continuous_thoughts_no_bot.shape)}\")\n",
    "\n",
    "if llama_continuous_thoughts_no_bot.shape[1] != 6:\n",
    "    print(f\"\\n‚ùå ERROR: Expected 6 positions, got {llama_continuous_thoughts_no_bot.shape[1]}\")\n",
    "else:\n",
    "    print(f\"\\n‚úì SUCCESS: Collected exactly 6 CoT positions (no BoT)!\")\n",
    "\n",
    "# Decode the continuous thoughts\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Decoding Continuous Thoughts (no BoT)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "llama_decoded_tokens_no_bot = []\n",
    "llama_number_positions_no_bot = []\n",
    "\n",
    "for i in range(llama_continuous_thoughts_no_bot.shape[1]):\n",
    "    hidden_state = llama_continuous_thoughts_no_bot[:, i, :]\n",
    "    \n",
    "    # Decode using CODI's LM head\n",
    "    logits = llama_model.codi.lm_head(hidden_state)\n",
    "    top1_token_id = torch.argmax(logits, dim=-1).item()\n",
    "    top1_token_str = llama_tokenizer.decode([top1_token_id])\n",
    "    \n",
    "    is_number = bool(number_regex.match(top1_token_str.strip()))\n",
    "    pos_type = f\"T{i}\"\n",
    "    \n",
    "    llama_decoded_tokens_no_bot.append((i, top1_token_id, top1_token_str, is_number))\n",
    "    if is_number:\n",
    "        llama_number_positions_no_bot.append(i)\n",
    "    \n",
    "    marker = \" üî¢ NUMBER!\" if is_number else \"\"\n",
    "    print(f\"  {pos_type:4s} [pos={i:2d}]: token_id={top1_token_id:5d} ‚Üí '{top1_token_str}'{marker}\")\n",
    "\n",
    "print(f\"\\nüìä Numbers at positions: {llama_number_positions_no_bot}\")\n",
    "print(f\"üìä Total numbers: {len(llama_number_positions_no_bot)}/6\")\n",
    "\n",
    "# Generate final answer (no BoT)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Generate Final Answer (no BoT)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Signal end-of-thought with EOT token\n",
    "    if llama_training_args.remove_eos:\n",
    "        eot_tensor = torch.tensor([[llama_model.eot_id]], dtype=torch.long).expand(batch_size, 1, 1).to(device)\n",
    "    else:\n",
    "        eot_tensor = torch.tensor([[llama_tokenizer.eos_token_id, llama_model.eot_id]], \n",
    "                                   dtype=torch.long).expand(batch_size, 1, 2).to(device)\n",
    "    \n",
    "    eot_emb = llama_model.get_embd(llama_model.codi, llama_model.model_name)(eot_tensor).squeeze(1)\n",
    "    output = eot_emb\n",
    "    \n",
    "    # Generate answer tokens\n",
    "    pred_tokens = []\n",
    "    \n",
    "    for step in range(256):\n",
    "        out = llama_model.codi(\n",
    "            inputs_embeds=output,\n",
    "            output_hidden_states=False,\n",
    "            attention_mask=None,\n",
    "            use_cache=True,\n",
    "            output_attentions=False,\n",
    "            past_key_values=past_key_values\n",
    "        )\n",
    "        past_key_values = out.past_key_values\n",
    "        \n",
    "        logits = out.logits[:, -1, :llama_model.codi.config.vocab_size-1]\n",
    "        next_token_id = torch.argmax(logits, dim=-1).item()\n",
    "        pred_tokens.append(next_token_id)\n",
    "        \n",
    "        # Decode current token\n",
    "        current_token_str = llama_tokenizer.decode([next_token_id])\n",
    "        \n",
    "        # Stop if EOS token\n",
    "        if next_token_id == llama_tokenizer.eos_token_id:\n",
    "            print(f\"Stopped at step {step} (EOS token)\")\n",
    "            break\n",
    "        \n",
    "        # Stop immediately after generating a number token\n",
    "        if number_regex.match(current_token_str.strip()):\n",
    "            print(f\"Stopped at step {step} (found number: '{current_token_str}')\")\n",
    "            break\n",
    "        \n",
    "        # Hard limit to prevent loops\n",
    "        if step >= 49:\n",
    "            print(f\"Stopped at step {step} (max length)\")\n",
    "            break\n",
    "        \n",
    "        output = llama_model.get_embd(llama_model.codi, llama_model.model_name)(\n",
    "            torch.tensor([[next_token_id]], device=device)\n",
    "        )\n",
    "    \n",
    "    # Decode full answer\n",
    "    full_answer_no_bot = llama_tokenizer.decode(pred_tokens, skip_special_tokens=True)\n",
    "    print(f\"\\nGenerated Answer (no BoT): {full_answer_no_bot}\")\n",
    "    \n",
    "    # Extract numerical answer\n",
    "    def extract_answer_number(text):\n",
    "        text = text.replace(',', '')\n",
    "        numbers = [s for s in re.findall(r'-?\\d+\\.?\\d*', text)]\n",
    "        if not numbers:\n",
    "            return None\n",
    "        return float(numbers[-1])\n",
    "    \n",
    "    print(f\"Qeustion: {question}\")\n",
    "    predicted_number_no_bot = extract_answer_number(full_answer_no_bot)\n",
    "    print(f\"Numerical answer (no BoT): {predicted_number_no_bot}\")\n",
    "    print(f\"Expected answer: \",answer)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Comparison: With BoT vs Without BoT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"With BoT:    7 positions (1 BoT + 6 CoT)\")\n",
    "print(f\"Without BoT: 6 positions (0 BoT + 6 CoT)\")\n",
    "print(f\"\\nThis tests whether the BoT token improves reasoning performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Tokenizers (Sanity Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Tokenizer Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_numbers = ['0', '1', '2', '3', '4', '5', '16', '18']\n",
    "\n",
    "print(\"\\nHow do tokenizers encode/decode numbers?\\n\")\n",
    "for num in test_numbers:\n",
    "    gpt2_ids = gpt2_tokenizer.encode(num, add_special_tokens=False)\n",
    "    gpt2_decoded = gpt2_tokenizer.decode(gpt2_ids)\n",
    "    gpt2_matches = bool(number_regex.match(gpt2_decoded))\n",
    "    \n",
    "    llama_ids = llama_tokenizer.encode(num, add_special_tokens=False)\n",
    "    llama_decoded = llama_tokenizer.decode(llama_ids)\n",
    "    llama_matches = bool(number_regex.match(llama_decoded))\n",
    "    \n",
    "    match_indicator = \"‚úì\" if gpt2_decoded == llama_decoded else \"‚úó MISMATCH\"\n",
    "    \n",
    "    print(f\"Number: '{num}'\")\n",
    "    print(f\"  GPT-2:  IDs={gpt2_ids} ‚Üí '{gpt2_decoded}' (matches={gpt2_matches})\")\n",
    "    print(f\"  LLaMA:  IDs={llama_ids} ‚Üí '{llama_decoded}' (matches={llama_matches})\")\n",
    "    print(f\"  {match_indicator}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nGPT-2:\")\n",
    "print(f\"  - Numbers detected: {len(gpt2_number_positions)}\")\n",
    "print(f\"  - Interventions: {len(gpt2_interventions)}\")\n",
    "\n",
    "print(f\"\\nLLaMA:\")\n",
    "print(f\"  - Numbers detected: {len(llama_number_positions)}\")\n",
    "print(f\"  - Interventions: {len(llama_interventions)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIAGNOSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(llama_number_positions) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  ROOT CAUSE IDENTIFIED:\")\n",
    "    print(\"LLaMA's lm_head does NOT predict number tokens from continuous thoughts.\")\n",
    "    print(\"This explains why there are 0 interventions.\")\n",
    "    print(\"\\nPossible reasons:\")\n",
    "    print(\"  1. Projection layers (bot_projection/thought_projection) not trained correctly\")\n",
    "    print(\"  2. lm_head not loaded correctly (check strict=False in load_state_dict)\")\n",
    "    print(\"  3. Vocabulary size mismatch causing wrong token predictions\")\n",
    "    print(\"  4. Continuous thoughts from different distribution than GPT-2\")\n",
    "elif len(llama_number_positions) < len(gpt2_number_positions):\n",
    "    print(\"\\n‚ö†Ô∏è  LLaMA detects fewer numbers than GPT-2\")\n",
    "    print(f\"  GPT-2: {len(gpt2_number_positions)} numbers\")\n",
    "    print(f\"  LLaMA: {len(llama_number_positions)} numbers\")\n",
    "else:\n",
    "    print(\"\\n‚úì Both models detect similar numbers of tokens\")\n",
    "    print(\"  The issue may be in the intervention logic, not the decoding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Investigation: Top-5 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Top-5 Predictions Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nGPT-2 - First 5 thought positions:\")\n",
    "for i in range(min(5, gpt2_latent_embd.shape[1])):\n",
    "    logits = gpt2_model.codi.lm_head(gpt2_latent_embd[:, i, :])\n",
    "    top5_vals, top5_ids = torch.topk(logits[0], 5)\n",
    "    top5_tokens = [gpt2_tokenizer.decode([tid.item()]) for tid in top5_ids]\n",
    "    \n",
    "    pos_type = \"BoT\" if i == 0 else f\"T{i}\"\n",
    "    print(f\"\\n  {pos_type} [pos={i}]:\")\n",
    "    for j, (token, val) in enumerate(zip(top5_tokens, top5_vals)):\n",
    "        is_num = \"‚Üê NUM\" if number_regex.match(token) else \"\"\n",
    "        print(f\"    {j+1}. '{token}' (logit={val.item():.2f}) {is_num}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"\\nLLaMA - First 5 thought positions:\")\n",
    "for i in range(min(5, llama_latent_embd.shape[1])):\n",
    "    logits = llama_model.codi.lm_head(llama_latent_embd[:, i, :])\n",
    "    top5_vals, top5_ids = torch.topk(logits[0], 5)\n",
    "    top5_tokens = [llama_tokenizer.decode([tid.item()]) for tid in top5_ids]\n",
    "    \n",
    "    pos_type = \"BoT\" if i == 0 else f\"T{i}\"\n",
    "    print(f\"\\n  {pos_type} [pos={i}]:\")\n",
    "    for j, (token, val) in enumerate(zip(top5_tokens, top5_vals)):\n",
    "        is_num = \"‚Üê NUM\" if number_regex.match(token) else \"\"\n",
    "        print(f\"    {j+1}. '{token}' (logit={val.item():.2f}) {is_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Activation Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Activation Norms\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "gpt2_norms = torch.norm(gpt2_latent_embd[0], dim=-1)\n",
    "llama_norms = torch.norm(llama_latent_embd[0], dim=-1)\n",
    "\n",
    "print(f\"\\nGPT-2 latent embedding norms (first 10 positions):\")\n",
    "for i in range(min(10, len(gpt2_norms))):\n",
    "    print(f\"  Position {i}: {gpt2_norms[i].item():.2f}\")\n",
    "\n",
    "print(f\"\\nLLaMA latent embedding norms (first 10 positions):\")\n",
    "for i in range(min(10, len(llama_norms))):\n",
    "    print(f\"  Position {i}: {llama_norms[i].item():.2f}\")\n",
    "\n",
    "print(f\"\\nGPT-2 mean norm: {gpt2_norms.mean().item():.2f}\")\n",
    "print(f\"LLaMA mean norm: {llama_norms.mean().item():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
