{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Projection Intervention: GPT-2 vs LLaMA\n",
    "\n",
    "This notebook investigates why GPT-2 produces 185-365 interventions while LLaMA produces 0.\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu128)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.17.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.7)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.3.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
      "Requirement already satisfied: hf_transfer in /usr/local/lib/python3.12/dist-packages (0.1.9)\n",
      "Requirement already satisfied: dotenv in /usr/local/lib/python3.12/dist-packages (0.9.9)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.10.23)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (7.1.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from peft) (1.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=3 in /usr/lib/python3/dist-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.2)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from dotenv) (1.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch peft matplotlib datasets tqdm hf_transfer dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train.py',\n",
       " 'test.py',\n",
       " 'src',\n",
       " 'scripts',\n",
       " 'requirements.txt',\n",
       " 'probe_latent_token.py',\n",
       " 'outputs',\n",
       " 'imgs',\n",
       " 'README.md',\n",
       " '.git']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"/workspace/CoT_Exploration/codi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.insert(0, \"/workspace/CoT_Exploration/codi\")\n",
    "from src.model import CODI, ModelArguments, TrainingArguments\n",
    "from peft import LoraConfig, TaskType\n",
    "from transformers import AutoTokenizer\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test text:\n",
      "Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
      "Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day. She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer's market. #### 18\n",
      "\n",
      "Length: 410 characters\n"
     ]
    }
   ],
   "source": [
    "# GSM8K example\n",
    "question = \"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\"\n",
    "answer = \"Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day. She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer's market. #### 18\"\n",
    "\n",
    "test_text = f\"{question}\\n{answer}\"\n",
    "print(f\"Test text:\\n{test_text}\")\n",
    "print(f\"\\nLength: {len(test_text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CODI-GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Loading CODI-GPT-2 from Local Checkpoint\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "  # Load from HuggingFace Hub\n",
    "gpt2_model_name = \"lhao499/codi-gpt2\"\n",
    "\n",
    "gpt2_model_args = ModelArguments(\n",
    "      model_name_or_path=\"gpt2\",\n",
    "      lora_init=True,\n",
    "      lora_r=128,\n",
    "      lora_alpha=32,\n",
    "      ckpt_dir=\"/workspace/CoT_Exploration/models/CODI-gpt2\",  # Local checkpoint\n",
    "      full_precision=True,\n",
    "      token=None\n",
    "  )\n",
    "\n",
    "gpt2_training_args = TrainingArguments(\n",
    "      output_dir=\"./outputs\",\n",
    "      model_max_length=512,\n",
    "      inf_latent_iterations=6,\n",
    "      use_prj=True,\n",
    "      prj_dim=768,\n",
    "      remove_eos=True,\n",
    "      greedy=True,\n",
    "      bf16=False,\n",
    "      inf_num_iterations=1\n",
    "  )\n",
    "\n",
    "gpt2_lora_config = LoraConfig(\n",
    "      task_type=TaskType.CAUSAL_LM,\n",
    "      inference_mode=False,\n",
    "      r=gpt2_model_args.lora_r,\n",
    "      lora_alpha=gpt2_model_args.lora_alpha,\n",
    "      lora_dropout=0.1,\n",
    "      target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],\n",
    "      init_lora_weights=True,\n",
    "  )\n",
    "\n",
    "gpt2_model = CODI(gpt2_model_args,\n",
    "gpt2_training_args, gpt2_lora_config)\n",
    "gpt2_model = gpt2_model.to(device)\n",
    "gpt2_model = gpt2_model.to(torch.bfloat16)\n",
    "gpt2_model.eval()\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "print(\"\u2713 CODI-GPT-2 loaded successfully from HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CODI-LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Logged in to HuggingFace\n"
     ]
    }
   ],
   "source": [
    "# Login to HuggingFace (optional - only needed  for gated models like LLaMA)\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "  # Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "  # Get HuggingFace token\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "\n",
    "if hf_token:\n",
    "      from huggingface_hub import login\n",
    "      login(token=hf_token)\n",
    "      print(\"\u2713 Logged in to HuggingFace\")\n",
    "else:\n",
    "      print(\"\u26a0 No HF_TOKEN found in .env file -  proceeding without authentication\")\n",
    "      print(\"  (This is fine if models are public)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Loading CODI-LLaMA from Local Checkpoint\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "  # Load from HuggingFace Hub\n",
    "llama_model_name = \"lhao499/codi-llama3.2-1b\"\n",
    "\n",
    "llama_model_args = ModelArguments(\n",
    "\n",
    "model_name_or_path=\"meta-llama/Llama-3.2-1B\",\n",
    "      lora_init=True,\n",
    "      lora_r=128,\n",
    "      lora_alpha=32,\n",
    "      ckpt_dir=\"/workspace/CoT_Exploration/models/CODI-llama3.2-1b\",  # Local checkpoint\n",
    "      full_precision=True,\n",
    "      token=None\n",
    "  )\n",
    "\n",
    "llama_training_args = TrainingArguments(\n",
    "      output_dir=\"./outputs\",\n",
    "      model_max_length=512,\n",
    "      inf_latent_iterations=6,\n",
    "      use_prj=True,\n",
    "      prj_dim=2048,\n",
    "      remove_eos=True,\n",
    "      greedy=True,\n",
    "      bf16=False,\n",
    "      inf_num_iterations=1\n",
    "  )\n",
    "\n",
    "llama_lora_config = LoraConfig(\n",
    "      task_type=TaskType.CAUSAL_LM,\n",
    "      inference_mode=False,\n",
    "      r=llama_model_args.lora_r,\n",
    "      lora_alpha=llama_model_args.lora_alpha,\n",
    "      lora_dropout=0.1,\n",
    "      target_modules=[\"q_proj\", \"v_proj\",\n",
    "  \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\",\n",
    "  \"down_proj\"],\n",
    "      init_lora_weights=True,\n",
    "  )\n",
    "\n",
    "llama_model = CODI(llama_model_args,\n",
    "  llama_training_args, llama_lora_config)\n",
    "llama_model = llama_model.to(device)\n",
    "llama_model = llama_model.to(torch.bfloat16)\n",
    "llama_model.eval()\n",
    "\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "print(\"\u2713 CODI-LLaMA loaded successfully from HuggingFace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run GPT-2 Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GPT-2 Forward Pass\n",
      "================================================================================\n",
      "Input shape: torch.Size([1, 114])\n",
      "Continuous thoughts shape: torch.Size([1, 114, 768])\n",
      "Number of thought positions: 114\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GPT-2 Forward Pass\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "gpt2_inputs = gpt2_tokenizer(test_text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "gpt2_input_ids = gpt2_inputs.input_ids.to(device)\n",
    "\n",
    "print(f\"Input shape: {gpt2_input_ids.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    gpt2_outputs = gpt2_model.codi(\n",
    "        input_ids=gpt2_input_ids,\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "    gpt2_continuous_thoughts = gpt2_outputs.hidden_states[-1]\n",
    "\n",
    "print(f\"Continuous thoughts shape: {gpt2_continuous_thoughts.shape}\")\n",
    "print(f\"Number of thought positions: {gpt2_continuous_thoughts.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run LLaMA Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LLaMA Forward Pass\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "llama_inputs = llama_tokenizer(test_text, return_tensors=\"pt\", add_special_tokens=True)\n",
    "llama_input_ids = llama_inputs.input_ids.to(device)\n",
    "\n",
    "print(f\"Input shape: {llama_input_ids.shape}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    llama_outputs = llama_model.codi(\n",
    "        input_ids=llama_input_ids,\n",
    "        output_hidden_states=True\n",
    "    )\n",
    "    llama_continuous_thoughts = llama_outputs.hidden_states[-1]\n",
    "\n",
    "print(f\"Continuous thoughts shape: {llama_continuous_thoughts.shape}\")\n",
    "print(f\"Number of thought positions: {llama_continuous_thoughts.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode GPT-2 Continuous Thoughts (like section5_analysis.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "GPT-2: Decoding Continuous Thoughts\n",
      "================================================================================\n",
      "Continuous thoughts shape: torch.Size([1, 114, 768])\n",
      "\n",
      "Decoded tokens from continuous thoughts:\n",
      "  BoT  [pos= 0]: token_id=   13 \u2192 '.'\n",
      "  T1   [pos= 1]: token_id=   11 \u2192 ','\n",
      "  T2   [pos= 2]: token_id=  717 \u2192 ' first'\n",
      "  T3   [pos= 3]: token_id=  389 \u2192 ' are'\n",
      "  T4   [pos= 4]: token_id=  287 \u2192 ' in'\n",
      "  T5   [pos= 5]: token_id= 3625 \u2192 ' feet'\n",
      "  T6   [pos= 6]: token_id=  287 \u2192 ' in'\n",
      "  T7   [pos= 7]: token_id= 1110 \u2192 ' day'\n",
      "  T8   [pos= 8]: token_id=   11 \u2192 ','\n",
      "  T9   [pos= 9]: token_id=  198 \u2192 '\n",
      "'\n",
      "  T10  [pos=10]: token_id=  318 \u2192 ' is'\n",
      "  T11  [pos=11]: token_id=  546 \u2192 ' about'\n",
      "  T12  [pos=12]: token_id=  284 \u2192 ' to'\n",
      "  T13  [pos=13]: token_id=  790 \u2192 ' every'\n",
      "  T14  [pos=14]: token_id= 1123 \u2192 ' each'\n",
      "\n",
      "\u2713 Numbers detected at positions: []\n",
      "\u2713 Total numbers: 0/15\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GPT-2: Decoding Continuous Thoughts\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Note: HuggingFace models don't have projection layers, decode directly\n",
    "print(f\"Continuous thoughts shape: {gpt2_continuous_thoughts.shape}\")\n",
    "\n",
    "# Number detection regex\n",
    "number_regex = re.compile(r'^\\s?\\d+')\n",
    "\n",
    "# Decode first 15 positions directly from continuous thoughts\n",
    "print(\"\\nDecoded tokens from continuous thoughts:\")\n",
    "gpt2_decoded_tokens = []\n",
    "gpt2_number_positions = []\n",
    "\n",
    "for i in range(min(15, gpt2_continuous_thoughts.shape[1])):\n",
    "    logits = gpt2_model.codi.lm_head(gpt2_continuous_thoughts[:, i, :])\n",
    "    top1_token_id = torch.argmax(logits, dim=-1).item()\n",
    "    top1_token_str = gpt2_tokenizer.decode([top1_token_id])\n",
    "\n",
    "    is_number = bool(number_regex.match(top1_token_str))\n",
    "    pos_type = \"BoT\" if i == 0 else f\"T{i}\"\n",
    "\n",
    "    gpt2_decoded_tokens.append((i, top1_token_id, top1_token_str, is_number))\n",
    "    if is_number:\n",
    "        gpt2_number_positions.append(i)\n",
    "\n",
    "    marker = \" \u2190 NUMBER!\" if is_number else \"\"\n",
    "    print(f\"  {pos_type:4s} [pos={i:2d}]: token_id={top1_token_id:5d} \u2192 '{top1_token_str}'{marker}\")\n",
    "\n",
    "print(f\"\\n\u2713 Numbers detected at positions: {gpt2_number_positions}\")\n",
    "print(f\"\u2713 Total numbers: {len(gpt2_number_positions)}/15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decode LLaMA Continuous Thoughts (like section5_analysis.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LLaMA: Decoding Continuous Thoughts\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Note: HuggingFace models don't have projection layers, decode directly\n",
    "print(f\"Continuous thoughts shape: {llama_continuous_thoughts.shape}\")\n",
    "\n",
    "# Decode first 15 positions directly from continuous thoughts\n",
    "print(\"\\nDecoded tokens from continuous thoughts:\")\n",
    "llama_decoded_tokens = []\n",
    "llama_number_positions = []\n",
    "\n",
    "for i in range(min(15, llama_continuous_thoughts.shape[1])):\n",
    "    logits = llama_model.codi.lm_head(llama_continuous_thoughts[:, i, :])\n",
    "    top1_token_id = torch.argmax(logits, dim=-1).item()\n",
    "    top1_token_str = llama_tokenizer.decode([top1_token_id])\n",
    "\n",
    "    is_number = bool(number_regex.match(top1_token_str))\n",
    "    pos_type = \"BoT\" if i == 0 else f\"T{i}\"\n",
    "\n",
    "    llama_decoded_tokens.append((i, top1_token_id, top1_token_str, is_number))\n",
    "    if is_number:\n",
    "        llama_number_positions.append(i)\n",
    "\n",
    "    marker = \" \u2190 NUMBER!\" if is_number else \"\"\n",
    "    print(f\"  {pos_type:4s} [pos={i:2d}]: token_id={top1_token_id:5d} \u2192 '{top1_token_str}'{marker}\")\n",
    "\n",
    "print(f\"\\n\u2713 Numbers detected at positions: {llama_number_positions}\")\n",
    "print(f\"\u2713 Total numbers: {len(llama_number_positions)}/15\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Tokenizers (Sanity Check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Tokenizer Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_numbers = ['0', '1', '2', '3', '4', '5', '16', '18']\n",
    "\n",
    "print(\"\\nHow do tokenizers encode/decode numbers?\\n\")\n",
    "for num in test_numbers:\n",
    "    gpt2_ids = gpt2_tokenizer.encode(num, add_special_tokens=False)\n",
    "    gpt2_decoded = gpt2_tokenizer.decode(gpt2_ids)\n",
    "    gpt2_matches = bool(number_regex.match(gpt2_decoded))\n",
    "    \n",
    "    llama_ids = llama_tokenizer.encode(num, add_special_tokens=False)\n",
    "    llama_decoded = llama_tokenizer.decode(llama_ids)\n",
    "    llama_matches = bool(number_regex.match(llama_decoded))\n",
    "    \n",
    "    match_indicator = \"\u2713\" if gpt2_decoded == llama_decoded else \"\u2717 MISMATCH\"\n",
    "    \n",
    "    print(f\"Number: '{num}'\")\n",
    "    print(f\"  GPT-2:  IDs={gpt2_ids} \u2192 '{gpt2_decoded}' (matches={gpt2_matches})\")\n",
    "    print(f\"  LLaMA:  IDs={llama_ids} \u2192 '{llama_decoded}' (matches={llama_matches})\")\n",
    "    print(f\"  {match_indicator}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2: Projection Intervention (Target Token = '5', k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"GPT-2: Projection Intervention\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "target_token = '5'\n",
    "k = 3\n",
    "\n",
    "# Get target token embedding\n",
    "target_token_id = gpt2_tokenizer.encode(target_token, add_special_tokens=False)[0]\n",
    "embedding_layer = gpt2_model.codi.get_input_embeddings()\n",
    "target_embd = embedding_layer(torch.tensor([target_token_id], device=device))\n",
    "\n",
    "print(f\"Target token: '{target_token}'\")\n",
    "print(f\"Target token ID: {target_token_id}\")\n",
    "print(f\"k (top-k intervention): {k}\")\n",
    "\n",
    "gpt2_interventions = []\n",
    "\n",
    "for i in range(min(15, gpt2_latent_embd.shape[1])):\n",
    "    pos_type = \"BoT\" if i == 0 else f\"T{i}\"\n",
    "    \n",
    "    # Get predicted token\n",
    "    logits = gpt2_model.codi.lm_head(gpt2_latent_embd[:, i, :])\n",
    "    top1_token_id = torch.argmax(logits, dim=-1).item()\n",
    "    top1_token_str = gpt2_tokenizer.decode([top1_token_id])\n",
    "    \n",
    "    # Check if it's a number\n",
    "    is_number = bool(number_regex.match(top1_token_str))\n",
    "    \n",
    "    if is_number:\n",
    "        # Get predicted token embedding\n",
    "        predicted_embd = embedding_layer(torch.tensor([top1_token_id], device=device))\n",
    "        \n",
    "        # Get activation\n",
    "        A = gpt2_latent_embd[:, i, :]  # [1, hidden_dim]\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        E_pred_norm = predicted_embd / torch.norm(predicted_embd, dim=-1, keepdim=True)\n",
    "        E_target_norm = target_embd / torch.norm(target_embd, dim=-1, keepdim=True)\n",
    "        \n",
    "        # Projection removal and replacement\n",
    "        proj_predicted = torch.sum(A * E_pred_norm, dim=-1, keepdim=True) * E_pred_norm\n",
    "        proj_target = torch.sum(A * E_target_norm, dim=-1, keepdim=True) * E_target_norm\n",
    "        \n",
    "        A_modified = A - proj_predicted + k * proj_target\n",
    "        \n",
    "        # Decode modified activation\n",
    "        logits_modified = gpt2_model.codi.lm_head(A_modified)\n",
    "        new_token_id = torch.argmax(logits_modified, dim=-1).item()\n",
    "        new_token_str = gpt2_tokenizer.decode([new_token_id])\n",
    "        \n",
    "        gpt2_interventions.append({\n",
    "            'position': i,\n",
    "            'predicted_token': top1_token_str,\n",
    "            'new_token': new_token_str,\n",
    "            'intervened': True\n",
    "        })\n",
    "        \n",
    "        print(f\"{pos_type:4s} [pos={i:2d}]: '{top1_token_str}' \u2192 '{new_token_str}' (intervened)\")\n",
    "    else:\n",
    "        print(f\"{pos_type:4s} [pos={i:2d}]: '{top1_token_str}' (not a number, skipped)\")\n",
    "\n",
    "print(f\"\\n\u2713 GPT-2 interventions: {len(gpt2_interventions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLaMA: Projection Intervention (Target Token = '5', k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LLaMA: Projection Intervention\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "target_token = '5'\n",
    "k = 3\n",
    "\n",
    "# Get target token embedding\n",
    "target_token_id = llama_tokenizer.encode(target_token, add_special_tokens=False)[0]\n",
    "embedding_layer = llama_model.codi.get_input_embeddings()\n",
    "target_embd = embedding_layer(torch.tensor([target_token_id], device=device))\n",
    "\n",
    "print(f\"Target token: '{target_token}'\")\n",
    "print(f\"Target token ID: {target_token_id}\")\n",
    "print(f\"k (top-k intervention): {k}\")\n",
    "\n",
    "llama_interventions = []\n",
    "\n",
    "for i in range(min(15, llama_latent_embd.shape[1])):\n",
    "    pos_type = \"BoT\" if i == 0 else f\"T{i}\"\n",
    "    \n",
    "    # Get predicted token\n",
    "    logits = llama_model.codi.lm_head(llama_latent_embd[:, i, :])\n",
    "    top1_token_id = torch.argmax(logits, dim=-1).item()\n",
    "    top1_token_str = llama_tokenizer.decode([top1_token_id])\n",
    "    \n",
    "    # Check if it's a number\n",
    "    is_number = bool(number_regex.match(top1_token_str))\n",
    "    \n",
    "    if is_number:\n",
    "        # Get predicted token embedding\n",
    "        predicted_embd = embedding_layer(torch.tensor([top1_token_id], device=device))\n",
    "        \n",
    "        # Get activation\n",
    "        A = llama_latent_embd[:, i, :]  # [1, hidden_dim]\n",
    "        \n",
    "        # Normalize embeddings\n",
    "        E_pred_norm = predicted_embd / torch.norm(predicted_embd, dim=-1, keepdim=True)\n",
    "        E_target_norm = target_embd / torch.norm(target_embd, dim=-1, keepdim=True)\n",
    "        \n",
    "        # Projection removal and replacement\n",
    "        proj_predicted = torch.sum(A * E_pred_norm, dim=-1, keepdim=True) * E_pred_norm\n",
    "        proj_target = torch.sum(A * E_target_norm, dim=-1, keepdim=True) * E_target_norm\n",
    "        \n",
    "        A_modified = A - proj_predicted + k * proj_target\n",
    "        \n",
    "        # Decode modified activation\n",
    "        logits_modified = llama_model.codi.lm_head(A_modified)\n",
    "        new_token_id = torch.argmax(logits_modified, dim=-1).item()\n",
    "        new_token_str = llama_tokenizer.decode([new_token_id])\n",
    "        \n",
    "        llama_interventions.append({\n",
    "            'position': i,\n",
    "            'predicted_token': top1_token_str,\n",
    "            'new_token': new_token_str,\n",
    "            'intervened': True\n",
    "        })\n",
    "        \n",
    "        print(f\"{pos_type:4s} [pos={i:2d}]: '{top1_token_str}' \u2192 '{new_token_str}' (intervened)\")\n",
    "    else:\n",
    "        print(f\"{pos_type:4s} [pos={i:2d}]: '{top1_token_str}' (not a number, skipped)\")\n",
    "\n",
    "print(f\"\\n\u2713 LLaMA interventions: {len(llama_interventions)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nGPT-2:\")\n",
    "print(f\"  - Numbers detected: {len(gpt2_number_positions)}\")\n",
    "print(f\"  - Interventions: {len(gpt2_interventions)}\")\n",
    "\n",
    "print(f\"\\nLLaMA:\")\n",
    "print(f\"  - Numbers detected: {len(llama_number_positions)}\")\n",
    "print(f\"  - Interventions: {len(llama_interventions)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIAGNOSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(llama_number_positions) == 0:\n",
    "    print(\"\\n\u26a0\ufe0f  ROOT CAUSE IDENTIFIED:\")\n",
    "    print(\"LLaMA's lm_head does NOT predict number tokens from continuous thoughts.\")\n",
    "    print(\"This explains why there are 0 interventions.\")\n",
    "    print(\"\\nPossible reasons:\")\n",
    "    print(\"  1. Projection layers (bot_projection/thought_projection) not trained correctly\")\n",
    "    print(\"  2. lm_head not loaded correctly (check strict=False in load_state_dict)\")\n",
    "    print(\"  3. Vocabulary size mismatch causing wrong token predictions\")\n",
    "    print(\"  4. Continuous thoughts from different distribution than GPT-2\")\n",
    "elif len(llama_number_positions) < len(gpt2_number_positions):\n",
    "    print(\"\\n\u26a0\ufe0f  LLaMA detects fewer numbers than GPT-2\")\n",
    "    print(f\"  GPT-2: {len(gpt2_number_positions)} numbers\")\n",
    "    print(f\"  LLaMA: {len(llama_number_positions)} numbers\")\n",
    "else:\n",
    "    print(\"\\n\u2713 Both models detect similar numbers of tokens\")\n",
    "    print(\"  The issue may be in the intervention logic, not the decoding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Investigation: Top-5 Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Top-5 Predictions Comparison\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nGPT-2 - First 5 thought positions:\")\n",
    "for i in range(min(5, gpt2_latent_embd.shape[1])):\n",
    "    logits = gpt2_model.codi.lm_head(gpt2_latent_embd[:, i, :])\n",
    "    top5_vals, top5_ids = torch.topk(logits[0], 5)\n",
    "    top5_tokens = [gpt2_tokenizer.decode([tid.item()]) for tid in top5_ids]\n",
    "    \n",
    "    pos_type = \"BoT\" if i == 0 else f\"T{i}\"\n",
    "    print(f\"\\n  {pos_type} [pos={i}]:\")\n",
    "    for j, (token, val) in enumerate(zip(top5_tokens, top5_vals)):\n",
    "        is_num = \"\u2190 NUM\" if number_regex.match(token) else \"\"\n",
    "        print(f\"    {j+1}. '{token}' (logit={val.item():.2f}) {is_num}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"\\nLLaMA - First 5 thought positions:\")\n",
    "for i in range(min(5, llama_latent_embd.shape[1])):\n",
    "    logits = llama_model.codi.lm_head(llama_latent_embd[:, i, :])\n",
    "    top5_vals, top5_ids = torch.topk(logits[0], 5)\n",
    "    top5_tokens = [llama_tokenizer.decode([tid.item()]) for tid in top5_ids]\n",
    "    \n",
    "    pos_type = \"BoT\" if i == 0 else f\"T{i}\"\n",
    "    print(f\"\\n  {pos_type} [pos={i}]:\")\n",
    "    for j, (token, val) in enumerate(zip(top5_tokens, top5_vals)):\n",
    "        is_num = \"\u2190 NUM\" if number_regex.match(token) else \"\"\n",
    "        print(f\"    {j+1}. '{token}' (logit={val.item():.2f}) {is_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect Activation Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"Activation Norms\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "gpt2_norms = torch.norm(gpt2_latent_embd[0], dim=-1)\n",
    "llama_norms = torch.norm(llama_latent_embd[0], dim=-1)\n",
    "\n",
    "print(f\"\\nGPT-2 latent embedding norms (first 10 positions):\")\n",
    "for i in range(min(10, len(gpt2_norms))):\n",
    "    print(f\"  Position {i}: {gpt2_norms[i].item():.2f}\")\n",
    "\n",
    "print(f\"\\nLLaMA latent embedding norms (first 10 positions):\")\n",
    "for i in range(min(10, len(llama_norms))):\n",
    "    print(f\"  Position {i}: {llama_norms[i].item():.2f}\")\n",
    "\n",
    "print(f\"\\nGPT-2 mean norm: {gpt2_norms.mean().item():.2f}\")\n",
    "print(f\"LLaMA mean norm: {llama_norms.mean().item():.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}