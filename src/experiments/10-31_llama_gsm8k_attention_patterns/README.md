# Experiment 3: CoT Attention Pattern Analysis

**Status:** 95% Complete - Infrastructure Ready, Minor Input Formatting Issue

## Purpose
Validate the parallel vs sequential hypothesis from Experiment 2 by analyzing attention patterns between CoT positions.

**Key Question:** Do CoT positions attend to each other sequentially (0â†’1â†’2â†’3â†’4) or in parallel/distributed fashion?

## What's Implemented âœ…

### Core Modules
1. **attention_extractor.py** - Hooks into model layers to capture attention weights for CoT positions
2. **attention_metrics.py** - Computes:
   - Sequential score (attention from Nâ†’N-1)
   - Self-attention score (Nâ†’N)
   - Entropy of attention distribution
   - Forward vs backward attention
3. **visualizations.py** - Creates:
   - Layer-wise heatmaps (16 layers)
   - Aggregated attention matrix
   - Metrics comparison plots
   - Layer evolution plots
4. **model_loader.py** - Loads CODI model with LoRA
5. **run_attention_analysis.py** - Main experiment script

### Configuration
- config.py with all experiment parameters
- TEST_MODE for quick validation
- W&B integration
- Proper CODI model loading

## Remaining Issue ðŸ”§

**Problem:** Need to properly format input for CODI with placeholder CoT tokens

**Current Status:**
- The script runs but can't find CoT positions because we need to insert placeholder tokens
- Line 153-154 in run_attention_analysis.py needs to be updated

**Solution Needed:**
The CODI model expects input format:
```
<|start_header_id|>user<|end_header_id|>\n\n{question}<|eot_id|><|start_header_id|>think<|end_header_id|>\n\n[PAD][PAD][PAD][PAD][PAD]<|eot_id|>
```

Where [PAD] represents 5 placeholder tokens for the continuous thought positions.

**Reference:** Check how `prepare_codi_input` works in:
- `src/experiments/10-30_llama_gsm8k_iterative_patching/core/model_loader.py` lines 87-117

**Fix Needed:**
1. Get BoT (beginning of thought) and EoT (end of thought) token IDs from tokenizer
2. Insert 5 placeholder tokens (use pad_token_id) between BoT and EoT
3. Track positions of these 5 tokens for attention extraction

## How to Run (Once Fixed)

```bash
cd /home/paperspace/dev/CoT_Exploration/src/experiments/10-31_llama_gsm8k_attention_patterns

# Test mode (10 examples)
export PYTHONPATH="/home/paperspace/dev/CoT_Exploration/codi:$PYTHONPATH"
python3 scripts/run_attention_analysis.py

# Full mode (57 examples) - edit config.py first:
# Set TEST_MODE = False
python3 scripts/run_attention_analysis.py
```

## Expected Output

### Metrics
- Sequential Score: ~0.2 if sequential, ~0.05 if parallel
- Entropy: ~2.3 bits (max) if parallel, ~1.0 if sequential
- Self-Attention: Higher if independent positions

### Visualizations
- `visualizations/layer_wise/` - 16 heatmaps showing attention evolution
- `visualizations/aggregated_attention.png` - Overall attention pattern
- `visualizations/metrics_comparison.png` - Bar charts of key metrics
- `visualizations/*_evolution.png` - How metrics change across layers

### Interpretation
- **High sequential score + low entropy** â†’ Sequential chain
- **Low sequential score + high entropy** â†’ Parallel ensemble (validates Exp 2)

## Files Created
- config.py
- core/__init__.py
- core/model_loader.py
- core/attention_extractor.py
- core/attention_metrics.py
- core/visualizations.py
- scripts/run_attention_analysis.py
- README.md (this file)

## Next Steps
1. Fix input formatting to insert placeholder tokens
2. Run TEST_MODE to validate
3. Run full experiment (57 pairs)
4. Analyze results and create visualizations
5. Document findings in docs/experiments/10-31_llama_gsm8k_attention_patterns.md
6. Commit results

## Value
This experiment provides **direct visual evidence** of whether CoT operates sequentially or in parallel, complementing the causal evidence from Experiment 2.
